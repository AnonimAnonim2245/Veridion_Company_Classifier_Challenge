{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Data Preprocessing for Token-Label Matching\n",
    "\n",
    "In this step, we prepare all necessary embeddings, contextual matrices, token metadata, and TF-IDF weighting structures for semantic matching and final classification.\n",
    "\n",
    "\n",
    "\n",
    "### Loading Vector Embeddings\n",
    "\n",
    "We load the following precomputed embedding matrices using `numpy`:\n",
    "\n",
    "- **Token embeddings (`vec_embeddings.txt`)**  \n",
    "  Dense semantic representations of company tokens.\n",
    "\n",
    "- **Label embeddings (`label_embeddings.txt`)**  \n",
    "  Dense semantic representations of insurance taxonomy labels.\n",
    "\n",
    "- **Category and Niche embeddings (`cat_niche_embeddings.txt`)**  \n",
    "  Embeddings representing combined niche and category structures.\n",
    "\n",
    "- **Context matrices**:\n",
    "  - **`context_matrix.txt`**: Captures the general semantic context for tokens.\n",
    "  - **`first_sentence_matrix_total1.txt`**: Captures semantic features specifically from the first sentence of company descriptions.\n",
    "\n",
    "\n",
    "\n",
    "### Loading Structured Tabular Data\n",
    "\n",
    "We load multiple structured datasets using `pandas`:\n",
    "\n",
    "- **`tokens_vector.csv`**  \n",
    "  Tokens associated with each company.\n",
    "\n",
    "- **`sectors.csv`**  \n",
    "  Sector information for each company.\n",
    "\n",
    "- **`counted_elements.csv`**  \n",
    "  Frequency counts of each token across company descriptions.\n",
    "\n",
    "- **`new_label_with_categories.csv`**  \n",
    "  Structured decomposition of labels into domain, modifier, and core parts.\n",
    "\n",
    "- **`ml_insurance_challenge.csv`**  \n",
    "  Main company dataset, cleaned by dropping missing values.\n",
    "\n",
    "- **`insurance_taxonomy.csv`**  \n",
    "  Official insurance taxonomy used for classification.\n",
    "- **`our_counted_words_category`**  \n",
    "  Frequency counts of each **category and niche term** across all company descriptions.\n",
    "- **`our_current_categories.csv`**  \n",
    "  Category and niche combinations, tokenized and preprocessed.\n",
    "\n",
    "\n",
    "### Text Normalization and Cleaning\n",
    "\n",
    "- **Lowercase Conversion**  \n",
    "  All tokens are converted to lowercase for consistency.\n",
    "\n",
    "- **Selective Punctuation Removal**  \n",
    "  Unwanted punctuation is removed, while useful characters such as underscores (`_`), periods (`.`), dashes (`-`), and semicolons (`;`) are preserved to maintain compound word structures.\n",
    "\n",
    "- **Tokenization**  \n",
    "  Fields such as `new_col` (company tokens) and `count_words` (token counts) are split into clean, tokenized word lists.  \n",
    "  Category and niche fields are also tokenized.\n",
    "\n",
    "\n",
    "### Loading Additional Structures\n",
    "\n",
    "We also load manually curated and precomputed resources:\n",
    "- **Most Important Words (`most_important_words.txt`)**  \n",
    "  Tokens extracted primarily from the **first sentences** of company descriptions (after filtering), representing highly significant features.\n",
    "\n",
    "- **Average TF-IDF Scores (`average_tf_idf_per_word.txt`)**  \n",
    "  Precomputed average TF-IDF values for important tokens across all companies.\n",
    "\n",
    "- **Per-Token TF-IDF Matches (`match_words_with_tf_idf.txt`)**  \n",
    "  Contains TF-IDF scores for each word-company pair.  \n",
    "  Higher TF-IDF indicates stronger relevance for that specific company.\n",
    "### Category and Niche Similarity Matrices\n",
    "\n",
    "Two important matrices reflect semantic similarity of companies to their metadata categories:\n",
    "\n",
    "- `similarity_desc_to_niche.txt` → (`similarity_to_niche`)  \n",
    "  Semantic match between company descriptions and niche embeddings.\n",
    "\n",
    "- `similarity_desc_to_category.txt` → (`similarity_to_cat`)  \n",
    "  Semantic match between company descriptions and category embeddings.\n",
    "\n",
    "These are used to **validate** or **boost** label candidates if their components align with niche/category fields.\n",
    "\n",
    "### Goal\n",
    "\n",
    "This preprocessing step ensures that:\n",
    "\n",
    "- All token-level semantic information (embeddings, context, first-sentence emphasis) is prepared.\n",
    "- Frequency and TF-IDF importance are properly computed for every token.\n",
    "- Tokens, sectors, and categories are normalized and consistent.\n",
    "- All necessary structures for fast and accurate semantic matching against insurance taxonomy labels are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "vec_embeddings = np.loadtxt(\"vec_embeddings.txt\", dtype=float)\n",
    "label_embeddings = np.loadtxt(\"label_embeddings.txt\", dtype=float)\n",
    "cat_niche_embeddings =np.loadtxt(\"cat_niche_embeddings.txt\", dtype=float)\n",
    "context_matrix = np.loadtxt(\"context_matrix.txt\", dtype=float)\n",
    "first_sentence_matrix = np.loadtxt(\"first_sentence_matrix_total1.txt\", dtype=float)\n",
    "similarity_to_cat = np.loadtxt(\"similarity_desc_to_category.txt\", dtype=float)\n",
    "similarity_to_niche = np.loadtxt(\"similarity_desc_to_niche.txt\", dtype=float)\n",
    "\n",
    "our_words = pd.read_csv(\"tokens_vector.csv\")\n",
    "our_sector = pd.read_csv(\"sectors.csv\")\n",
    "our_sector['sector']=our_sector['sector'].fillna(\"\")\n",
    "our_counted_words = pd.read_csv('counted_elements.csv')\n",
    "our_counted_words_category = pd.read_csv('counted_categories.csv')\n",
    "\n",
    "divided_words = pd.read_csv(\"new_label_with_categories.csv\")\n",
    "all_punctuation_except_hyphen = string.punctuation.replace(\"_\",\"\").replace(\".\",\"\").replace(\"-\",\"\").replace(\";\",\"\").replace(\"-\",\"\")\n",
    "\n",
    "our_words['new_col2'] = our_words['new_col'].str.lower().replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()\n",
    "our_counted_words['count_words'] = our_counted_words['new_col'].str.lower().replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()\n",
    "our_counted_words_category['count_cat'] = our_counted_words_category['count_niche_cat'].str.lower().replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()\n",
    "\n",
    "df = pd.read_csv(\"../inputs/ml_insurance_challenge.csv\")\n",
    "our_classes = pd.read_csv(\"../inputs/insurance_taxonomy - insurance_taxonomy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_matrix_niche = np.loadtxt(\"niche_matrix.txt\", dtype=float)\n",
    "context_matrix_category = np.loadtxt(\"category_matrix.txt\", dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this step, we preprocess the tokens by combining frequency-based and TF-IDF-based information.  \n",
    "We identify **strong terms** (high z-score and relevance) and **weak terms** (low z-score), based on a combination of frequency counts and TF-IDF similarities.  \n",
    "Strong terms are later **boosted** to have more impact during label assignment, while weak terms are **filtered out** or deboosted to minimize noise.  \n",
    "This ensures that only the most relevant tokens contribute significantly to the final classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sector = pd.read_csv(\"sectors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sector['sector']=our_sector['sector'].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words_occurence = []\n",
    "for idx, _ in our_counted_words.iterrows():\n",
    "    element =(our_counted_words.at[idx, 'count_words'].copy())\n",
    "    element_category = (our_counted_words_category.at[idx, 'count_cat'].copy())\n",
    "    occurance = [int(element[i]) for i in range(1, len(element), 2)]\n",
    "    element = {element[i]: int(element[i + 1]) for i in range(0, len(element), 2)}    \n",
    "    element = {k : v for k, v in element.items() if k in our_words['new_col2'][idx]}\n",
    "    \n",
    "\n",
    "    sum_words = sum([element[k] for k in element])\n",
    "    our_counted_words.at[idx, 'count_words'] = element.copy()\n",
    "    sum_words_occurence.append(sum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, _ in our_counted_words.iterrows():\n",
    "    element_category = (our_counted_words_category.at[idx, 'count_cat'].copy())\n",
    "    element_category[0] = element_category[0].replace(\"counter\", \"\")\n",
    "    if len(element_category) == 1 and element_category[0] == \"\":\n",
    "        our_counted_words_category.at[idx, 'count_cat'] = element_category.copy()\n",
    "        continue\n",
    "    element_category = {element_category[i]: int(element_category[i + 1]) for i in range(0, len(element_category), 2)}\n",
    "    our_counted_words_category.at[idx, 'count_cat'] = element_category.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_categories_niche_words = pd.read_csv(\"our_current_categories.csv\")\n",
    "our_categories_niche_words['niche_plus_cat'] = our_categories_niche_words['niche_plus_cat'].str.lower().replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_words = []\n",
    "with  open('most_important_words.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split(\" \")\n",
    "        most_important_words.append(values)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_tf_idf= dict()\n",
    "with  open('average_tf_idf_per_word.txt') as f:\n",
    "    min_v=23\n",
    "    for line in f:\n",
    "        values = line.strip().split(\" \")\n",
    "        average_tf_idf[values[0]] = float(values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_words_with_tf_idf_valuess = {}\n",
    "elements_all = []\n",
    "for idx, _ in enumerate(our_words['new_col2']):\n",
    "    elements  = []\n",
    "    elements.append((1, 'z'))\n",
    "    elements_all.append(elements)\n",
    "\n",
    "with  open('match_words_with_tf_idf.txt') as f:\n",
    "    min_v=23\n",
    "    for line in f:\n",
    "        values = line.strip().split(\" \")\n",
    "        match_words_with_tf_idf_valuess[(values[0], int(values[1]))] = float(values[2])\n",
    "        if int(values[1])==1:\n",
    "           \n",
    "            if match_words_with_tf_idf_valuess[(values[0],int(values[1]))] < min_v and int(values[1])==1:\n",
    "                min_v = match_words_with_tf_idf_valuess[(values[0],int(values[1]))]\n",
    "        list_elements_all = elements_all[int(values[1])]\n",
    "        set_elements_all = set(list_elements_all)\n",
    "        set_elements_all.add((match_words_with_tf_idf_valuess[(values[0],int(values[1]))], values[0]))\n",
    "        list_elements_all = list(set_elements_all)\n",
    "        list_elements_all.sort(reverse=True)\n",
    "        elements_all[int(values[1])] = list_elements_all\n",
    "\n",
    "        if elements_all[int(values[1])][0][0] == 1 and  elements_all[int(values[1])][0][1] == 'z':\n",
    "            elements_all[int(values[1])] = elements_all[int(values[1])][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_value_all = []\n",
    "our_score_dict = {}\n",
    "\n",
    "for idx, _ in our_counted_words.iterrows():\n",
    "    element_value = []\n",
    "    sum_whole_tf_idf = sum([match_words_with_tf_idf_valuess[(wd, idx)] for wd in our_words['new_col2'][idx]])\n",
    "    for z in our_counted_words['count_words'][idx]:\n",
    "        element_value.append(our_counted_words['count_words'][idx][z]/sum_words_occurence[idx] * 0.5 + 0.5* match_words_with_tf_idf_valuess[(z, idx)]/sum_whole_tf_idf)\n",
    "        our_score_dict[(z, idx)] = our_counted_words['count_words'][idx][z]/sum_words_occurence[idx] * 0.5 + 0.5* match_words_with_tf_idf_valuess[(z, idx)]/sum_whole_tf_idf\n",
    "    element_value = list(zip(element_value, list(our_counted_words['count_words'][idx])))\n",
    "    element_value.sort(reverse=True)\n",
    "    element_value_all.append(element_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx=66\n",
    "strong_values_all = []\n",
    "weak_values_all = []\n",
    "\n",
    "for idx, _ in our_words.iterrows():\n",
    "\n",
    "    scores = [iz[0] for iz in element_value_all[idx]]\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    strong_values = []\n",
    "    weak_values = []\n",
    "\n",
    "    for score, word in element_value_all[idx]:\n",
    "        if std != 0:\n",
    "            z = (score - mean) / std\n",
    "        else:\n",
    "            std = 1\n",
    "        \n",
    "        \n",
    "        if z > 0.3 and (word not in average_tf_idf or average_tf_idf[word]>0.075):\n",
    "            strong_values.append(word)\n",
    "        elif z < -0.3:\n",
    "            weak_values.append(word)\n",
    "\n",
    "    strong_values_all.append(strong_values)\n",
    "    weak_values_all.append(weak_values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [iz[0] for iz in element_value_all[96]]\n",
    "mean = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "strong_values = []\n",
    "weak_values = []\n",
    "\n",
    "for score, word in element_value_all[idx]:\n",
    "    z = (score - mean) / std\n",
    "    \n",
    "    if z > 0.3:\n",
    "        strong_values.append(word)\n",
    "    elif z < -0.3:\n",
    "        weak_values.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_list = [2] * len(our_words['new_col2'])\n",
    "max_val_list = [-2] * len(our_words['new_col2'])\n",
    "z=0\n",
    "for idx, row in enumerate(our_words['new_col2']):\n",
    "    min_val = 2\n",
    "    max_val = -2\n",
    "    for i in row:\n",
    "        value = match_words_with_tf_idf_valuess[(i, z)]\n",
    "        if min_val > value:\n",
    "            min_val = value\n",
    "        if max_val < value:\n",
    "            max_val = value\n",
    "\n",
    "    min_val_list[idx] = min_val\n",
    "    max_val_list[idx] = max_val\n",
    "    z+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Classification with FAISS and Contextual Tie-Breaking\n",
    "\n",
    "In this file, we work with the **preprocessed data** obtained from the previous step.  \n",
    "We use **FAISS** to compare the `vec_embeddings` of company tokens with the `label_embeddings`. These results are then **combined with the contextual similarity matrix** (`context_matrix`) that we previously generated using `SentenceTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(label_embeddings.shape[1])\n",
    "label_embeddings = np.array(label_embeddings, dtype=np.float32, order='C')\n",
    "faiss.normalize_L2(label_embeddings)\n",
    "index.add(label_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def find_best_labels(vec_embeddings):\n",
    "    vec_embeddings = np.array(vec_embeddings, dtype=np.float32, order='C')\n",
    "\n",
    "    faiss.normalize_L2(vec_embeddings)\n",
    "\n",
    "    distances, indices = index.search(vec_embeddings, len(label_embeddings))\n",
    "    return (distances, indices) \n",
    "\n",
    "(distances, indices) = find_best_labels(vec_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def find_best_labels(vec_embeddings):\n",
    "    vec_embeddings = np.array(vec_embeddings, dtype=np.float32, order='C')\n",
    "\n",
    "    faiss.normalize_L2(vec_embeddings)\n",
    "\n",
    "    distances_cat, indices_cat = index.search(vec_embeddings, len(label_embeddings))\n",
    "    return (distances_cat, indices_cat) \n",
    "\n",
    "(distances_cat, indices_cat) = find_best_labels(cat_niche_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_functions_f import generate_noun_for_adj, generate_embedings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = generate_embedings_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Weighting Between FAISS and Contextual Similarity\n",
    "\n",
    "Through experimentation, we found that the most effective proportion for combining our two similarity measures is:\n",
    "\n",
    "- **0.7** for the **FAISS-based vector similarity** (token-level matching)\n",
    "- **0.3** for the **contextual similarity** (SentenceTransformer-based)\n",
    "\n",
    "This weighting balances **fine-grained token relevance** with **overall semantic meaning**, and led to more accurate and consistent label predictions in our testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "values_matrix = context_matrix\n",
    "\n",
    "list_labels_found_for_companies = []\n",
    "companies_matrix = copy.deepcopy(values_matrix)\n",
    "list_matrix=[]\n",
    "list_matrix2=[]\n",
    "similarity_matrix_total = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(df['description'])):\n",
    "    list__ = []\n",
    "    max1=-1\n",
    "    max_index = -1\n",
    "    list__cat=[]\n",
    "\n",
    "    las = set()\n",
    "    for j in range(0, 220):\n",
    "        val_distance_faiss = 1/(1+distances[i][j])\n",
    "        val_distance_faiss_cat = 1/(1+distances_cat[i][j])\n",
    "        las.add(indices[i][j])\n",
    "    \n",
    "        list__.append((val_distance_faiss * 0.7+ values_matrix[i][indices[i][j]]*0.3, indices[i][j]))\n",
    "        list__cat.append((val_distance_faiss_cat, indices_cat[i][j]))\n",
    "        \n",
    "        if max1 == -1 or max1 < list__[j][0]:\n",
    "            max1 = list__[j][0]\n",
    "\n",
    "            max_index = j\n",
    "    \n",
    "    list_matrix.append(list__)\n",
    "    list_matrix2.append(list__cat)\n",
    "    list_labels_found_for_companies.append(our_classes['label'][indices[i][max_index]])\n",
    "    list_matrix2.append(max_index)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_the_second_term_common_term(word1, word2):\n",
    "    syns1 = wn.synsets(word1)\n",
    "    syns2 = wn.synsets(word2)\n",
    "    new_word2 = generate_noun_for_adj(word2, embeddings_index)\n",
    "    new_word1 = generate_noun_for_adj(word1, embeddings_index)\n",
    "    new_syns1 = None\n",
    "    new_syns2 = None\n",
    "\n",
    "    if new_word1 == \"\":\n",
    "        new_word1 = word1\n",
    "        new_syns1 = syns1\n",
    "    else:\n",
    "        new_syns1 = wn.synsets(new_word1)\n",
    "\n",
    "    if new_word2 == \"\":\n",
    "        new_word2 = word2\n",
    "        new_syns2 = syns2\n",
    "    else:\n",
    "        new_syns2 = wn.synsets(new_word2)\n",
    "\n",
    "    while syns1 == [] and len(word1.split(\"_\"))>1:\n",
    "        word1 = \"_\".join(word1.split(\"_\")[:-1])\n",
    "        syns1 = wn.synsets(word1)\n",
    "    \n",
    "\n",
    "    for s1 in syns1:\n",
    "        for s2 in syns2:\n",
    "            lch = s1.lowest_common_hypernyms(s2)\n",
    "            if not lch:\n",
    "                continue\n",
    "\n",
    "            common_name = lch[0].lemmas()[0].name()\n",
    "            if common_name == word2 or word2 in common_name:\n",
    "                return 1\n",
    "    if(word1!=new_word1 or word2!=new_word2):\n",
    "        for s1 in new_syns1:\n",
    "            for s2 in new_syns2:\n",
    "                lch = s1.lowest_common_hypernyms(s2)\n",
    "                if not lch:\n",
    "                    continue\n",
    "\n",
    "                common_name = lch[0].lemmas()[0].name()\n",
    "\n",
    "                if common_name == word2 or word2 in common_name:\n",
    "                    return 1\n",
    "\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_terms = set()\n",
    "correlated_terms_with_each_word = {}\n",
    "with  open('correlated_terms.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split(\" \")\n",
    "        score = np.dot(embeddings_index[values[0]].reshape(1,-1), embeddings_index[values[1]].reshape(1,-1).T)\n",
    "        if score > 0.3 and is_the_second_term_common_term(values[1], values[0]) == -1:\n",
    "            correlated_terms.add((values[0], values[1]))   \n",
    "            if values[0] not in correlated_terms_with_each_word.keys() and values[1] in embeddings_index.keys():\n",
    "                correlated_terms_with_each_word[values[0]] = [values[1]]\n",
    "            elif values[1] in embeddings_index.keys():\n",
    "                correlated_terms_with_each_word[values[0]].append(values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specificity(term_):\n",
    "    synsets_ = wn.synsets(term_)\n",
    "    if not synsets_:\n",
    "        return 0\n",
    "    return np.max([len(paths) for paths in synsets_[0].hypernym_paths()])\n",
    "def get_specificity2(term_):\n",
    "    synsets_ = wn.synsets(term_)\n",
    "    if not synsets_:\n",
    "        return 0\n",
    "    return np.min([len(paths) for paths in synsets_[0].hypernym_paths()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Handling of Genericity, Specificity, and Semantic Relationships\n",
    "\n",
    "To maximize label precision and robustness, we implement fine-grained control over matching and scoring based on term specificity, genericity, and semantic relationships.\n",
    "\n",
    "\n",
    "#### Detecting Very Generic Terms (WordNet-Based)\n",
    "\n",
    "- We use **WordNet hierarchies** to identify overly generic terms:\n",
    "  - Terms appearing **≥ 70 times** at shallow depth or **≥ 450 times** overall are flagged.\n",
    "  - Additional heuristics detect generic words like `prepared`, `object`, `thing`, `matter`.\n",
    "- **Very generic terms** are excluded early to prevent noisy matches.\n",
    "\n",
    "\n",
    "#### Differentiating Generic (`__`) vs Super Generic (`::`) Terms\n",
    "\n",
    "When analyzing label components:\n",
    "\n",
    "- **Generic terms (`__`)**:  \n",
    "  ➔ Moderately broad but still potentially useful.  \n",
    "  ➔ Example: `furniture manufacturing__1` — \"manufacturing\" is generic, but can act as a differentiator in context.\n",
    "\n",
    "- **Super generic terms (`::`)**:  \n",
    "  ➔ Extremely broad concepts spanning many fields.  \n",
    "  ➔ Example: `bakery production::1` — \"production\" is vague and adds less specificity.\n",
    "\n",
    "\n",
    "#### Specificity-Based Matching Adjustment\n",
    "\n",
    "- **Dynamic thresholding**:\n",
    "  - For **specific critical terms** (e.g., `\"veterinary\"` in `\"veterinary clinic\"`), we require **higher cosine similarity** to accept a match.\n",
    "  - For **generic and super generic terms**, we allow **lower cosine similarities** during matching.\n",
    "- **Scoring adjustment**:\n",
    "  - **Generic terms (`__`)** are **moderately downweighted**.\n",
    "  - **Super generic terms (`::`)** are **heavily downweighted** inside the final label score.\n",
    "  - **Specific, highly meaningful words dominate** the scoring for more accurate label assignment.\n",
    "\n",
    "#### Generic-Aware Semantic Checking\n",
    "\n",
    "During token-to-label matching:\n",
    "\n",
    "- **Priority rule**:\n",
    "  - If a token has **high cosine similarity** or is **semantically correlated** (via WordNet/ConceptNet), we **keep** the match.\n",
    "  - Otherwise, if the token is **too generic relative to the label component**, it is **excluded**, even if its raw cosine similarity is acceptable.\n",
    "- We use custom methods like `is_the_second_term_common_term()` for semantic genericity detection.\n",
    "\n",
    "\n",
    "#### Boosting Related Words and Handling Antonyms\n",
    "\n",
    "- **Related words** are expanded dynamically using:\n",
    "  - **WordNet** `is-a` relationships.\n",
    "  - **Embedding similarity** (threshold ≥ 0.4).\n",
    "- **Antonyms** are explicitly detected and **penalized** to prevent assigning contradictory labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "antonyms_words = set()\n",
    "antonyms_words_for_each_word = {}\n",
    "\n",
    "isA_relationship = {}\n",
    "isA_reverse_relationship = {}\n",
    "isA_reverse_relationship_even_invalid_terms = {}\n",
    "\n",
    "\n",
    "hasContext_relationship = {}\n",
    "hasContext_reverse_relationship = {}\n",
    "\n",
    "\n",
    "related_terms_for_antonyms = {}\n",
    "convertPluralToSingular = {}\n",
    "occuranceTermForParent = {}\n",
    "\n",
    "with open(\"../inputs/filtered_assertions.txt\") as f:\n",
    "    rows = f.read().split(\"\\n\")\n",
    "    for row in rows:\n",
    "        if row==\"\":\n",
    "            continue\n",
    "        words = row.split(\" \")\n",
    "        if (words[0]==\"/r/Antonym\"):\n",
    "            word1 = words[1]\n",
    "            word2 = words[2]\n",
    "            antonyms_words.add((word1, word2))\n",
    "            if word1 not in antonyms_words_for_each_word.keys():\n",
    "                antonyms_words_for_each_word[word1] = [word2]\n",
    "            else:\n",
    "                antonyms_words_for_each_word[word1].append(word2)\n",
    "            \n",
    "            if word2 not in antonyms_words_for_each_word.keys():\n",
    "                antonyms_words_for_each_word[word2] = [word1]\n",
    "            else:\n",
    "                antonyms_words_for_each_word[word2].append(word1)\n",
    "    \n",
    "        if (words[0]==\"/r/IsA\"):\n",
    "            word1 = words[1]\n",
    "            word2 = words[2]\n",
    "            list_full = isA_reverse_relationship_even_invalid_terms.get(word2, set())\n",
    "            list_full.add(word1)\n",
    "            isA_reverse_relationship_even_invalid_terms[word2] = list_full\n",
    "\n",
    "            if word1.startswith(f\"{words[2]}_\"):\n",
    "                \n",
    "                if word1.split(f\"{words[2]}_\")[1] in embeddings_index.keys() and words[2] in embeddings_index.keys():\n",
    "                    word_temp = word1.split(f\"{words[2]}_\")[1]\n",
    "                    value = np.dot(embeddings_index[word_temp], embeddings_index[words[2]].T)\n",
    "\n",
    "                    if value > 0.35:\n",
    "                        word1 = word1.split(f\"{words[2]}_\")[1]\n",
    "            elif word1.endswith(f\"_{words[2]}\"): \n",
    "                if word1.split(f\"_{words[2]}\")[0] in embeddings_index.keys() and words[2] in embeddings_index.keys(): \n",
    "                    word_temp = word1.split(f\"_{words[2]}\")[0]\n",
    "                    value = np.dot(embeddings_index[word_temp], embeddings_index[words[2]].T)\n",
    "\n",
    "                    if value > 0.35:\n",
    "                        word1 = word1.split(f\"_{words[2]}\")[0]\n",
    "          \n",
    "        \n",
    "            \n",
    "\n",
    "            if word1 in embeddings_index.keys() and word2 in embeddings_index.keys():\n",
    "                list_is = isA_relationship.get(word1, set())\n",
    "                list_is.add(word2)\n",
    "                isA_relationship[word1] = list_is\n",
    "                \n",
    "                list_is_rev = isA_reverse_relationship.get(word2, set())\n",
    "                \n",
    "                list_is_rev.add(word1)\n",
    "                isA_reverse_relationship[word2] = list_is_rev\n",
    "                occuranceTermForParent[word2] = len(list_is_rev)\n",
    "                    \n",
    "             \n",
    "        \n",
    "        if (words[0]==\"/r/FormOf\"):\n",
    "            word1 = words[1]\n",
    "            word2 = words[2]\n",
    "            convertPluralToSingular[word1] = word2\n",
    "\n",
    "        if (words[0]==\"/r/HasContext\"):\n",
    "            word1 = words[1]\n",
    "            word2 = words[2]\n",
    "            if word1 not in hasContext_relationship.keys():\n",
    "                hasContext_relationship[word1] = [word2]\n",
    "            else:\n",
    "                hasContext_relationship[word1].append(word2)\n",
    "            if word2 not in hasContext_reverse_relationship.keys():\n",
    "                hasContext_reverse_relationship[word2] = [word1]\n",
    "            else:\n",
    "                hasContext_reverse_relationship[word2].append(word1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque \n",
    "\n",
    "def bfs(dict_terms, start):\n",
    "    visited = []\n",
    "    queue = deque([(start, 0)])\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node[0] not in visited:\n",
    "\n",
    "            visited.append(node[0])\n",
    "        \n",
    "            if node[0] not in dict_terms.keys():\n",
    "                return node[1]                \n",
    "            for neighbor in dict_terms[node[0]]:\n",
    "                if neighbor not in visited:\n",
    "                    queue.append((neighbor, node[1]+1)) \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_terms_for_parent = occuranceTermForParent.copy()\n",
    "occuranceTermForParent = dict(sorted(occuranceTermForParent.items(), key=lambda item: item[1]))\n",
    "occuranceTermForParent = {k: v for k, v in occuranceTermForParent.items() if (v>=70 and  bfs(isA_relationship,k)<=1) or (v >=450 and bfs(isA_relationship,k)<=2) }\n",
    "very_generic_terms = set([k for k, _ in occuranceTermForParent.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_for_look_for_beginning = {\"prepared\", \"shaped\", \"solid\", \"liquid\", \"long\", \"light\", \"dirty\", \"busy\", \"organized\",\"constructed\", \"processed\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_for_look_for_beginning = {\"prepared\", \"shaped\", \"processed\"}\n",
    "terms_for_look_for_end = {\"object\", \"thing\", \"fluid\", \"thing\", \"matter\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_term in original_terms_for_parent.keys():\n",
    "    \n",
    "    if ((original_term in occuranceTermForParent and occuranceTermForParent[original_term]>4) or (original_term not in occuranceTermForParent.keys())) and len(original_term.split(\"_\")) > 1 and get_specificity(original_term)<4:\n",
    "        is_generic_term = original_term.split(\"_\")[0] in terms_for_look_for_beginning or (original_term.split(\"_\")[-1] in terms_for_look_for_end and get_specificity(\"_\".join(original_term.split(\"_\")[:-1])) >1 and get_specificity(\"_\".join(original_term.split(\"_\")[:-1]))<5) \n",
    "\n",
    "        if is_generic_term:\n",
    "            very_generic_terms.add(original_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words_to_a_word_for_antonyms = {}\n",
    "related_words_to_a_word_similarity = {}\n",
    "\n",
    "for word in isA_relationship:\n",
    "    set_element = isA_relationship[word]\n",
    "    set_element_temp = set_element.copy()\n",
    "    \n",
    "    for word2 in set_element:\n",
    "        if word==word2:\n",
    "            continue\n",
    "        set_is_A_relationship = {}\n",
    "        set_is_A_reverse_relationship = {}\n",
    "\n",
    "        if word2 in isA_relationship.keys():\n",
    "            set_is_A_relationship = isA_relationship[word2]\n",
    "\n",
    "        if word2 in isA_reverse_relationship.keys()  and word2 not in very_generic_terms:\n",
    "            set_is_A_reverse_relationship = isA_reverse_relationship[word2]\n",
    "        set_element_temp.update(set_is_A_relationship)\n",
    "\n",
    "        set_element_temp.update(set_is_A_reverse_relationship)\n",
    "\n",
    "        if word in set_element_temp:\n",
    "            set_element_temp.remove(word)\n",
    "\n",
    "    if set_element_temp !=set():\n",
    "\n",
    "        list_words = list(set_element_temp)\n",
    "        current_word_embedding = (embeddings_index[word]).reshape(1,-1)\n",
    "        faiss.normalize_L2(current_word_embedding)\n",
    "        all_word_embeddings = np.array([embeddings_index[wd] if wd in embeddings_index else np.zeros(300) for wd in list_words]).astype(np.float32, order='C')\n",
    "        faiss.normalize_L2(all_word_embeddings)\n",
    "        our_values = np.dot(current_word_embedding, all_word_embeddings.T)[0]\n",
    "        list_words_zip_for_antonyms = list(filter(lambda x: x[1]>=0.4, list(zip(list_words, our_values))))\n",
    "        set_element_temp = set(map(lambda x: x[0], list_words_zip_for_antonyms))\n",
    "\n",
    "    \n",
    "    if word not in related_words_to_a_word_for_antonyms.keys():\n",
    "        related_words_to_a_word_for_antonyms[word] = set_element_temp\n",
    "    else:\n",
    "        set_word = related_words_to_a_word_for_antonyms[word]\n",
    "\n",
    "        set_word.update(set_element_temp)\n",
    "        related_words_to_a_word_for_antonyms[word] = set_word\n",
    "\n",
    "    if word not in related_words_to_a_word_similarity.keys():\n",
    "        related_words_to_a_word_similarity[word] = set_element_temp\n",
    "    else:\n",
    "        set_word_sim = related_words_to_a_word_similarity[word] \n",
    "\n",
    "        set_word_sim.update(set_element_temp)\n",
    "        related_words_to_a_word_similarity[word] = set_word_sim\n",
    "\n",
    "\n",
    "related_words_to_a_word_for_antonyms2 = {}\n",
    "for word in isA_reverse_relationship:\n",
    "    set_element = isA_reverse_relationship[word]\n",
    "    set_element_temp = set_element.copy()\n",
    "    for word_related in set_element:\n",
    "        set_is_A_relationship = {}\n",
    "        set_is_A_reverse_relationship = {}\n",
    "\n",
    "        if word_related in isA_relationship.keys():\n",
    "            set_is_A_relationship = isA_relationship[word_related]\n",
    "\n",
    "        if word_related in isA_reverse_relationship.keys() and word_related not in very_generic_terms:\n",
    "            set_is_A_reverse_relationship = isA_reverse_relationship[word_related]\n",
    "        \n",
    "        set_element_temp.update(set_is_A_relationship)\n",
    "        set_element_temp.update(set_is_A_reverse_relationship)\n",
    "        if word in set_element_temp:\n",
    "            set_element_temp.remove(word)\n",
    "\n",
    "\n",
    "    if set_element_temp !=set():\n",
    "        list_words = list(set_element_temp)\n",
    "        current_word_embedding = (embeddings_index[word]).reshape(1,-1)\n",
    "        faiss.normalize_L2(current_word_embedding)\n",
    "        all_word_embeddings = np.array([embeddings_index[wd] if wd in embeddings_index else np.zeros(300) for wd in list_words]).astype(np.float32, order='C')\n",
    "        faiss.normalize_L2(all_word_embeddings)\n",
    "        our_values = np.dot(current_word_embedding, all_word_embeddings.T)[0]\n",
    "        list_words_zip_with_antonyms = list(filter(lambda x: x[1]>=0.4, list(zip(list_words, our_values))))\n",
    "        set_element_temp = set(map(lambda x: x[0], list_words_zip_with_antonyms))\n",
    "\n",
    "    if word not in related_words_to_a_word_for_antonyms2.keys():\n",
    "        related_words_to_a_word_for_antonyms2[word] = set_element_temp\n",
    "    else:\n",
    "        set_word = related_words_to_a_word_for_antonyms2[word]\n",
    "        set_word.update(set_element_temp)\n",
    "        related_words_to_a_word_for_antonyms2[word] = set_word\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value_word in related_words_to_a_word_similarity.keys():\n",
    "    list_value_word = related_words_to_a_word_similarity[value_word]\n",
    "    if value_word in isA_reverse_relationship.keys():\n",
    "        for wd in isA_reverse_relationship[value_word]:\n",
    "            list_value_word = set(filter(lambda x: x if wd not in x else '', related_words_to_a_word_similarity[value_word]))\n",
    "            related_words_to_a_word_similarity[value_word] = list_value_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(100):\n",
    "       \n",
    "        if max_value - list_matrix[i][j][0] <= 0.20:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "    final_list.append(list111)\n",
    "    elements_filtered.append(row_filtered)    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = [\"\"] * len(divided_words)\n",
    "original_domain = [\"\"] * len(divided_words)\n",
    "modifier = [\"\"] * len(divided_words)\n",
    "original_modifier = [\"\"] * len(divided_words)\n",
    "core = [\"\"] * len(divided_words)\n",
    "original_core = [\"\"] * len(divided_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(divided_words)):\n",
    "    all_words = divided_words.at[i,'new_label_with_categories'].split(\" \")\n",
    "    domain[i], modifier[i], core[i] = all_words\n",
    "    original_domain[i] = domain[i].split(\"__\")[0].split(\"::\")[0]\n",
    "    original_core[i] = core[i].split(\"__\")[0].split(\"::\")[0]\n",
    "    original_modifier[i] = modifier[i].split(\"__\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all = []\n",
    "list_elements = []\n",
    "for i in range(len(our_classes)):\n",
    "    scores = []\n",
    "    elements = []\n",
    "    a, b, c = 0.3, 0.4, 0.3\n",
    "    if modifier[i]==\"0\":\n",
    "        a, b, c = 0.325, 0, 0.275\n",
    "    if len(domain[i].split(\"__\"))>1:\n",
    "        a*=0.7\n",
    "\n",
    "    if len(modifier[i].split(\"__\"))>1:\n",
    "        b*=0.75\n",
    "    \n",
    "    if len(domain[i].split(\"::\"))>1:\n",
    "        a*=0.4\n",
    "    if len(core[i].split(\"__\"))>1:\n",
    "        c*=0.7\n",
    "\n",
    "    if len(core[i].split(\"::\"))>1:\n",
    "        c*=0.4\n",
    "\n",
    "    if modifier[i] == \"0\" and core[i] == \"0\":\n",
    "        val_score = 0.05 * a/a\n",
    "        if len(original_domain[i].split(\"_\")) > 2:\n",
    "            val_score *=1.125\n",
    "        scores.append(val_score)\n",
    "        elements.append(original_domain[i])\n",
    "\n",
    "    elif modifier[i] == \"0\":\n",
    "        val_score1 = 0.065 * a/(a+c)\n",
    "        val_score2 = 0.065 * c/(a+c)\n",
    "        if len(original_domain[i].split(\"_\")) > 2:\n",
    "            val_score1 *=1.125\n",
    "        if len(original_core[i].split(\"_\")) > 2:\n",
    "            val_score2 *=1.125\n",
    "        scores.append(val_score1)\n",
    "        scores.append(val_score2)\n",
    "        elements.append(original_domain[i])\n",
    "        elements.append(original_core[i])\n",
    "    else:\n",
    "        val_score1 = 0.08 * a\n",
    "        val_score2 = 0.08 * b\n",
    "        val_score3 = 0.08 * c\n",
    "        scores.append(val_score1)\n",
    "        scores.append(val_score2)\n",
    "        scores.append(val_score3)\n",
    "        elements.append(original_domain[i])\n",
    "        elements.append(original_modifier[i])\n",
    "        elements.append(original_core[i])\n",
    "\n",
    "    if f\"{original_domain[i]}_{original_modifier[i]}\" in embeddings_index.keys() and modifier[i]==original_modifier[i] and original_domain[i]==domain[i]:\n",
    "        elements = []\n",
    "        elements.append(f\"{original_domain[i]}_{original_modifier[i]}\")\n",
    "        elements.append(original_core[i])\n",
    "        original_domain[i] = f\"{original_domain[i]}_{original_modifier[i]}\"\n",
    "        domain[i] = original_domain[i]\n",
    "        modifier[i] = \"0\"\n",
    "        original_modifier[i] = \"0\"\n",
    "        \n",
    "    if f\"{original_domain[i]}_{original_core[i]}\" in embeddings_index.keys() and original_domain[i] == domain[i] and original_core[i]==core[i]:\n",
    "        elements = []\n",
    "\n",
    "        elements.append(f\"{original_domain[i]}_{original_core[i]}\")\n",
    "        original_domain[i] = f\"{original_domain[i]}_{original_core[i]}\"\n",
    "        domain[i] = original_domain[i]\n",
    "        modifier[i] = \"0\"\n",
    "        original_modifier[i] = \"0\"\n",
    "        core[i] = \"0\"\n",
    "        original_core[i] = \"0\"\n",
    "    if f\"{original_modifier[i]}_{original_core[i]}\" in embeddings_index.keys()  and original_core[i]==core[i] and original_modifier[i]==modifier[i]:\n",
    "        elements = []\n",
    "        elements.append(original_domain[i])\n",
    "        elements.append(f\"{original_modifier[i]}_{original_core[i]}\")\n",
    "        original_core[i] = f\"{original_modifier[i]}_{original_core[i]}\"\n",
    "        core[i] = original_core[i]\n",
    "        original_modifier[i] = \"0\"\n",
    "        modifier[i]=\"0\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    scores_all.append(scores)\n",
    "    list_elements.append(elements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_list(word_key_label, wd, index_label, dict_words_same_category):\n",
    "    if (word_key_label, wd) not in antonyms_words and ((word_key_label, wd) not in dict_words_same_category or dict_words_same_category[(word_key_label, wd)]==-1):\n",
    "        filtered_similarity_list = set()\n",
    "        modify_word=wd\n",
    "        if(wd not in related_words_to_a_word_similarity and wd in convertPluralToSingular):\n",
    "            modify_word = convertPluralToSingular[wd]\n",
    "        if modify_word in related_words_to_a_word_similarity:\n",
    "            filtered_similarity_list = set(filter(lambda x: f\"_{word_key_label}_\" in x or word_key_label == x or f\"{word_key_label}_\" == x[0:(len(word_key_label)+1)] or f\"_{word_key_label}\" in x, related_words_to_a_word_similarity[modify_word]))\n",
    "        is_term_generic = False\n",
    "        is_term_generic = (word_key_label == original_domain[index_label] and original_domain[index_label]==domain[index_label])\n",
    "        is_term_generic = is_term_generic or (word_key_label == original_modifier[index_label])\n",
    "        is_term_generic = is_term_generic or (word_key_label == original_core[index_label] and original_core[index_label]==core[index_label])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if filtered_similarity_list != set() and is_term_generic:\n",
    "            specialize_wd =  get_specificity2(wd)\n",
    "            specialize_word_key_label =  get_specificity2(word_key_label)\n",
    "        \n",
    "                \n",
    "            is_term_more_generic_than_the_key_word = (specialize_wd >= specialize_word_key_label)\n",
    "            if is_term_more_generic_than_the_key_word == False:\n",
    "                specialize_wd =  get_specificity(wd)\n",
    "                specialize_word_key_label =  get_specificity(word_key_label)\n",
    "                    \n",
    "                is_term_more_generic_than_the_key_word = specialize_wd >= specialize_word_key_label\n",
    "            filtered_similarity_list = set(filter(lambda x: is_the_second_term_common_term(x, wd)==-1, filtered_similarity_list))\n",
    "            \n",
    "            return filtered_similarity_list\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: Handling `non` Labels\n",
    "\n",
    "Labels starting with `non` (e.g., `non_alcoholic`) are tricky because they often score high with their opposite meanings.\n",
    "\n",
    "#### Strategy:\n",
    "\n",
    "- **First**, if the label itself (`non_alcoholic`) isn't found:\n",
    "  - Check if the term is **closer to an antonym** (cosine > 0.7).\n",
    "  - If the antonym is closer than the label, **penalize** the match.\n",
    "\n",
    "- **Second**, if needed:\n",
    "  - Expand to **similar words of the antonym**.\n",
    "  - Filter out confusing terms (cosine > 0.7 with the label).\n",
    "\n",
    "#### Goal:\n",
    "- Avoid misclassifying opposites.\n",
    "- Keep only labels truly matching the non-term meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_matrix_original = []\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list_matrix_original.append(list_matrix[i].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Similarity Recovery via Parent-Child Heuristics\n",
    "\n",
    "In some cases, label terms may not appear extremely specific but still carry meaningful business context. These often show **medium cosine similarity** (0.3–0.4) with company tokens.\n",
    "\n",
    "We retain such terms when:\n",
    "\n",
    "- The **parent term** (e.g., `laboratory`) has only a moderate similarity,\n",
    "- But a more **specialized child term** (e.g., `medical_laboratory`) shows stronger contextual alignment,\n",
    "- And the term has a **moderate specificity score** (between 8 and 12), indicating it's precise but not overly niche.\n",
    "\n",
    "🔍 **Example**:\n",
    "- Parent: `laboratory` (cosine ~0.35)\n",
    "- Child: `medical_laboratory` (cosine ~0.52) → More relevant in a healthcare-related context.\n",
    "\n",
    "This heuristic allows us to **recover contextually meaningful labels** that might otherwise be excluded by rigid similarity thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list_companies = []\n",
    "dict_words_same_category = {}\n",
    "specifity_word = {}\n",
    "is_the_set_valid_or_not = {}\n",
    "is_the_set_valid_or_not2 = {}\n",
    "\n",
    "\n",
    "dict_words_dif_category = {}\n",
    "list_matrix_elements_val = np.zeros((len(df['description']), len(label_embeddings), 100))\n",
    "list_matrix_core_val = np.zeros((len(df['description']), len(label_embeddings), 100))\n",
    "\n",
    "antonyms_words_for_companies = set()\n",
    "are_antonyms_with_word = {}\n",
    "are_antonyms = set()\n",
    "noun_for_adj_dict = {}\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list111 = []\n",
    "    max_origin = list_matrix[i][0][0]\n",
    "    max_val = list_matrix[i][j][0]\n",
    "    element = {}\n",
    "\n",
    "\n",
    "    for j in range(len(elements_filtered[i])):\n",
    "       \n",
    "        if(max_val - list_matrix[i][j][0]>0.15):\n",
    "            continue\n",
    "        \n",
    "        index_label = list_matrix[i][j][1]\n",
    "        word_most_specific = original_domain[index_label]\n",
    "\n",
    "        is_valid_term=0\n",
    "        should_discard_label = 0\n",
    "\n",
    "        list_words = []\n",
    "        word_most_specific1 = None\n",
    "        all_word_embeddings = None\n",
    "        domain_values = None\n",
    "        core_values = None\n",
    "        modifier_values = None\n",
    "\n",
    "        word_most_specific1 = word_most_specific.replace(\"-\",\"_\")\n",
    "        we1 = (embeddings_index[original_domain[index_label]]).reshape(1,-1)\n",
    "        faiss.normalize_L2(we1)\n",
    "        all_word_embeddings = np.array([embeddings_index[wd.replace(\"-\",\"_\")]  for wd in our_words['new_col2'][i]]).astype(np.float32, order='C')\n",
    "        faiss.normalize_L2(all_word_embeddings)\n",
    "        domain_values = np.dot(we1, all_word_embeddings.T)[0]\n",
    "        total_value = domain_values\n",
    "        core_values = np.zeros(0)\n",
    "        if np.max(domain_values) < 0.2 and original_domain[index_label]==domain[index_label]:\n",
    "                should_discard_label=1\n",
    "\n",
    "\n",
    "        we2 = None\n",
    "        we3 = None\n",
    "\n",
    "        if(core[index_label]!=\"0\" and should_discard_label==0):\n",
    "            we2 = (embeddings_index[original_core[index_label]]).reshape(1,-1)\n",
    "            faiss.normalize_L2(we2)\n",
    "            core_values = np.dot(we2, all_word_embeddings.T)[0]\n",
    "\n",
    "\n",
    "            if np.max(core_values) < 0.2 and original_core[index_label]==core[index_label]:\n",
    "                should_discard_label=1\n",
    "\n",
    "            if original_core[index_label] == core[index_label]:\n",
    "                total_value = domain_values * 0.5 + core_values * 0.5\n",
    "            elif len(core[index_label].split(\"__\"))>1:\n",
    "                total_value = domain_values * 0.7 + core_values * 0.3\n",
    "            elif len(core[index_label].split(\"::\"))>1:\n",
    "                total_value = domain_values * 0.9 + core_values * 0.1\n",
    "        \n",
    "        if(modifier[index_label]!=\"0\" and should_discard_label==0):\n",
    "            we3 = (embeddings_index[original_modifier[index_label]]).reshape(1,-1)\n",
    "            faiss.normalize_L2(we3)\n",
    "            modifer_values = np.dot(we3, all_word_embeddings.T)[0]\n",
    "\n",
    "\n",
    "            if np.max(modifer_values) < 0.2:\n",
    "                should_discard_label=1\n",
    "            if original_core[index_label] != core[index_label] and original_domain[index_label]!=domain[index_label] and len(core[index_label].split(\"::\"))>1:\n",
    "                total_value = domain_values * 0.235 + modifer_values * 0.665 + core_values * 0.1\n",
    "            elif original_core[index_label] != core[index_label] and original_domain[index_label]!=domain[index_label]:\n",
    "                total_value = domain_values * 0.2 + modifer_values * 0.6 + core_values * 0.2\n",
    "            elif original_core[index_label] != core[index_label] and len(core[index_label].split(\"::\"))>1:\n",
    "                total_value = domain_values * 0.425 + modifer_values * 0.475 + core_values * 0.1\n",
    "            elif original_core[index_label] != core[index_label] and len(core[index_label].split(\"__\"))>1:\n",
    "                if original_modifier[index_label] == modifier[index_label]:\n",
    "                    total_value = domain_values * 0.35 + modifer_values * 0.45 + core_values * 0.2\n",
    "                else:\n",
    "                     total_value = domain_values * 0.55 + modifer_values * 0.25 + core_values * 0.2\n",
    "            elif original_domain[index_label] != domain[index_label]:\n",
    "                total_value = domain_values * 0.2 + modifer_values * 0.425 + core_values * 0.375\n",
    "            else:\n",
    "                total_value = domain_values * 0.3 + modifer_values * 0.4 + core_values * 0.3\n",
    "        if len(total_value) < 100:\n",
    "            mult = 1\n",
    "            \n",
    "            list_matrix_elements_val[i][index_label] = np.pad(total_value, (0, 100 - len(total_value)), mode='constant')\n",
    "            list_matrix_core_val[i][index_label] = np.pad(core_values, (0, 100 - len(core_values)), mode='constant')\n",
    "\n",
    "        if(np.max(total_value) < 0.25):\n",
    "            should_discard_label=1\n",
    "\n",
    "        is_value_antonym = 0\n",
    "\n",
    "        \n",
    "        if domain[index_label] != original_domain[index_label] and modifier[index_label]!='0':\n",
    "\n",
    "\n",
    "            if should_discard_label==0:\n",
    "                domain_values = modifer_values.copy()\n",
    "            else:\n",
    "                we3 = (embeddings_index[original_modifier[index_label]]).reshape(1,-1)\n",
    "                faiss.normalize_L2(we3)\n",
    "                domain_values = np.dot(we3, all_word_embeddings.T)[0]\n",
    "\n",
    "            word_most_specific = original_modifier[index_label]\n",
    "            word_most_specific1 = word_most_specific.replace(\"-\",\"_\")\n",
    "        elif domain[index_label] != original_domain[index_label] and core[index_label]!='0' and core[index_label]==original_core[index_label]:\n",
    "            if should_discard_label==0:\n",
    "                domain_values = core_values.copy()\n",
    "            else:\n",
    "                we3 = (embeddings_index[original_core[index_label]]).reshape(1,-1)\n",
    "                faiss.normalize_L2(we3)\n",
    "                domain_values = np.dot(we3, all_word_embeddings.T)[0]\n",
    "            word_most_specific = core[index_label]\n",
    "            word_most_specific1 = word_most_specific.replace(\"-\",\"_\")\n",
    "        elif domain[index_label] != original_domain[index_label]:\n",
    "            continue\n",
    "\n",
    "\n",
    "        max_our_value = np.max(domain_values)\n",
    "        all_values = domain_values\n",
    "        best_antonyms_word = ''\n",
    "        scores_max_list_antonyms = []\n",
    "        is_not_antonyms = 0\n",
    "        if \"non\" == word_most_specific[0:3]:\n",
    "            word_most_specific1 = word_most_specific\n",
    "            if word_most_specific1 not in antonyms_words_for_each_word.keys() and word_most_specific in convertPluralToSingular.keys() and  convertPluralToSingular[word_most_specific1] in antonyms_words_for_each_word.keys():\n",
    "                    word_most_specific1 = convertPluralToSingular[word_most_specific1]\n",
    "            \n",
    "\n",
    "            if word_most_specific1 in antonyms_words_for_each_word.keys():\n",
    "                \n",
    "                filter_antonyms_if_they_are_there = any(x in our_categories_niche_words['niche_plus_cat'][i] for x in antonyms_words_for_each_word[word_most_specific1])\n",
    "                filter_antonyms_if_they_are_there_most_important = any(x in most_important_words[i] for x in antonyms_words_for_each_word[word_most_specific1])\n",
    "                filter_similar_if_they_are_there_most_important = word_most_specific in most_important_words[i] or word_most_specific1 in most_important_words[i]\n",
    "                if filter_antonyms_if_they_are_there or (filter_antonyms_if_they_are_there_most_important and not filter_similar_if_they_are_there_most_important):\n",
    "                    is_not_antonyms = 2\n",
    "                elif not filter_similar_if_they_are_there_most_important:\n",
    "                    antonyms_word_for_that =  embeddings_index.get(antonyms_words_for_each_word[word_most_specific1][0], np.zeros(300))\n",
    "                    most_important_word_emb =  np.array([embeddings_index.get(wd, np.zeros(300)) if wd[0:3]!=\"non\" else np.zeros(300) for wd in most_important_words[i]], dtype=np.float32, order='C')\n",
    "                    faiss.normalize_L2(most_important_word_emb)\n",
    "                    values_most_imp_ant = np.dot(antonyms_word_for_that, most_important_word_emb.T)\n",
    "                    values_most_imp = np.dot(embeddings_index.get(word_most_specific, np.zeros(300)), most_important_word_emb.T)\n",
    "                    indices_list = range(len(values_most_imp))\n",
    "                    list_values_most_imp_ant =  list(values_most_imp_ant)\n",
    "                    list_values_most_imp = list(values_most_imp)\n",
    "\n",
    "\n",
    "                    indices_list = list(filter(lambda x: list_values_most_imp_ant[x]>0.7 and list_values_most_imp_ant[x] > list_values_most_imp[x], indices_list))\n",
    "                    for ind in indices_list:\n",
    "                        if list_values_most_imp_ant[ind]>0.7:\n",
    "                            index_val = list_values_most_imp_ant.index(list_values_most_imp_ant[ind])\n",
    "                            if word_most_specific not in are_antonyms_with_word:\n",
    "                                are_antonyms_with_word[word_most_specific] = [most_important_words[i][index_val]]\n",
    "                            else:\n",
    "                                are_antonyms_with_word[word_most_specific].append(most_important_words[i][index_val])\n",
    "                            is_not_antonyms = 2\n",
    "\n",
    "\n",
    "        is_antonyms = 0\n",
    "            \n",
    "        for idx, wd in enumerate(our_words['new_col2'][i]):\n",
    "            our_values = all_values[idx]\n",
    "\n",
    "\n",
    "            if core[index_label] != \"0\" and core[index_label] != original_core[index_label] and list_matrix_core_val[i][index_label][idx]>0.6 and list_matrix_core_val[i][index_label][idx]>list_matrix_elements_val[i][index_label][idx]:\n",
    "                continue\n",
    "            if scores_max_list_antonyms != []:\n",
    "                our_values_antonyms = scores_max_list_antonyms[idx]\n",
    "            else:\n",
    "                our_values_antonyms = 0\n",
    "            \n",
    "            \n",
    "            if(is_not_antonyms==2):\n",
    "                continue\n",
    "           \n",
    "            specificity_score=-1\n",
    "\n",
    "            if our_values > 0.85 and (scores_max_list_antonyms == [] or our_values >= our_values_antonyms):\n",
    "                is_valid_term=1\n",
    "                break\n",
    "        \n",
    "            if word_most_specific not in specifity_word.keys():\n",
    "                specificity_score = get_specificity(word_most_specific)\n",
    "                specifity_word[word_most_specific] = specificity_score\n",
    "            else:\n",
    "                specificity_score = specifity_word[word_most_specific]\n",
    "            \n",
    "            if scores_max_list_antonyms != [] and our_values < our_values_antonyms and our_values_antonyms!=0:\n",
    "                \n",
    "                are_antonyms.add((word_most_specific, wd))\n",
    "                \n",
    "                is_antonyms = 1  \n",
    "                continue\n",
    "            \n",
    "            if our_values > 0.5  and our_values<0.6 and (word_most_specific, wd) in dict_words_same_category.keys() and dict_words_same_category[(word_most_specific, wd)] == 1 and word_most_specific!=wd:\n",
    "                continue\n",
    "            \n",
    "\n",
    "\n",
    "            if our_values>0.5 and our_values<0.6 and (word_most_specific, wd) not in dict_words_same_category.keys() and (is_the_second_term_common_term(word_most_specific, wd) == 1) and word_most_specific!=wd:\n",
    "                dict_words_same_category[(word_most_specific, wd)] = 1\n",
    "                dict_words_same_category[(wd, word_most_specific)] = -1\n",
    "                continue\n",
    "\n",
    "            if word_most_specific in isA_relationship.keys() and wd in isA_relationship[word_most_specific]:\n",
    "                dict_words_same_category[(word_most_specific, wd)] = 1\n",
    "                dict_words_same_category[(wd, word_most_specific)] = -1\n",
    "                continue\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            if (wd, word_most_specific) not in antonyms_words:\n",
    "                the_set_is_valid=False\n",
    "                if(specificity_score>=8):\n",
    "                    if (word_most_specific, wd) not in is_the_set_valid_or_not.keys():\n",
    "                        each_words = get_sim_list(word_most_specific, wd, index_label, dict_words_same_category)\n",
    "                        the_set_is_valid = each_words != None and each_words != set()\n",
    "                        if the_set_is_valid == True:\n",
    "                            is_the_set_valid_or_not[(word_most_specific, wd)] = True\n",
    "                        else:\n",
    "                            is_the_set_valid_or_not[(word_most_specific, wd)] = False\n",
    "                    else:\n",
    "                        the_set_is_valid = is_the_set_valid_or_not[(word_most_specific, wd)]\n",
    "\n",
    "                    \n",
    "                    if our_values >= 0.3 and specificity_score<12 and our_values <= 0.4 and (word_most_specific, wd) not in is_the_set_valid_or_not2 and not the_set_is_valid:\n",
    "                        current_words = isA_reverse_relationship_even_invalid_terms.get(word_most_specific, set())\n",
    "\n",
    "                        if current_words != set():\n",
    "\n",
    "                            embeddings_words = np.array([embeddings_index.get(wd, np.mean([embeddings_index.get(wd2, np.zeros(300)) for wd2 in wd.split(\"_\")], axis=0)) for wd in current_words], dtype=np.float32, order='C')\n",
    "                    \n",
    "                            faiss.normalize_L2(embeddings_words)\n",
    "                            cur_list = list(np.dot(embeddings_words, embeddings_index[wd].T))\n",
    "\n",
    "                            cur_list_val = any(x>0.42 for x in cur_list)\n",
    "                            \n",
    "                            if cur_list_val:\n",
    "                                the_set_is_valid = True\n",
    "                              \n",
    "                                is_the_set_valid_or_not2[(word_most_specific, wd)] = True\n",
    "                            else:\n",
    "                                the_set_is_valid = False\n",
    "                                is_the_set_valid_or_not2[(word_most_specific, wd)] = False\n",
    "                        else:\n",
    "                            the_set_is_valid = False\n",
    "                            is_the_set_valid_or_not2[(word_most_specific, wd)] = False\n",
    "                    elif (word_most_specific, wd) in is_the_set_valid_or_not2.keys() and the_set_is_valid==False:\n",
    "                        the_set_is_valid = is_the_set_valid_or_not2[(word_most_specific, wd)]\n",
    "\n",
    "\n",
    "                \n",
    "               \n",
    "                \n",
    "                is_generic_term = (i in original_core and i not in core) or (i in original_domain and i not in domain)\n",
    "\n",
    "                \n",
    "                if specificity_score < 8 and our_values > 0.3:\n",
    "                    is_valid_term = 1 \n",
    "                    is_antonyms = 0           \n",
    "                    break\n",
    "                elif specificity_score < 12 and (our_values > 0.4 or the_set_is_valid):\n",
    "\n",
    "                    \n",
    "                    is_valid_term = 1\n",
    "                    is_antonyms = 0   \n",
    "                    break\n",
    "                elif specificity_score >= 12 and (our_values > 0.6 or (the_set_is_valid and our_values>0.42)):\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    if our_values > 0.75:\n",
    "                        is_valid_term=1\n",
    "                        is_antonyms = 0\n",
    "                        break\n",
    "                    is_very_specific = None\n",
    "                    is_second_term = None\n",
    "                    if (the_set_is_valid):\n",
    "                        if word_most_specific not in specifity_word.keys():\n",
    "                            specifity_word[word_most_specific] = get_specificity(word_most_specific)\n",
    "                        specifity1 = specifity_word[word_most_specific]\n",
    "                        if word_most_specific not in noun_for_adj_dict.keys():\n",
    "                            noun_for_adj_dict[word_most_specific] = generate_noun_for_adj(word_most_specific, embeddings_index)\n",
    "                        noun_for_adj = noun_for_adj_dict[word_most_specific]\n",
    "\n",
    "                        if noun_for_adj not in specifity_word.keys():\n",
    "                            specifity_word[noun_for_adj] = get_specificity(noun_for_adj)\n",
    "                        specifity1_5 = specifity_word[noun_for_adj]\n",
    "\n",
    "\n",
    "\n",
    "                        if wd not in specifity_word.keys():\n",
    "                            specifity_word[wd] = get_specificity(wd)\n",
    "                        specifity2 = specifity_word[wd]\n",
    "\n",
    "                        if wd not in noun_for_adj_dict.keys():\n",
    "                            noun_for_adj_dict[wd] = generate_noun_for_adj(wd, embeddings_index)\n",
    "                        noun_for_adj2 = noun_for_adj_dict[wd]\n",
    "\n",
    "                        if noun_for_adj2 not in specifity_word.keys():\n",
    "                            specifity_word[noun_for_adj2] = get_specificity(noun_for_adj2)\n",
    "                        specifity2_5 = specifity_word[noun_for_adj2]\n",
    "                        \n",
    "                        is_very_specific = max(specifity1, specifity1_5)\n",
    "                        is_second_term = max(specifity2, specifity2_5)\n",
    "\n",
    "\n",
    "                    if not the_set_is_valid or (not(is_very_specific==0 and is_second_term>0) and not(is_very_specific>0 and is_second_term<is_very_specific)):\n",
    "                        is_valid_term = 1\n",
    "                        is_antonyms = 0   \n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                is_value_antonym = 1\n",
    "        \n",
    "    \n",
    "              \n",
    "        if(is_valid_term==0 or should_discard_label==1):\n",
    "          \n",
    "            if(is_valid_term==0 and (is_value_antonym==1 or is_antonyms==1 or is_not_antonyms==2)):\n",
    "                antonyms_words_for_companies.add((word_most_specific, i))\n",
    "            list_matrix[i][j] = (-1, list_matrix[i][j][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list111 = []\n",
    "    max_origin = list_matrix[i][0][0]\n",
    "    max_val = list_matrix[i][j][0]\n",
    "    element = {}\n",
    "\n",
    "    for j in range(len(elements_filtered[i])):\n",
    "       \n",
    "        if(max_val - list_matrix[i][j][0]>0.15):\n",
    "            continue\n",
    "        \n",
    "        index_label = list_matrix[i][j][1]\n",
    "\n",
    "        if domain[index_label][0:3] == \"non\" and domain[index_label] in are_antonyms_with_word: \n",
    "\n",
    "\n",
    "            set_elements = set(are_antonyms_with_word[domain[index_label]])\n",
    "\n",
    "            set_elements_emb = np.array([embeddings_index[wd] for wd in set_elements], dtype=np.float32, order='C')\n",
    "            faiss.normalize_L2(set_elements_emb)\n",
    "            values = np.max(np.dot(embeddings_index[domain[index_label]], set_elements_emb.T))\n",
    "            if values > 0.8:\n",
    "                continue\n",
    "            set_elements_expanded = set()\n",
    "            for el in set_elements:\n",
    "                if el in related_words_to_a_word_for_antonyms2:\n",
    "                    \n",
    "                    set_elements_embeddings_emb = np.array([embeddings_index[wd] for wd in list(related_words_to_a_word_for_antonyms2[el])], dtype=np.float32, order='C')\n",
    "                    faiss.normalize_L2(set_elements_embeddings_emb)\n",
    "                    if domain[index_label] in antonyms_words_for_each_word.keys():\n",
    "                        antonyms_domain = antonyms_words_for_each_word[domain[index_label]][0]\n",
    "                    elif domain[index_label] in convertPluralToSingular and convertPluralToSingular[domain[index_label]] in antonyms_words_for_each_word.keys():\n",
    "                        antonyms_domain = antonyms_words_for_each_word[convertPluralToSingular[domain[index_label]]][0]\n",
    "                    our_values_set_elements = np.dot(embeddings_index[antonyms_domain], set_elements_embeddings_emb.T)\n",
    "                    range_list = range(len(our_values_set_elements))\n",
    "                    range_list_filtered = list(filter(lambda x: our_values_set_elements[x]>=0.45, range_list))\n",
    "                    \n",
    "\n",
    "\n",
    "                    for z in range_list_filtered:\n",
    "                        set_elements_expanded.add(list(related_words_to_a_word_for_antonyms2[el])[z])\n",
    "\n",
    "            if set_element == set():\n",
    "                continue\n",
    "            set_elements_filtered = set(filter(lambda x: x in most_important_words[i] or x in our_categories_niche_words['niche_plus_cat'][i], set_elements_expanded))\n",
    "            if set_elements_filtered == set():\n",
    "                continue\n",
    "            \n",
    "            set_elements_emb_anto = np.array([embeddings_index[wd] for wd in set_elements_filtered], dtype=np.float32, order='C')\n",
    "            values_anto = np.min(np.dot(embeddings_index[domain[index_label]], set_elements_emb_anto.T))\n",
    "            if values_anto > 0.7:\n",
    "                continue\n",
    "            list_matrix[i][j] = (-1, list_matrix[i][j][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list111 = []\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(30):\n",
    "        \n",
    "        if max_value - list_matrix[i][j][0] <= 0.3:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            \n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "    final_list.append(list111)\n",
    "    elements_filtered.append(row_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Related-to-Antonym Conflicts\n",
    "\n",
    "After we finish the main matching between company words and labels, we add one more filtering step to catch tricky cases where a company's description might **hint at the opposite** of what a label means — even if it's not a direct antonym.\n",
    "\n",
    "\n",
    "### How It Works\n",
    "\n",
    "- For each company, we go through its **top label candidates** (ranked by cosine similarity).\n",
    "- We use our prepared set (`set_antonyms_list_final`) to catch possible conflicts.\n",
    "- For every candidate label (up to 100):\n",
    "  - If the label already looks **pretty weak** compared to the best match (cosine difference > 0.2), we **skip it** — no point wasting time on low-scoring ones.\n",
    "  - If the **contextual similarity** (`context_matrix[i][index_label]`) is already high (> 0.4), we **skip it too** — the label is probably fine.\n",
    "- Otherwise:\n",
    "  - We check the **domain term** of the label.\n",
    "  - Using our graphs (`related_words_to_a_word_for_antonyms` and `related_words_to_a_word_for_antonyms2`), we look for company words that are **related to antonyms** of the label’s domain.\n",
    "    - Example: \"livestock\" is closely related to \"meat\" — even though they aren't direct opposites, they point to **different industries**.\n",
    "  - If we find a match like this, we **invalidate** the label by setting its score to -1.\n",
    "\n",
    "\n",
    "### Why We Do This\n",
    "\n",
    "Not all conflicts are obvious.  \n",
    "Sometimes words like \"livestock\" and \"meat\" seem close, but they **mean very different things** depending on context.  \n",
    "Without this step, we could accidentally assign labels that look correct based on similarity, but **completely miss the real meaning**.\n",
    "\n",
    "By checking for words **related to antonyms**, we catch these fuzzy mismatches and avoid bad predictions — while still being efficient and not stressing over weak candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list111 = []\n",
    "    max_origin = list_matrix[i][0][0]\n",
    "    max_val = list_matrix[i][j][0]\n",
    "    set_antonyms_list = set(filter(lambda x: x if x[1] == i else \"\", antonyms_words_for_companies))\n",
    "    set_antonyms_list_final = set(map(lambda x: x[0], set_antonyms_list ))\n",
    "    \n",
    "    if set_antonyms_list_final != set():\n",
    "        for j in range(100): \n",
    "            \n",
    "            if(max_val - list_matrix[i][j][0]>0.2):\n",
    "                continue\n",
    "            \n",
    "            index_label = list_matrix[i][j][1]\n",
    "\n",
    "            if context_matrix[i][index_label] > 0.4:\n",
    "                continue\n",
    "\n",
    "            word_most_specific = original_domain[index_label]\n",
    "            val = False\n",
    "            \n",
    "            if original_domain[index_label]==domain[index_label]:\n",
    "\n",
    "                for wd in set_antonyms_list_final:\n",
    "                    val = True\n",
    "                    \n",
    "                    if word_most_specific in related_words_to_a_word_for_antonyms:\n",
    "                        val = any(set(filter(lambda x:x == antonym_word or  f\"{antonym_word}_\" == x[0:len(antonym_word)+1] or f\"_{antonym_word}_\" in x or f\"_{antonym_word}\" == x[-(len(antonym_word)+1):0] , related_words_to_a_word_for_antonyms[word_most_specific])) != set() for antonym_word in set_antonyms_list_final)\n",
    "                        \n",
    "                    else:\n",
    "                        val = False\n",
    "                    \n",
    "                    if (val==False):\n",
    "                        if word_most_specific in related_words_to_a_word_for_antonyms2:                           \n",
    "                            val = any(set(filter(lambda x: f\"{antonym_word}_\" == x[0:len(antonym_word)+1] or f\"_{antonym_word}_\" in x or f\"_{antonym_word}\" == x[-(len(antonym_word)+1):0] or x == antonym_word, related_words_to_a_word_for_antonyms2[word_most_specific])) != set() for antonym_word in set_antonyms_list_final)\n",
    "                        \n",
    "                    if(val==True):\n",
    "                        break\n",
    "\n",
    "            \n",
    "                \n",
    "            if(val):\n",
    "                list_matrix[i][j] = (-1, list_matrix[i][j][1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_antonyms_list = set(filter(lambda x: x if x[1] == 8 else \"\", antonyms_words_for_companies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Top Candidate Labels\n",
    "\n",
    "As mentioned earlier, we only select **labels that have scores close to the highest-scoring label**.  \n",
    "This helps us narrow down the candidates to only the most relevant options and avoids including unrelated or weakly matching labels.\n",
    "\n",
    "Specifically, we define \"close\" as being within a **0.15 score difference** from the top label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(30):\n",
    "        if max_value - list_matrix[i][j][0] <= 0.2:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "      \n",
    "    \n",
    "    final_list.append(list111)\n",
    "    elements_filtered.append(row_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Grained Boosting Strategy for Token-Level Matching\n",
    "\n",
    "In our matching pipeline, we apply **dynamic boosting** based on the strength of each token match:\n",
    "\n",
    "#### Boosting Based on Cosine Similarity\n",
    "\n",
    "- If the cosine similarity is **above 0.68**, we apply a **strong boost** to the label score.\n",
    "- If the similarity is between **0.45 and 0.68**, we apply a **moderate boost**.\n",
    "- If the similarity is **above 0.95** (perfect match), we apply the **highest possible boost**.\n",
    "\n",
    "\n",
    "#### Additional Contextual Boosts\n",
    "\n",
    "- If a company token is **similar to niche or category words** (`our_categories_niche_words['niche_plus_cat'][i]`), we apply a **slight extra boost** to favor domain alignment.\n",
    "- If a token matches **relevant terms from the first sentence** (`most_important_words`), we apply **targeted boosts** based on its contextual importance.\n",
    "\n",
    "\n",
    "#### Per-Token Boosting Adjustments\n",
    "\n",
    "- Using `strongest_values_all[i]`, we **increase** the score for tokens that are globally strong indicators.\n",
    "- Using `weakest_values_all[i]`, we:\n",
    "  - **Avoid discarding** tokens with cosine similarity **greater than 0.525** (even if they are weak).\n",
    "  - **Deboost** tokens with lower similarity accordingly to avoid noise.\n",
    "\n",
    "\n",
    "#### Final Scoring\n",
    "\n",
    "- All individual token-level boosts are **summed and integrated** into the final label score.\n",
    "- This ensures that **strong, domain-relevant, and contextually supported matches** have the greatest impact on the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_min_tf_idf_all = []\n",
    "for idx, _ in enumerate(our_words['new_col2']):\n",
    "    list_min_tf_idf=[]\n",
    "    for each_word in our_words['new_col2'][idx]:\n",
    "        list_min_tf_idf.append(match_words_with_tf_idf_valuess[(each_word, idx)])\n",
    "    list_min_tf_idf.sort()\n",
    "    list_min_tf_idf_all.append(list_min_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list_companies = []\n",
    "terms_not_compatible = set()\n",
    "frequency_strong_terms_per_label = {}\n",
    "\n",
    "terms_are_similar_or_not = {}\n",
    "\n",
    "list_scores = np.zeros((len(df['description']), len(label_embeddings), 3))\n",
    "\n",
    "\n",
    "match_cosine_score_word_with_label = {}\n",
    "match_cosine_score_word_with_company_word = {}\n",
    "\n",
    "i=0\n",
    "for index, _ in df.iterrows():\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list111 = []\n",
    "    max_val = list_matrix[i][0][0].copy()\n",
    "\n",
    "  \n",
    "    for j, z in enumerate(elements_filtered[i]):\n",
    "        \n",
    "        index_label = list_matrix[i][j][1]\n",
    "       \n",
    "\n",
    "       \n",
    "        \n",
    "        current_scores = np.zeros(3)\n",
    "        element_weight = np.zeros(3)\n",
    "        multiply_word = -1\n",
    "        for idx, word_key_label in enumerate(list_elements[index_label]):\n",
    "            current_score = 1\n",
    "            we1 = cat_niche_embeddings[i].reshape(1,-1).astype(np.float32, order='C')\n",
    "            faiss.normalize_L2(we1)\n",
    "            label_word_embedding = embeddings_index[word_key_label]\n",
    "            if len(label_word_embedding) < 400:\n",
    "                label_word_embedding = np.pad(label_word_embedding, (0, 400 - len(label_word_embedding)), mode='constant')\n",
    "            le13 = label_word_embedding.reshape(1,-1).astype(np.float32, order='C')\n",
    "            our_values_cat = np.max(np.dot(we1, le13.T))\n",
    "\n",
    "   \n",
    "            if our_values_cat > 0.8:\n",
    "                if (original_domain[index_label] == word_key_label and  domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                    current_score = 1.35\n",
    "                else:\n",
    "                    current_score = 1.75\n",
    "            \n",
    "            if our_values_cat > 0.7:\n",
    "                if (original_domain[index_label] == word_key_label and  domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                    current_score = 1.3\n",
    "                else:\n",
    "                    current_score = 1.65\n",
    "\n",
    "            elif our_values_cat > 0.5:\n",
    "                if (original_domain[index_label] == word_key_label and  domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                    current_score = 1.2 \n",
    "                else:\n",
    "                    current_score = 1.5\n",
    "\n",
    "            else:\n",
    "\n",
    "                list_current = list(our_categories_niche_words['niche_plus_cat'][i])\n",
    "                if our_values_cat > 0.3 and list_current!=[]:\n",
    "                    word_embedding_list1 = np.array([embeddings_index.get(wd.lower(), np.zeros(300)) if (wd, i) in match_words_with_tf_idf_valuess and wd not in weak_values_all[i] else np.zeros(300) for wd in list_current])\n",
    "                    we_all = np.array(word_embedding_list1, dtype=np.float32, order='C')\n",
    "                    faiss.normalize_L2(we_all)\n",
    "                    le_wd = embeddings_index[word_key_label].reshape(1,-1).astype(np.float32, order='C')\n",
    "                    faiss.normalize_L2(le_wd)\n",
    "                    our_values_cat_indiv = np.max(np.dot(we_all, le_wd.T))\n",
    "                   \n",
    "                    if our_values_cat_indiv > 0.75:\n",
    "                        if (original_domain[index_label] == word_key_label and domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                            current_score = 1.15\n",
    "                        else:\n",
    "                            current_score = 1.5\n",
    "                    elif our_values_cat_indiv > 0.6 and (original_domain[index_label] == word_key_label and domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                        if current_score != 1:\n",
    "                            current_score = 1.3\n",
    "\n",
    "            if list(most_important_words[i]) != []:\n",
    "                list_current = list(most_important_words[i])\n",
    "                word_embedding_list1 = np.array([embeddings_index.get(wd.lower(), np.zeros(300)) if  wd not in weak_values_all[i] else np.zeros(300) for wd in list_current])\n",
    "                we_all = np.array(word_embedding_list1, dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(we_all)\n",
    "                le_wd = embeddings_index[word_key_label].reshape(1,-1).astype(np.float32, order='C')\n",
    "                faiss.normalize_L2(le_wd)\n",
    "                our_values_cat_indiv = np.max(np.dot(we_all, le_wd.T))\n",
    "                if our_values_cat_indiv > 0.70:\n",
    "                    if (original_domain[index_label] == word_key_label and domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                        if current_score != 1:\n",
    "                            current_score *= 1.05\n",
    "                        else:\n",
    "                            current_score = 1.15\n",
    "                    else:\n",
    "                        if current_score != 1:\n",
    "                            current_score *= 1.2\n",
    "                        else:\n",
    "                            current_score = 1.35\n",
    "                elif our_values_cat_indiv > 0.6 and (original_domain[index_label] == word_key_label and domain[index_label]!=original_domain[index_label]) or (original_core[index_label] == word_key_label and core[index_label]!=original_core[index_label]):\n",
    "                    if current_score != 1:\n",
    "                        if current_score != 1:\n",
    "                            current_score *= 1.125\n",
    "                        else:\n",
    "                            current_score = 1.2\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            current_scores[idx] = current_score\n",
    "\n",
    "        if len(list_elements[index_label])==3:\n",
    "            \n",
    "            if original_core[index_label] != core[index_label] and original_domain[index_label]!=domain[index_label] and len(core[index_label].split(\"::\"))>1:\n",
    "                element_weight = [0.235, 0.665, 0.1]\n",
    "            elif original_core[index_label] != core[index_label] and original_domain[index_label]!=domain[index_label]:\n",
    "                element_weight = [0.2, 0.6, 0.2]\n",
    "            elif original_core[index_label] != core[index_label] and len(core[index_label].split(\"::\"))>1:\n",
    "                element_weight = [0.425, 0.475, 0.1]\n",
    "            elif original_core[index_label] != core[index_label]:\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    element_weight = [0.55, 0.25, 0.2]\n",
    "                else:\n",
    "                    element_weight = [0.35, 0.45, 0.2]\n",
    "            elif original_domain[index_label] != domain[index_label]:\n",
    "                element_weight = [0.2, 0.425, 0.375]\n",
    "            else:\n",
    "                element_weight = [0.3, 0.4, 0.3]\n",
    "            \n",
    "            score_total = current_scores[0] * element_weight[0] + current_scores[1] * element_weight[1] + current_scores[2] * element_weight[2]\n",
    "            list_scores[i][index_label] = [current_scores[0] * element_weight[0], current_scores[1] * element_weight[1], current_scores[2] * element_weight[2]]\n",
    "\n",
    "            \n",
    "            score_total *=0.07\n",
    "            multiply_word = 0.07\n",
    "        elif len(list_elements[index_label])==2:\n",
    "            if original_core[index_label] != core[index_label] and original_domain[index_label] != domain[index_label]:\n",
    "                if len(core[index_label].split(\"__\")) > 1:\n",
    "                    element_weight = [0.3, 0.3, 0]\n",
    "                    list_scores[i][index_label] = [current_scores[0] * 0.3, 0, current_scores[1] * 0.3]\n",
    "                    score_total = current_scores[0] * 0.3 + current_scores[1] * 0.3\n",
    "                else:\n",
    "                    element_weight = [0.35, 0.1, 0]\n",
    "                    list_scores[i][index_label] = [current_scores[0] * 0.35, 0, current_scores[1] * 0.1]\n",
    "                    score_total = current_scores[0] * 0.35 + current_scores[1] * 0.1\n",
    "            elif original_core[index_label] != core[index_label]:\n",
    "                if len(core[index_label].split(\"__\")) > 1:\n",
    "                    element_weight = [0.7, 0.3, 0]\n",
    "                    list_scores[i][index_label] = [current_scores[0] * 0.7, 0, current_scores[1] * 0.1]\n",
    "                    score_total = current_scores[0] * 0.7 + current_scores[1] * 0.3\n",
    "                else:\n",
    "                    list_scores[i][index_label] = [current_scores[0] * 0.9, 0, current_scores[1] * 0.1]\n",
    "                    element_weight = [0.9, 0.1]\n",
    "\n",
    "                    score_total = current_scores[0] * 0.9 + current_scores[1] * 0.1\n",
    "            elif original_domain[index_label] != domain[index_label]:\n",
    "                element_weight = [0.3, 0.7, 0]\n",
    "                list_scores[i][index_label] = [current_scores[0] * 0.3, 0, current_scores[1] * 0.7]\n",
    "                score_total = current_scores[0] * 0.3 + current_scores[1] * 0.7\n",
    "            else:\n",
    "                list_scores[i][index_label] = [current_scores[0] * 0.5, 0, current_scores[1] * 0.5]\n",
    "                element_weight = [0.5, 0.5, 0]\n",
    "                score_total = current_scores[0] * 0.5 + current_scores[1] * 0.5\n",
    "\n",
    "            score_total *=0.06\n",
    "            multiply_word = 0.06\n",
    "\n",
    "\n",
    "        else:\n",
    "            score_total = current_scores[0]\n",
    "            list_scores[i][index_label] = [current_scores[0], 0, 0]\n",
    "\n",
    "            if domain[index_label] != original_domain[index_label]:\n",
    "                score_total*=0.5\n",
    "            score_total *=0.05\n",
    "            multiply_word = 0.05\n",
    "\n",
    "\n",
    "        weight_core = 0\n",
    "        if len(list_elements[index_label])==2:\n",
    "            weight_core = element_weight[1]\n",
    "        elif len(list_elements[index_label])==3:\n",
    "            weight_core = element_weight[2]\n",
    "            \n",
    "        list_wd = [wd for wd in our_words['new_col2'][i]]\n",
    "        for idx2, wd in enumerate(list_wd):\n",
    "            mult_value = 1\n",
    "\n",
    "            score_total1 = score_total\n",
    "            \n",
    "            our_values = list_matrix_elements_val[i][index_label][idx2]\n",
    "            \n",
    "            \n",
    "            if core[index_label] != \"0\" and core[index_label] != original_core[index_label] and list_matrix_core_val[i][index_label][idx2]>0.6 and list_matrix_core_val[i][index_label][idx2]>list_matrix_elements_val[i][index_label][idx2]:\n",
    "                score_total1*=weight_core\n",
    "\n",
    "            if our_values < 0.57 and  original_domain[index_label]!= wd and (original_domain[index_label], wd) in dict_words_same_category.keys() and dict_words_same_category[(original_domain[index_label], wd)]==1:\n",
    "                continue\n",
    "\n",
    "            if our_values < 0.57 and original_modifier[index_label]!= wd and  (original_modifier[index_label], wd) in dict_words_same_category.keys() and dict_words_same_category[(original_modifier[index_label], wd)]==1:\n",
    "                continue\n",
    "\n",
    "            if our_values < 0.57 and original_core[index_label]!= wd and (original_core[index_label], wd) in dict_words_same_category.keys() and dict_words_same_category[(original_core[index_label], wd)]==1:\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            if (i, index_label) not in frequency_strong_terms_per_label.keys():\n",
    "                frequency_strong_terms_per_label[(i,index_label)] = set()\n",
    "            list_set = frequency_strong_terms_per_label[(i,index_label)]\n",
    "    \n",
    "            tf_idf_value = match_words_with_tf_idf_valuess[(wd, i)]\n",
    "\n",
    "\n",
    "            \n",
    "            can_continue=True\n",
    "            can_continue2=True\n",
    "\n",
    "            \n",
    "            if can_continue and can_continue2:\n",
    "                if our_values > 0.95:\n",
    "                    list_set.add(wd.lower())\n",
    "                    if wd in strong_values_all[i]:\n",
    "                        score_total1 *=1.25\n",
    "                    elif wd in weak_values_all[i]:\n",
    "                        score_total1 *=0.90\n",
    "                    list_matrix[i][j] = (list_matrix[i][j][0]+score_total1*1.8,list_matrix[i][j][1])\n",
    "                    continue\n",
    "\n",
    "                elif our_values > 0.68:\n",
    "                    list_set.add(wd.lower())\n",
    "                    if wd in strong_values_all[i]:\n",
    "                        score_total1 *=1.25\n",
    "                    elif wd in weak_values_all[i] and wd not in our_categories_niche_words['niche_plus_cat'][i]:\n",
    "                        score_total1 *=0.85\n",
    "                    \n",
    "                    list_matrix[i][j] = (list_matrix[i][j][0]+score_total1*1.5,list_matrix[i][j][1])\n",
    "                    continue\n",
    "\n",
    "                elif our_values > 0.45 and (wd not in weak_values_all[i] or wd in our_categories_niche_words['niche_plus_cat'][i]) or our_values > 0.525:\n",
    "                    if len(wd.split(\"_\")) > 1 and our_values < 0.45:\n",
    "                        list_Min = [len(wd1) for wd1 in wd.split(\"_\")]\n",
    "                        min_value  = min(list_Min)\n",
    "                        if min_value > 3:\n",
    "                            length_n = len(wd.split(\"_\"))\n",
    "                            wd_splitted_embeddings = np.array([embeddings_index.get(wd1, np.zeros(300)) for wd1 in wd.split(\"_\")], dtype=np.float32, order='C')\n",
    "                            faiss.normalize_L2(wd_splitted_embeddings)\n",
    "                            total_value1 = embeddings_index[original_domain[index_label]] * element_weight[0]\n",
    "                            if modifier[index_label]!='0' and core[index_label]!='0':\n",
    "                                total_value1+=embeddings_index[original_modifier[index_label]]*element_weight[1]\n",
    "                                total_value1+=embeddings_index[original_core[index_label]]*element_weight[2]\n",
    "                            elif modifier[index_label]=='0':\n",
    "                                total_value1+=embeddings_index[original_core[index_label]]*element_weight[1]\n",
    "                            maximum_value = np.max(np.dot(total_value1, wd_splitted_embeddings.T))\n",
    "\n",
    "                            if (maximum_value<0.3):\n",
    "                                terms_not_compatible.add((wd, word_most_specific))\n",
    "                                continue\n",
    "                    \n",
    "                    list_set.add(wd.lower())\n",
    "                    if wd in strong_values_all[i]:\n",
    "                        score_total1 *=1.25\n",
    "\n",
    "                    if wd in weak_values_all[i] and wd not in our_categories_niche_words['niche_plus_cat'][i]:\n",
    "                        score_total1 *=0.75\n",
    "                   \n",
    "                    list_matrix[i][j] = (list_matrix[i][j][0]+score_total1,list_matrix[i][j][1])\n",
    "                    continue\n",
    "            \n",
    "                elif (f\"{domain[index_label]}_\" == wd[0:len(domain[index_label])+1]) and our_values<0.4 and len(wd.split(\"_\"))>1:\n",
    "                        \n",
    "                        word_separate_value = np.array([embeddings_index[our_words] for our_words in wd.split(\"_\")], dtype=np.float32, order='C')\n",
    "                        faiss.normalize_L2(word_separate_value)\n",
    "                        array_weighted_labels = sum(embeddings_index.get(word_key_label, np.zeros(300)) * element_weight[idx] for idx, word_key_label in enumerate(list_elements[index_label]))\n",
    "                        our_value_common = np.mean(np.dot(array_weighted_labels, word_separate_value.T))\n",
    "                        if wd in strong_values_all[i] and word_key_label == original_domain[index_label] and original_domain[index_label] == domain[index_label]:\n",
    "                            score_total1*=1.25\n",
    "                        elif wd in weak_values_all[i] and wd not in our_categories_niche_words['niche_plus_cat'][i]:\n",
    "                            score_total1*=0.75\n",
    "                        \n",
    "                        if our_value_common > 0.55 and word_key_label == original_domain[index_label] and original_domain[index_label] != domain[index_label]:\n",
    "                            list_set.add(wd.lower())\n",
    "                            list_matrix[i][j] = (list_matrix[i][j][0]+score_total1*0.6,list_matrix[i][j][1])\n",
    "                            continue\n",
    "                        elif our_value_common > 0.55:\n",
    "                            list_set.add(wd.lower())\n",
    "                            list_matrix[i][j] = (list_matrix[i][j][0]+ score_total1,list_matrix[i][j][1])\n",
    "                            continue\n",
    "                \n",
    "                if our_values > 0.35 and wd not in weak_values_all[i]:\n",
    "                    list_set.add(wd.lower())\n",
    "                    list_matrix[i][j] = (list_matrix[i][j][0]+our_values*score_total1*0.8,list_matrix[i][j][1])\n",
    "\n",
    "\n",
    "        frequency_strong_terms_per_label[(i,index_label)] = list_set\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_matrix_original_2 = []\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list_matrix_original_2.append(list_matrix[i].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(30):\n",
    "       \n",
    "        if max_value - list_matrix[i][j][0] <= 0.1:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "        \n",
    "      \n",
    "    \n",
    "    final_list.append(list111) \n",
    "    elements_filtered.append(row_filtered)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_wrong_similar_elements(word, each_key_word, correlated_terms_filter_list):\n",
    "    values_list = np.array([embeddings_index[wd] for wd in correlated_terms_filter_list], dtype=np.float32, order='C')\n",
    "    faiss.normalize_L2(values_list)\n",
    "    values = list(np.dot(values_list, embeddings_index[each_key_word].T))\n",
    "\n",
    "    if values == []:\n",
    "        return False\n",
    "    \n",
    "    antonyms_terms = [w2 for w1, w2 in antonyms_words if w1==word and w2 in embeddings_index]\n",
    "    if antonyms_terms != list():\n",
    "        values_list2 = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in antonyms_terms], dtype=np.float32, order='C')\n",
    "        faiss.normalize_L2(values_list2)\n",
    "\n",
    "        list_values = list(np.max(np.dot(values_list, values_list2.T), axis=1))\n",
    "        is_true = any((values[index]>=0.55 and list_values[index]<0.5)  for index in range(len(values)))\n",
    "        return is_true\n",
    "    return any(val>=0.55 for val in values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_current_embeddings = []\n",
    "for idx, _ in enumerate(our_words['new_col2']):\n",
    "    emb_for_the_current_one = []\n",
    "\n",
    "    \n",
    "    for j, z in enumerate(elements_filtered[idx]):\n",
    "\n",
    "\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "\n",
    "        col_emb = np.array([embeddings_index[wd] for wd in our_words['new_col2'][idx]], dtype =np.float32, order='C')\n",
    "        filter_emb = np.array([embeddings_index[wd] for wd in list_elements[index_label]], dtype =np.float32, order='C')\n",
    "        faiss.normalize_L2(col_emb)\n",
    "        faiss.normalize_L2(filter_emb)\n",
    "\n",
    "        current_embedding = np.dot(filter_emb, col_emb.T)\n",
    "        emb_for_the_current_one.append(current_embedding)\n",
    "\n",
    "       \n",
    "    all_current_embeddings.append(emb_for_the_current_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_core_embeddings_terms = []\n",
    "for idx, _ in enumerate(our_words['new_col2']):\n",
    "    emb_for_the_current_one = []\n",
    "\n",
    "    \n",
    "    for j, z in enumerate(elements_filtered[idx]):\n",
    "\n",
    "\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "\n",
    "        col_emb = np.array([embeddings_index[wd] for wd in our_words['new_col2'][idx]], dtype =np.float32, order='C')\n",
    "        faiss.normalize_L2(col_emb)\n",
    "\n",
    "        if(original_core[index_label]!='0') and original_core[index_label]!=core[index_label]:\n",
    "            current_embedding = np.dot(embeddings_index[original_core[index_label]], col_emb.T)\n",
    "        else:\n",
    "            current_embedding = np.dot(np.zeros(300), col_emb.T)\n",
    "        emb_for_the_current_one.append(current_embedding)\n",
    "\n",
    "       \n",
    "    all_core_embeddings_terms.append(emb_for_the_current_one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Filtering and Boosting Logic for Noisy or Ambiguous Terms\n",
    "\n",
    "In this phase, we refine matching by **cleaning noisy matches** and **boosting relevant but low-similarity terms** carefully.\n",
    "\n",
    "\n",
    "### 1. Cleaning Weak Matches\n",
    "\n",
    "- We try to **filter out tokens** that have **low similarity** with label parts and are **not strong terms** (`weak_values_all`).\n",
    "- This helps to **reduce noise** and **focus on meaningful words** when matching labels.\n",
    "\n",
    "\n",
    "### 2. Boosting Low Similarity but Relevant Terms\n",
    "\n",
    "To avoid missing useful matches, we apply two strategies:\n",
    "\n",
    "#### (a) Dictionary-Based Similar Term Expansion\n",
    "- We use a **custom dictionary** (built using ConceptNet relationships) to find words **semantically related** to the current token.\n",
    "- We ensure that:\n",
    "  - The term is **not more generic** than the label term.\n",
    "  - The match is **specific enough** to avoid false positives.\n",
    "- If the relationship is strong, we **boost the label score** even if the cosine similarity is low.\n",
    "\n",
    "#### (b) Splitting and Validating Compound Tokens\n",
    "- If the token is a **compound word** (e.g., `agriculture_machinery`), we split it into parts.\n",
    "- For each part:\n",
    "  - If a part matches well with the label term (cosine > 0.7), we **boost** the score proportionally (e.g., `+1/len(word.split(\"_\"))` per good part).\n",
    "  - If parts match indirectly, we still use them carefully to **adjust the boost**.\n",
    "- If splitting leads to strong matches, we **accumulate** a multiplier boost.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Using Correlated Terms Fallback\n",
    "\n",
    "If a term is **not directly correlated**, we:\n",
    "\n",
    "- Look into `correlated_terms_with_each_word` dictionary.\n",
    "- Try to find **terms containing the label** or having **high cosine similarity** with it.\n",
    "- Apply `eliminate_wrong_similar_elements` to **filter out false positives** by checking:\n",
    "  - If the term has **high similarity with antonyms** (then discard it).\n",
    "  - If not, and the remaining terms are good, **accept** and **boost** the label.\n",
    "\n",
    "\n",
    "### 4. Memory Optimization and Smart Multipliers\n",
    "\n",
    "- We **store results in dictionaries** (like `are_terms_correlated`) to avoid redundant computation.\n",
    "- In complex cases, if relatedness is detected through **second-level correlated terms**, we apply a **small boost** (`mult = 0.05`) to avoid wrongly promoting a false positive.\n",
    "\n",
    "\n",
    "### 5. Final Goal\n",
    "\n",
    "This filtering and boosting phase ensures:\n",
    "- **Relevant terms are rescued** even if they are not directly obvious by cosine.\n",
    "- **Irrelevant or noisy labels are heavily penalized** and **filtered out**.\n",
    "- **Compound terms** are handled smartly without missing real matches.\n",
    "- **False positives** caused by generic or antonymic confusion are minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index_elements = [-1] * len(our_words['new_col2'])\n",
    "zzz=0\n",
    "are_terms_correlated = {}\n",
    "are_terms_asssociated = {}\n",
    "does_contain_similar_elements = {}\n",
    "each_key_word_specifity_score = {}\n",
    "for idx, _ in enumerate(our_words['new_col2']):\n",
    "    min_25th = (min_val_list[idx] + (max_val_list[idx]-min_val_list[idx]) * 0.25)\n",
    "\n",
    "    max_25th = (max_val_list[idx] - (max_val_list[idx]-min_val_list[idx]) * 0.25)\n",
    "    list_matrix[idx].sort(reverse=True)\n",
    "    max_val = list_matrix[idx][0][0]\n",
    "\n",
    "    if(len(elements_filtered[idx])==1 or (list_matrix[idx][0][0] - list_matrix[idx][1][0])>=0.1):\n",
    "        last_index_elements[idx] = list_matrix[idx][0][1]\n",
    "        continue\n",
    "    \n",
    "    \n",
    "\n",
    "    for j, z in enumerate(elements_filtered[idx]):\n",
    "\n",
    "\n",
    "        if(max_val - list_matrix[idx][j][0]>0.15):\n",
    "            continue\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "        \n",
    "      \n",
    "        if (idx, index_label) not in frequency_strong_terms_per_label.keys():\n",
    "            \n",
    "            list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "            continue\n",
    "\n",
    "\n",
    "        for idx2, each_key_word in enumerate(list_elements[index_label]):               \n",
    "            \n",
    "            if list_matrix[idx][j][0] == -1:\n",
    "                continue\n",
    "            end_loop = -1\n",
    "            \n",
    "            if each_key_word == core[index_label] or each_key_word == domain[index_label] or each_key_word == modifier[index_label]:\n",
    "                end_loop = 0\n",
    "                max_val = -1\n",
    "                keydf = \"\"\n",
    "\n",
    "                for our_index, i in enumerate(our_words['new_col2'][idx]):\n",
    "\n",
    "                    if core[index_label] != \"0\" and core[index_label] != original_core[index_label] and list_matrix_core_val[idx][index_label][our_index]>0.7:\n",
    "                        continue\n",
    "\n",
    "                    if each_key_word not in specifity_word.keys():\n",
    "                        specifity_word[each_key_word] = get_specificity(each_key_word)\n",
    "                        \n",
    "                    \n",
    "                    specifity_score2 = specifity_word[each_key_word]\n",
    "\n",
    "\n",
    "                    \n",
    "                    cos_similarity = all_current_embeddings[idx][j][idx2][our_index]\n",
    "\n",
    "                \n",
    "                    did_not_split_compound_word = 1\n",
    "                    cos_similarity1 = -1\n",
    "\n",
    "                    \n",
    "                    if (i not in weak_values_all[idx]) and ((specifity_score2!=0 and cos_similarity >0.36) or cos_similarity >0.55) and (did_not_split_compound_word==1 or cos_similarity1 > 0.35 ):\n",
    "                        \n",
    "                        end_loop = 1\n",
    "                        max_val = cos_similarity1\n",
    "                        keydf = i\n",
    "                       \n",
    "                \n",
    "                    elif match_words_with_tf_idf_valuess[(i, idx)] > min_25th and cos_similarity<=0.35 and ((i, each_key_word) not in are_terms_asssociated or are_terms_asssociated[(i, index_label)] != 0):\n",
    "                        \n",
    "                        similar_words_to_that =get_sim_list(each_key_word, i, index_label, dict_words_same_category)\n",
    "                       \n",
    "                        if each_key_word not in specifity_word.keys():\n",
    "                            specifity_word[each_key_word] = get_specificity(each_key_word)\n",
    "                        specifity1 = specifity_word[each_key_word]\n",
    "                        if each_key_word not in noun_for_adj_dict.keys():\n",
    "                            noun_for_adj_dict[each_key_word] = generate_noun_for_adj(each_key_word, embeddings_index)\n",
    "                        noun_for_adj = noun_for_adj_dict[each_key_word]\n",
    "\n",
    "                        if noun_for_adj not in specifity_word.keys():\n",
    "                            specifity_word[noun_for_adj] = get_specificity(noun_for_adj)\n",
    "                        specifity1_5 = specifity_word[noun_for_adj]\n",
    "\n",
    "\n",
    "\n",
    "                        if i not in specifity_word.keys():\n",
    "                            specifity_word[i] = get_specificity(i)\n",
    "                        specifity2 = specifity_word[i]\n",
    "\n",
    "                        if i not in noun_for_adj_dict.keys():\n",
    "                            noun_for_adj_dict[i] = generate_noun_for_adj(i, embeddings_index)\n",
    "                        noun_for_adj2 = noun_for_adj_dict[i]\n",
    "\n",
    "                        if noun_for_adj2 not in specifity_word.keys():\n",
    "                            specifity_word[noun_for_adj2] = get_specificity(noun_for_adj2)\n",
    "                        specifity2_5 = specifity_word[noun_for_adj2]\n",
    "                        \n",
    "                        is_very_specific = max(specifity1, specifity1_5)\n",
    "                        is_second_term = max(specifity2, specifity2_5)\n",
    "                                  \n",
    "                        \n",
    "\n",
    "                        mult = 1\n",
    "                        values = []\n",
    "\n",
    "                        if len(i.split(\"_\")) > 1 and each_key_word==core[index_label] and core[index_label]==original_core[index_label]:\n",
    "                            values_list = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in i.split(\"_\")], dtype=np.float32, order='C')\n",
    "                            number_splitted_words = len(i.split(\"_\"))\n",
    "                            faiss.normalize_L2(values_list)\n",
    "                            elements = list(np.dot(embeddings_index[each_key_word], values_list.T))\n",
    "                            elements = list(filter(lambda x: x>0.5, elements))\n",
    "                            if len(elements)!=0:\n",
    "                                mult = len(elements)/number_splitted_words\n",
    "                                values = [1,2,3]\n",
    "                            else:\n",
    "                                mult = 0\n",
    "                                continue\n",
    "                            are_terms_correlated[(i, each_key_word)] = mult\n",
    "\n",
    "\n",
    "                        if values == [] and (i, each_key_word) in are_terms_correlated and (similar_words_to_that == None or similar_words_to_that==set()):\n",
    "                            \n",
    "                            mult = are_terms_correlated[(i, each_key_word)]\n",
    "                           \n",
    "                            if mult == 0:\n",
    "                                continue\n",
    "                            else:\n",
    "                                values = [1,2,3]\n",
    "                        elif values == [] and (similar_words_to_that == None or similar_words_to_that==set()):\n",
    "                            mult=0\n",
    "                            \n",
    "                            if i in correlated_terms_with_each_word.keys():\n",
    "                                correlated_terms_filter_list = correlated_terms_with_each_word[i]\n",
    "                                value_mlt = 1\n",
    "                                if (i, each_key_word) not in does_contain_similar_elements:\n",
    "                                    bool_value = eliminate_wrong_similar_elements(i, each_key_word, correlated_terms_filter_list)\n",
    "                                    if bool_value:\n",
    "                                        values = [1,2,3]\n",
    "                                    else:\n",
    "                                        values = []\n",
    "                                    does_contain_similar_elements[(i, each_key_word)] = values\n",
    "                                    if values == list():\n",
    "                                        set_temp = set()\n",
    "                                        \n",
    "                                        for val in correlated_terms_with_each_word[i]:\n",
    "                                            if val in correlated_terms_with_each_word.keys():\n",
    "                                                lzt = list(filter(lambda x: x in our_words['new_col2'][idx] , correlated_terms_with_each_word[val]))\n",
    "                                                set_temp.update(lzt)\n",
    "                                                                                    \n",
    "                                        if set_temp != set():\n",
    "\n",
    "                                            values =  list(set_temp)\n",
    "                                            values_emb = np.array([embeddings_index[wd] for wd in values],dtype=np.float32, order='C')\n",
    "                                            our_values = list(np.dot(values_emb, embeddings_index[each_key_word].T))\n",
    "                                            is_true = any(val>=0.5 for val in our_values)\n",
    "\n",
    "                                            if(is_true):\n",
    "                                                values = [1,2,3]\n",
    "                                            value_mlt = 0.05\n",
    "                                    else:\n",
    "                                        does_contain_similar_elements[(i, each_key_word)] = values\n",
    "                                        values = does_contain_similar_elements[(i, each_key_word)]\n",
    "                                else:\n",
    "                                    values = does_contain_similar_elements[(i, each_key_word)]\n",
    "                                        \n",
    "\n",
    "            \n",
    "                                if values !=list():\n",
    "                                    values = [1,2,3]\n",
    "                                    mult=1\n",
    "                                mult = mult * value_mlt\n",
    "                                are_terms_correlated[(i, each_key_word)] = mult * value_mlt\n",
    "\n",
    "                            elif len(i.split(\"_\")) > 1:\n",
    "                                later_list = []\n",
    "                                now_list = []\n",
    "                                mult = 0\n",
    "\n",
    "                                values = []\n",
    "\n",
    "                                for wd2 in i.split(\"_\"):\n",
    "                                    curr_list = set()\n",
    "                                    values2 = []\n",
    "                                    \n",
    "                                    \n",
    "                                    if wd2 in correlated_terms_with_each_word.keys():\n",
    "                                        correlated_terms_filter_list = correlated_terms_with_each_word[wd2]\n",
    "                                        if (wd2, each_key_word) not in does_contain_similar_elements and wd2 in embeddings_index.keys():\n",
    "                                            bool_value = eliminate_wrong_similar_elements(wd2, each_key_word, correlated_terms_filter_list)\n",
    "                                            if bool_value:\n",
    "                                                values2 = [1,2,3]\n",
    "                                            else:\n",
    "                                                values2 = []\n",
    "                                            does_contain_similar_elements[(wd2, each_key_word)] = values2\n",
    "                                            \n",
    "                                        elif wd2 not in embeddings_index.keys():\n",
    "                                            does_contain_similar_elements[(wd2, each_key_word)] = []\n",
    "                                        \n",
    "                                        values2 = does_contain_similar_elements[(wd2, each_key_word)]\n",
    "\n",
    "                                        if values2!=[]:\n",
    "                                            values = values2.copy()\n",
    "                                            now_list.append(wd2)\n",
    "                                            mult+=1/len(i.split(\"_\"))\n",
    "\n",
    "                                    \n",
    "                                    elif wd2 in embeddings_index.keys():\n",
    "                                        later_list.append(wd2)\n",
    "                                \n",
    "                                if now_list!=[] and later_list!=[]:\n",
    "                                    now_values_list = np.array([embeddings_index[wd_now] for wd_now in now_list], dtype=np.float32, order='C')\n",
    "                                    faiss.normalize_L2(now_values_list)\n",
    "                                    later_values_list = np.array([embeddings_index[wd_later] for wd_later in later_list], dtype=np.float32, order='C')\n",
    "                                    faiss.normalize_L2(later_values_list)\n",
    "\n",
    "                                    current_scores_later = np.dot(later_values_list, now_values_list.T)\n",
    "\n",
    "                                    for idx3, our_word in enumerate(later_list):\n",
    "                                        values_list = current_scores_later[idx3]\n",
    "\n",
    "                                    \n",
    "                                        values_list = any(val>=0.7 for val in values_list)\n",
    "\n",
    "                                        if values_list!= []:\n",
    "                                            mult+=1/len(i.split(\"_\"))\n",
    "\n",
    "                                            if our_word in our_words['new_col2'][idx]:\n",
    "                                                if (our_word, each_key_word) in are_terms_correlated and are_terms_correlated[(our_word, each_key_word)] == 0:\n",
    "                                                    our_words['new_col2'][idx].remove(our_word)\n",
    "                                                    our_words['new_col2'][idx].append(our_word)\n",
    "\n",
    "\n",
    "                                                are_terms_correlated[(our_word, each_key_word)] = 1\n",
    "\n",
    "                                            \n",
    "                                if values == []:\n",
    "                                    mult=0\n",
    "                            are_terms_correlated[(i, each_key_word)] = mult\n",
    "            \n",
    "                        \n",
    "                        is_generic_term = (i in original_core and i not in core) or (i in original_domain and i not in domain)\n",
    "                        \n",
    "                        if (mult!=0 and ((similar_words_to_that != None and similar_words_to_that!=set()) or (values!=list())) and not is_generic_term):\n",
    "                            \n",
    "                            end_loop = 1\n",
    "\n",
    "                            list_set.add(wd.lower())\n",
    "                            frequency_strong_terms_per_label[(i,index_label)] = list_set\n",
    "                            are_terms_correlated[(i, each_key_word)] = mult\n",
    "                            each_key_word1 = each_key_word\n",
    "                            if each_key_word not in average_tf_idf.keys() and each_key_word1 in convertPluralToSingular.keys() and convertPluralToSingular[each_key_word1] in average_tf_idf.keys():\n",
    "                                each_key_word1 = convertPluralToSingular[each_key_word]\n",
    "                            if len(list_elements[index_label]) == 3 and (each_key_word1 not in average_tf_idf or  average_tf_idf[each_key_word1]>0.075):\n",
    "                                mult3 = 0.07\n",
    "                                our_current_score = list_scores[idx][index_label][idx2] * mult3 * mult\n",
    "                            \n",
    "                                list_matrix[idx][j] = (list_matrix[idx][j][0]+our_current_score, list_matrix[idx][j][1])\n",
    "                            elif len(list_elements[index_label]) == 2 and (each_key_word1 not in average_tf_idf or average_tf_idf[each_key_word1]>0.075):\n",
    "                                mult3 = 0.06\n",
    "                                our_current_score = list_scores[idx][index_label][idx2] * mult3 * mult\n",
    "                                list_matrix[idx][j] = (list_matrix[idx][j][0]+our_current_score, list_matrix[idx][j][1])\n",
    "                            elif each_key_word1 not in average_tf_idf or average_tf_idf[each_key_word1]>0.075:\n",
    "                                mult3 = 0.06\n",
    "                                our_current_score = list_scores[idx][index_label][idx2] * mult3 * mult\n",
    "                                \n",
    "                                list_matrix[idx][j] = (list_matrix[idx][j][0]+our_current_score, list_matrix[idx][j][1])\n",
    "                        else:\n",
    "                            are_terms_correlated[(i, each_key_word)] = 0\n",
    "                            are_terms_asssociated[(i, index_label)] = 0\n",
    "            if end_loop == 0 and list_matrix[idx][j][0]!=-1:\n",
    "               list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "            else:        \n",
    "                last_index_elements[idx] = index_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    \n",
    "    for j in range(30):\n",
    "\n",
    "\n",
    "        \n",
    "        list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "        row_filtered.append(list_matrix[i][j])\n",
    "        \n",
    "       \n",
    "      \n",
    "    \n",
    "    final_list.append(list111)\n",
    "    elements_filtered.append(row_filtered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(30):\n",
    "        if max_value - list_matrix[i][j][0] <= 0.2:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "    \n",
    "    final_list.append(list111)\n",
    "    elements_filtered.append(row_filtered)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_if_label_contains = [\"promotion\", \"solutions\", \"support\", \"building\"]\n",
    "terms_to_consider_ignoring = [\"commercial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_values_all_copy = weak_values_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Label Invalidation\n",
    "\n",
    "After scoring all label candidates, we apply an additional pass to **invalidate weak labels** when stronger ones are present:\n",
    "\n",
    "- For each company:\n",
    "  - If the **top label score ≥ 0.30** (i.e., a strong match exists):\n",
    "    - All labels scoring **< 0.30** and within **0.4 of the top score** are **discarded**.\n",
    "    - This prevents weaker labels from influencing downstream selection or ranking.\n",
    "    \n",
    "This ensures that only **semantically confident** labels are retained, while borderline or noisy ones are filtered out early in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "    if list_matrix[i][0][0] >=0.3:\n",
    "        for j in range(50):\n",
    "            \n",
    "            if max_value - list_matrix[i][j][0] <= 0.4:\n",
    "                if list_matrix[i][j][0] < 0.3:\n",
    "                    list_matrix[i][j] = (-1, list_matrix[i][j][1])\n",
    "                \n",
    "\n",
    "    list_matrix[i].sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_list_matrix = []\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    old_list_matrix.append(list_matrix[i].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(30):\n",
    "        if max_value - list_matrix[i][j][0] <= 0.2:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "    \n",
    "    final_list.append(list111)\n",
    "    elements_filtered.append(row_filtered)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasContext_relevant_words = {}\n",
    "for iz in domain:\n",
    "    if iz in hasContext_relationship.keys():\n",
    "        if len(iz.split(\"_\")) > 1:\n",
    "            list_values = list(set(hasContext_relationship[iz]))\n",
    "            embeddings_value = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in list_values], dtype=np.float32, order='C')\n",
    "            our_values = np.dot(embeddings_index[iz], embeddings_value.T)\n",
    "            list_values_range = range(len(list_values))\n",
    "\n",
    "            wd_splitted_embeddings = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in iz.split(\"_\")], dtype=np.float32, order='C')\n",
    "            faiss.normalize_L2(wd_splitted_embeddings)\n",
    "\n",
    "            list_values_range = list(filter(lambda x: our_values[x]>=0.4 and our_values[x]>np.max(np.dot(embeddings_index[list_values[x]], wd_splitted_embeddings.T)), list_values_range))\n",
    "            if list_values_range!=[]:\n",
    "                values_add = []\n",
    "                for i in list_values_range:\n",
    "                    values_add.append(list_values[i])\n",
    "                hasContext_relevant_words[iz] = values_add\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index_elements = [-1] * len(our_words['new_col2'])\n",
    "idx=-1\n",
    "each_key_word_specifity_score = {}\n",
    "while idx + 1 < len(our_words['new_col2']):\n",
    "    idx+=1\n",
    "    \n",
    "    list_min_tf_idf=[]\n",
    "    for each_word in our_words['new_col2'][idx]:\n",
    "        list_min_tf_idf.append(match_words_with_tf_idf_valuess[(each_word, idx)])\n",
    "    list_min_tf_idf.sort()\n",
    "\n",
    "    list_matrix[idx].sort(reverse=True)\n",
    "    max_val = list_matrix[idx][0][0]\n",
    "\n",
    "    if list_matrix[idx][0][0] - list_matrix[idx][1][0]>=0.3 and list_matrix[idx][1][0]<=0.5:\n",
    "        continue\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    words_eliminated=0\n",
    "\n",
    "    copy_list_matrix = list_matrix[idx].copy()\n",
    "    no_val=0\n",
    "    if strong_values_all[idx] == []:\n",
    "        no_val = 10000.0\n",
    "    else:\n",
    "        no_val = (len(weak_values_all[idx])/len(strong_values_all[idx]))\n",
    "\n",
    "    copy_weak = weak_values_all[idx].copy()\n",
    "    if no_val > 3.5:\n",
    "        weak_values_all[idx] = []\n",
    "\n",
    "    for j in range(20):\n",
    "\n",
    "        list_matrix_copy_row = list_matrix[idx].copy()\n",
    "        no_key_word_eliminated=0\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "        \n",
    "        if (idx, index_label) not in frequency_strong_terms_per_label.keys() or list_matrix[idx][j][0] == -1:\n",
    "            words_eliminated+=1\n",
    "            list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "            continue\n",
    "     \n",
    "\n",
    "        \n",
    "        \n",
    "      \n",
    "        for idx2, each_key_word in enumerate(list_elements[index_label]):               \n",
    "\n",
    "            end_loop = -1\n",
    "\n",
    "            \n",
    "\n",
    "            ignore_term = not(idx2 == 1 or ((core[index_label] !='0' and core[index_label]==original_core[index_label]) and each_key_word!=original_core[index_label]) or ((modifier[index_label]!='0' and modifier[index_label]==original_modifier[index_label]) and each_key_word!=original_modifier[index_label]) or ((domain[index_label]!='0' and domain[index_label]==original_domain[index_label]) and each_key_word!=original_domain[index_label])) and each_key_word in terms_to_consider_ignoring\n",
    "          \n",
    "            \n",
    "           \n",
    "            \n",
    "            if  each_key_word not in ignore_if_label_contains and (ignore_term or each_key_word not in terms_to_consider_ignoring):\n",
    "                end_loop = 0\n",
    "                max_val = -1\n",
    "                keydf = \"\"\n",
    "\n",
    "                max_v=-1\n",
    "\n",
    "                \n",
    "        \n",
    "                array_value = np.array([embeddings_index[wd] for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(array_value)\n",
    "\n",
    "                our_values_all = np.dot(embeddings_index[each_key_word], array_value.T)\n",
    "                if each_key_word == domain[index_label]:\n",
    "                    \n",
    "                    max_value = np.max(our_values_all)\n",
    "                    if max_value < 0.3:\n",
    "                        list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "                        no_key_word_eliminated+=1\n",
    "                        continue\n",
    "                merge_word = ''\n",
    "                cos_merged = 0.0\n",
    "\n",
    "                if each_key_word == original_core[index_label] and original_core[index_label] != core[index_label] and f\"{original_domain[index_label]}_{original_core[index_label]}\" in embeddings_index.keys():\n",
    "                        if modifier[index_label] == '0':\n",
    "                            merge_word = f\"{original_domain[index_label]}_{original_core[index_label]}\"\n",
    "                elif each_key_word == original_core[index_label] and original_core[index_label] != core[index_label] and modifier[index_label]==\"0\" and original_domain[index_label] in isA_relationship.keys():\n",
    "                    \n",
    "                    for word_sub in isA_relationship[original_domain[index_label]]:\n",
    "                        if f\"{word_sub}_{original_core[index_label]}\" in embeddings_index.keys():\n",
    "                            merge_word = f\"{word_sub}_{original_core[index_label]}\"\n",
    "                            break\n",
    "                if each_key_word == original_core[index_label] and original_core[index_label] != core[index_label] and original_modifier[index_label]!=\"0\" and f\"{original_modifier[index_label]}_{original_core[index_label]}\" in embeddings_index.keys():      \n",
    "                    merge_word = f\"{original_modifier[index_label]}_{original_core[index_label]}\"\n",
    "                elif each_key_word == original_core[index_label] and original_core[index_label] != core[index_label] and original_modifier[index_label]!=\"0\" and original_modifier[index_label] in isA_relationship.keys():\n",
    "                    for word_sub in isA_relationship[original_modifier[index_label]]:\n",
    "                        if f\"{word_sub}_{original_core[index_label]}\" in embeddings_index.keys():\n",
    "                            merge_word = f\"{word_sub}_{original_core[index_label]}\"\n",
    "                            break                 \n",
    "                    \n",
    "                if each_key_word == original_modifier[index_label] and original_modifier[index_label] != modifier[index_label] and f\"{original_domain[index_label]}_{original_modifier[index_label]}\" in embeddings_index.keys():\n",
    "                    merge_word = f\"{original_domain[index_label]}_{original_modifier[index_label]}\"\n",
    "                elif each_key_word == original_modifier[index_label] and original_modifier[index_label] != modifier[index_label] and original_domain[index_label] in isA_relationship.keys():\n",
    "                    for word_sub in isA_relationship[original_domain[index_label]]:\n",
    "                        if f\"{word_sub}_{original_modifier[index_label]}\" in embeddings_index.keys():\n",
    "                            merge_word = f\"{word_sub}_{original_modifier[index_label]}\"                            \n",
    "                            break\n",
    "                \n",
    "                \n",
    "                if merge_word != '':\n",
    "                    values_emb = np.array([embeddings_index[wd] for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                    faiss.normalize_L2(values_emb)\n",
    "                    values_merges = np.dot(embeddings_index[merge_word], values_emb.T)\n",
    "                else:\n",
    "                    values_merges = np.zeros(300)\n",
    "                \n",
    "                for idx2, i in enumerate(our_words['new_col2'][idx]):\n",
    "\n",
    "                    \n",
    "                    if each_key_word not in specifity_word.keys():\n",
    "                        specifity_word[each_key_word] = get_specificity(each_key_word)\n",
    "                    specifity_score2 = specifity_word[each_key_word]\n",
    "\n",
    "\n",
    "                    cos_similarity = our_values_all[idx2]\n",
    "                    cos_merged = values_merges[idx2]\n",
    "                    \n",
    "\n",
    "                    did_not_split_compound_word = 1                    \n",
    "                    is_correlated = (i, each_key_word) in are_terms_correlated and are_terms_correlated[(i, each_key_word)]>0.05\n",
    "\n",
    "                   \n",
    "\n",
    "                    if each_key_word !=original_core[index_label] and core[index_label] != \"0\" and core[index_label] != original_core[index_label] and list_matrix_core_val[idx][index_label][idx2]>0.7:\n",
    "                        continue\n",
    "                    is_okay = False\n",
    "                    if len(i.split(\"_\"))>1 and cos_similarity < 0.3 and each_key_word!=original_domain[index_label] and each_key_word == original_core[index_label]:\n",
    "                        array_emb = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in i.split(\"_\")], dtype=np.float32, order='C')\n",
    "                        faiss.normalize_L2(array_emb)\n",
    "                        values_list = np.max(np.dot(embeddings_index[each_key_word], array_emb.T))\n",
    "                        values_list_domain = np.max(np.dot(embeddings_index[original_domain[index_label]], array_emb.T))\n",
    "\n",
    "                        if values_list > 0.5 and values_list_domain > 0.6:\n",
    "                            is_okay = True\n",
    "\n",
    "                    if len(i.split(\"_\"))>1 and cos_similarity < 0.3 and original_modifier[index_label]!='0' and each_key_word!=original_modifier[index_label] and each_key_word == original_core[index_label]:\n",
    "                        array_emb = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in i.split(\"_\")], dtype=np.float32, order='C')\n",
    "                        faiss.normalize_L2(array_emb)\n",
    "                        values_list = np.max(np.dot(embeddings_index[each_key_word], array_emb.T))\n",
    "                        values_list_domain = np.max(np.dot(embeddings_index[original_modifier[index_label]], array_emb.T))\n",
    "\n",
    "                        if values_list > 0.5 and values_list_domain > 0.6:\n",
    "                            is_okay = True\n",
    "                    \n",
    "                    if len(i.split(\"_\"))>1 and cos_similarity < 0.3 and each_key_word!=original_domain[index_label] and each_key_word == original_modifier[index_label]:\n",
    "                        array_emb = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in i.split(\"_\")], dtype=np.float32, order='C')\n",
    "                        faiss.normalize_L2(array_emb)\n",
    "                        values_list = np.max(np.dot(embeddings_index[each_key_word], array_emb.T))\n",
    "                        values_list_domain = np.max(np.dot(embeddings_index[original_domain[index_label]], array_emb.T))\n",
    "\n",
    "                        if values_list > 0.5 and values_list_domain > 0.6:\n",
    "                            is_okay = True\n",
    "\n",
    "                    \n",
    "                        \n",
    "                   \n",
    "                    if each_key_word == domain[index_label] and domain[index_label]==original_domain[index_label] and i in average_tf_idf and average_tf_idf[i] > 0.075 and (i not in weak_values_all[idx] or (match_words_with_tf_idf_valuess[(i, idx)]>=list_min_tf_idf_all[idx][-len(our_words['new_col2'][idx])//5])) and ((cos_similarity > 0.325 or is_correlated or is_okay)) or ((cos_similarity>0.41 or cos_merged>0.42 or is_okay) and i in average_tf_idf and  average_tf_idf[i] > 0.1) or cos_similarity>0.5:\n",
    "                        if each_key_word in hasContext_relevant_words.keys():\n",
    "                            related_words = hasContext_relevant_words[each_key_word]\n",
    "                            values_list = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in related_words], dtype=np.float32, order='C')\n",
    "                            faiss.normalize_L2(values_list)\n",
    "                            values_min = np.min(np.dot(values_list, embeddings_index[i].T))\n",
    "                            \n",
    "                            if values_min < 0.20:\n",
    "                                continue\n",
    "                        end_loop = 1\n",
    "                        max_val = cos_similarity1\n",
    "                        keydf = i\n",
    "\n",
    "                    elif (each_key_word != domain[index_label] or domain[index_label]!=original_domain[index_label]) and i in average_tf_idf and average_tf_idf[i] > 0.065 and (i not in weak_values_all[idx] or (match_words_with_tf_idf_valuess[(i, idx)]>=list_min_tf_idf_all[idx][-len(our_words['new_col2'][idx])//5])) and ((cos_similarity > 0.30 or is_correlated or cos_merged>0.35 or is_okay) or ((cos_similarity > 0.25 or is_correlated or is_okay) and (len(core[index_label].split(\"::\"))>1 and each_key_word==original_core[index_label]))) or ((cos_similarity>0.42 or cos_merged>0.42) and i in average_tf_idf and  average_tf_idf[i] > 0.1):\n",
    "                        end_loop = 1\n",
    "                        max_val = cos_similarity1\n",
    "                        keydf = i\n",
    "\n",
    "                    if cos_similarity > 0.51:\n",
    "                        \n",
    "                        end_loop = 1\n",
    "                        max_val = cos_similarity1\n",
    "                        keydf = i\n",
    "                       \n",
    "  \n",
    "                        break\n",
    "\n",
    "            \n",
    "            if end_loop == 0 and list_matrix[idx][j][0]!=-1:  \n",
    "                list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "                no_key_word_eliminated+=1\n",
    "\n",
    "            else:   \n",
    "                last_index_elements[idx] = index_label\n",
    "\n",
    "        if no_key_word_eliminated!=0:\n",
    "            words_eliminated+=1\n",
    "    if no_val > 3.5:\n",
    "            weak_values_all[idx] = copy_weak\n",
    "    if(words_eliminated==20):\n",
    "        list_matrix[idx] = copy_list_matrix.copy()\n",
    "    \n",
    "        if weak_values_all[idx] != []:\n",
    "            weak_values_all[idx] = []\n",
    "            idx-=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_values_all = weak_values_all_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Context-Aware Candidate Filtering (Post-Scoring Refinement)\n",
    "\n",
    "After computing scores for all label candidates (`list_matrix`), we apply a **contextual filtering step** to remove weak or irrelevant matches using semantic signals.\n",
    "\n",
    "#### How It Works:\n",
    "\n",
    "- For each company:\n",
    "  - Retain the top 30 label candidates, **as long as their score is within 0.15** of the best one.\n",
    "  - For each of these, compute contextual alignment using:\n",
    "    - `first_sentence_matrix` → alignment with the company’s opening sentence.\n",
    "    - `context_matrix` → general semantic similarity (via BERT embeddings).\n",
    "    - `context_matrix_niche` → similarity to niche-specific signals.\n",
    "    - `context_matrix_category` → similarity to broader category context.\n",
    "\n",
    "- **Keep the label** if at least one of the following holds:\n",
    "  - `first_sentence_matrix > 0.3`\n",
    "  - `context_matrix > 0.3`\n",
    "  - `context_matrix_niche > 0.5`\n",
    "  - `context_matrix_category > 0.5`\n",
    "\n",
    "- All other labels are dropped by setting their score to `-1` in `list_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "z=0\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "    list_el = []\n",
    "    list_el2 = []\n",
    "    list_el3 = []\n",
    "    list_el4 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "\n",
    "\n",
    "    for j in range(30):\n",
    "        \n",
    "        if max_value - list_matrix[i][j][0] <= 0.15:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "            list_el.append((our_classes['label'][list_matrix[i][j][1]], first_sentence_matrix[i][list_matrix[i][j][1]]))\n",
    "            list_el2.append((our_classes['label'][list_matrix[i][j][1]], context_matrix[i][list_matrix[i][j][1]]))\n",
    "            list_el3.append((our_classes['label'][list_matrix[i][j][1]], context_matrix_niche[i][list_matrix[i][j][1]]))\n",
    "            list_el4.append((our_classes['label'][list_matrix[i][j][1]],context_matrix_category[i][list_matrix[i][j][1]]))\n",
    "        \n",
    "    if len(row_filtered)==1:\n",
    "        z+=1\n",
    "    \n",
    "    final_list.append(list111)\n",
    "    list_range = list(range(len(row_filtered)))\n",
    "   \n",
    "    list_range_filtered = list(filter(lambda x: list_el[x][1] > 0.3 or list_el2[x][1]>0.3 or list_el3[x][1]>0.5 or list_el4[x][1]>0.5 , list_range))\n",
    "    if list_range_filtered != []:\n",
    "        new_list = []\n",
    "        for k in list_range_filtered:\n",
    "            new_list.append(list111[k])\n",
    "        for indices in range(len(list_matrix[i])):\n",
    "            if indices not in list_range_filtered:\n",
    "                list_matrix[i][indices] = (-1, list_matrix[i][indices][1])\n",
    "    elements_filtered.append(row_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "row_no1=0\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix_original_2[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix_original_2[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(20):\n",
    "        if max_value - list_matrix_original_2[i][j][0]<=0.1:\n",
    "            list111.append(our_classes['label'][list_matrix_original_2[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "    final_list.append(list111)\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, (idx, _) in enumerate(df.iterrows()):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "    if list_matrix[i][0][0] - list_matrix[i][1][0]>=0.1:\n",
    "        continue\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j, _ in enumerate(elements_filtered[i]):\n",
    "        if max_value - list_matrix[i][j][0] <= 0.1:\n",
    "\n",
    "            if(first_sentence_matrix[i][list_matrix[i][j][1]]>=0.41):\n",
    "                if (context_matrix[i][list_matrix[i][j][1]]>=0.45) and  (first_sentence_matrix[i][list_matrix[i][j][1]]>=0.43):\n",
    "                    list_matrix[i][j] = (list_matrix[i][j][0]+0.30, list_matrix[i][j][1])\n",
    "\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "row_no1=0\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "    for j in range(20):\n",
    "        if max_value - list_matrix[i][j][0] <= 0.1:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "    \n",
    "    final_list.append(list111)\n",
    "\n",
    "    if(len(row_filtered)==1):\n",
    "        row_no1+=1\n",
    "    elements_filtered.append(row_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_values_all = weak_values_all_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector-Aware Label Filtering\n",
    "\n",
    "This module refines the top label predictions by checking if key terms from each label (core/modifier/domain) align with the company's sector and context.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To prevent selecting labels that score high numerically but are semantically inconsistent with the company’s actual business activity.\n",
    "\n",
    "\n",
    "### Logic Summary\n",
    "\n",
    "1. **Normalize Cores**: Convert plural core terms to singular using a mapping dictionary.\n",
    "\n",
    "2. **Sector-Term Vocabulary**:  \n",
    "   - Extract valid business terms from sector metadata and labels.\n",
    "   - Remove sectors not present in any label core or domain.\n",
    "\n",
    "3. **Strong Term Identification**:  \n",
    "   - A term is considered “strong” if its cosine similarity with any sector term > 0.4.\n",
    "\n",
    "4. **Per-Company Validation**:\n",
    "   - For each label, extract a representative term (last term - either domain or core).\n",
    "   - Compare this term’s embeddings with:\n",
    "     - The sector embedding\n",
    "     - Category/niche tokens\n",
    "     - All company context tokens\n",
    "   - If all similarities are below threshold, remove the label.\n",
    "\n",
    "5. **Fallback**:  \n",
    "   - If all filtered labels are eliminated, revert to the original top labels.\n",
    "\n",
    "\n",
    "### Benefit\n",
    "\n",
    "Ensures that selected labels are not just similar in wording, but contextually appropriate based on the company’s sector and descriptive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_cores= []\n",
    "original_cores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for core_ in original_core:\n",
    "    if core_ in convertPluralToSingular:\n",
    "        simplified_cores.append(convertPluralToSingular[core_])\n",
    "        original_cores.append(core_)\n",
    "    else:\n",
    "        original_cores.append(core_)\n",
    "        simplified_cores.append(core_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_element_sectors = set()\n",
    "our_sector=our_sector.dropna()\n",
    "for i in our_sector['sector']:\n",
    "    set_element_sectors.add(i)\n",
    "\n",
    "sectors_last = set()\n",
    "for label in our_classes['label']:\n",
    "    sectors_last.add(label.split(\" \")[-1].lower())\n",
    "\n",
    "sectors_cores = set()\n",
    "for co in original_core:\n",
    "    if co != '0':\n",
    "        sectors_cores.add(co)\n",
    "\n",
    "for idx, la in enumerate(original_domain):\n",
    "    if original_modifier[idx] == '0' and original_cores[idx] == '0':\n",
    "        sectors_cores.add(la)\n",
    "\n",
    "list_set_element_sectors = set_element_sectors.copy()\n",
    "for i in list_set_element_sectors:\n",
    "    if i not in sectors_cores and i in sectors_last:\n",
    "        set_element_sectors.remove(i)\n",
    "set_element_sectors = set(list(filter(lambda x: x!='', set_element_sectors)))\n",
    "list_element_sectors = list(set_element_sectors)\n",
    "\n",
    "list_element_sectors_embeddings = np.array([embeddings_index[wd] for wd in list_element_sectors], dtype=np.float32, order='C')\n",
    "faiss.normalize_L2(list_element_sectors_embeddings)\n",
    "\n",
    "strong_terms = []\n",
    "for i in set(original_core):\n",
    "    if i != '0' and np.max(np.dot(list_element_sectors_embeddings, embeddings_index[i].T))>0.4:\n",
    "        strong_terms.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sector['sector']=our_sector['sector'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index_elements = [-1] * len(our_words['new_col2'])\n",
    "zzz=0\n",
    "for idx, _ in enumerate(our_words['new_col2']):\n",
    "    list_min_tf_idf=[]\n",
    "    for each_word in our_words['new_col2'][idx]:\n",
    "        list_min_tf_idf.append(match_words_with_tf_idf_valuess[(each_word, idx)])\n",
    "    list_min_tf_idf.sort()\n",
    "\n",
    "    list_matrix[idx].sort(reverse=True)\n",
    "    max_val = list_matrix[idx][0][0]\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    words_eliminated=0\n",
    "\n",
    "    copy_list_matrix = list_matrix[idx].copy()\n",
    "\n",
    "    if list_matrix[idx][0][0] - list_matrix[idx][1][0]>=0.10:\n",
    "        continue\n",
    "    no_key_word_eliminated=0\n",
    "    list_matrix_copy_row = list_matrix[idx].copy()\n",
    "    last_element = -1\n",
    "        \n",
    "    for  j in range(30):\n",
    "\n",
    "        if list_matrix[idx][0][0] - list_matrix[idx][j][0]>=0.15:\n",
    "            last_element = j\n",
    "            break\n",
    "\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "      \n",
    "        our_terms = \"\"\n",
    "\n",
    "        if modifier[index_label] == '0' and core[index_label]==\"0\":\n",
    "            our_terms = original_domain[index_label]\n",
    "        else:\n",
    "            our_terms = original_core[index_label]\n",
    "\n",
    "     \n",
    "        if our_terms not in strong_terms:\n",
    "            continue\n",
    "        if our_sector['sector'][idx] in our_words['new_col2'][idx]:\n",
    "            our_array = np.array([embeddings_index[wd] for wd in our_categories_niche_words['niche_plus_cat'][idx]], dtype=np.float32, order='C')\n",
    "            our_array_words = np.array([embeddings_index[wd] for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "\n",
    "            if our_categories_niche_words['niche_plus_cat'][idx] != []:\n",
    "                faiss.normalize_L2(our_array)\n",
    "                max_sim_cat_niche = np.max(np.dot(embeddings_index[our_terms], our_array.T))\n",
    "            else:\n",
    "                max_sim_cat_niche = 0\n",
    "\n",
    "            if our_words['new_col2'][idx] != []:\n",
    "                faiss.normalize_L2(our_array_words)\n",
    "                max_words = np.max(np.dot(embeddings_index[our_terms], our_array_words.T))\n",
    "\n",
    "            else:\n",
    "                max_words = 0\n",
    "\n",
    "\n",
    "            cos_similarity = np.dot(embeddings_index[our_terms],embeddings_index[our_sector['sector'][idx]].T )\n",
    "\n",
    "            if cos_similarity < 0.4 and max_sim_cat_niche < 0.5 and max_words < 0.55:\n",
    "                no_key_word_eliminated+=1     \n",
    "                list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "                \n",
    "            else:        \n",
    "                last_index_elements[idx] = index_label\n",
    "        else:\n",
    "            last_index_elements[idx] = index_label\n",
    "   \n",
    "    if(no_key_word_eliminated==last_element):\n",
    "       \n",
    "       list_matrix[idx] = list_matrix_copy_row.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category and Niche-Aware Boosting and Filtering\n",
    "\n",
    "This module performs selective boosting of label scores based on metadata fields such as `niche` and `category`. The approach ensures that only contextually validated metadata contributes to boosting decisions, avoiding overreliance on noisy or weakly aligned metadata.\n",
    "\n",
    "### Core Logic\n",
    "\n",
    "For each of the top label candidates in `list_matrix[i]`, the algorithm:\n",
    "\n",
    "1. **Checks Label Presence in Niche**\n",
    "   - If the label is found in the `niche` field, it proceeds to validate the `domain`, `modifier`, and `core` components of the label.\n",
    "   - Validation is done using:\n",
    "     - Cosine similarity between the label components and the filtered token set (`new_col2`) of the company.\n",
    "     - Embeddings from `embeddings_index`, normalized via FAISS.\n",
    "   - If the component is not well-supported (cosine similarity < 0.4), the label is either ignored or penalized.\n",
    "   - If all components pass the validation, the label is boosted by +0.30.\n",
    "\n",
    "2. **Checks Label Presence in Category**\n",
    "   - Similar logic as for niche.\n",
    "   - Additional verification is applied using the count of supporting words in `our_counted_words` and `our_counted_words_category`.\n",
    "\n",
    "3. **Contextual Safeguards**\n",
    "   - Even if the label appears in niche or category, its domain/modifier/core must also appear meaningfully in the full description or related fields.\n",
    "   - This ensures that we avoid boosting false positives coming from overly broad, generic, or mismatched metadata.\n",
    "   - If the label passes all checks, it is boosted; otherwise, it is penalized or ignored.\n",
    "\n",
    "4. **Anchor Filtering**\n",
    "   - Labels that do not meet the required semantic similarity thresholds are explicitly filtered.\n",
    "   - The system ensures that labels are only retained if they fall within 0.1 of the top score, and are supported by strong tokens.\n",
    "\n",
    "### Output\n",
    "\n",
    "- `final_list`: Final list of boosted and filtered label names per company.\n",
    "- `elements_filtered`: Intermediate list of (score, label) tuples that survived all filtering stages.\n",
    "\n",
    "This layered approach ensures robustness against misleading metadata and enforces semantic coherence in label assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "df['niche']=df['niche'].fillna(\"\")\n",
    "df['category']=df['category'].fillna(\"\")\n",
    "skip_categories_for_this_one = []\n",
    "\n",
    "i=0\n",
    "\n",
    "zz=0\n",
    "for idx, _ in df.iterrows():\n",
    "    \n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    list111 = []\n",
    "\n",
    "    \n",
    "    row_filtered = []\n",
    "        \n",
    "    for j in range(30):\n",
    "\n",
    "        index_label = list_matrix[i][j][1]\n",
    "        okay = 1\n",
    "        if our_classes['label'][list_matrix[idx][j][1]] in df['niche'][idx]:\n",
    "            okay = 1\n",
    "            if original_domain[index_label]!='0' and original_domain[index_label] not in our_counted_words['count_words'][idx]:\n",
    "                okay = -1\n",
    "            elif our_counted_words['count_words'][idx][original_domain[index_label]] == our_counted_words_category['count_cat'][idx][original_domain[index_label]]:\n",
    "                values_domain = np.array([embeddings_index[wd] if wd != original_domain[index_label] and (wd not in our_categories_niche_words['niche_plus_cat'] or (our_counted_words['count_words'][idx][wd] != our_counted_words_category['count_cat'][idx][wd])) else np.zeros(300) for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(values_domain)\n",
    "                values_list = np.max(np.dot(embeddings_index[original_domain[index_label]], values_domain.T))\n",
    "                if values_list < 0.4:\n",
    "                    okay = -1\n",
    "                    list_matrix[i][j] = (-1, list_matrix[i][j][1])\n",
    "                    context_matrix_category[idx][index_label] = 0\n",
    "\n",
    "                    skip_categories_for_this_one.append(i)\n",
    "            \n",
    "            \n",
    "            if original_modifier[index_label]!='0' and original_modifier[index_label] not in our_counted_words['count_words'][idx]:\n",
    "                okay = -1\n",
    "            elif okay == 1 and modifier[index_label] != '0' and our_counted_words['count_words'][idx][original_modifier[index_label]] == our_counted_words_category['count_cat'][idx][original_modifier[index_label]]:\n",
    "                values_domain = np.array([embeddings_index[wd] if wd != modifier[index_label] and (wd not in our_categories_niche_words['niche_plus_cat'] or our_counted_words['count_words'][idx][wd] != our_counted_words_category['count_cat'][idx][wd])  else np.zeros(300) for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(values_domain)\n",
    "                values_list = np.max(np.dot(embeddings_index[original_modifier[index_label]], values_domain.T))\n",
    "                if values_list < 0.4:\n",
    "                    okay = -1\n",
    "                    \n",
    "\n",
    "            if okay == 1 and core[index_label] != '0' and original_core[index_label] not in our_counted_words['count_words'][idx]:\n",
    "                okay = -1\n",
    "            \n",
    "            elif okay == 1 and core[index_label] != '0' and original_core[index_label] in our_counted_words and our_counted_words['count_words'][idx][original_core[index_label]] == our_counted_words_category['count_cat'][idx][original_core[index_label]]:\n",
    "                values_domain = np.array([embeddings_index[wd] if wd != original_core[index_label] and (wd not in our_categories_niche_words['niche_plus_cat'] or our_counted_words['count_words'][idx][wd] != our_counted_words_category['count_cat'][idx][wd])  else np.zeros(300) for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(values_domain)\n",
    "                values_list = np.max(np.dot(embeddings_index[original_core[index_label]], values_domain.T))\n",
    "                if values_list < 0.4:\n",
    "\n",
    "                    okay = -1\n",
    "            \n",
    "\n",
    "            if (okay==1):\n",
    "                list_matrix[i][j] = (list_matrix[i][j][0]+0.30, list_matrix[i][j][1])\n",
    "                okay = 2\n",
    "\n",
    "\n",
    "        if our_classes['label'][list_matrix[idx][j][1]] in df['category'][idx]!=2:\n",
    "            okay = 1\n",
    "            if original_domain[index_label]!='0' and original_domain[index_label] not in our_counted_words['count_words'][idx]:\n",
    "                okay = -1\n",
    "            elif our_counted_words['count_words'][idx][original_domain[index_label]] == 2:\n",
    "                values_domain = np.array([embeddings_index[wd] if wd != original_domain[index_label] and (wd not in our_categories_niche_words['niche_plus_cat'] or our_counted_words['count_words'][idx][wd] != 2) else np.zeros(300) for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(values_domain)\n",
    "                values_list = np.max(np.dot(embeddings_index[original_domain[index_label]], values_domain.T))\n",
    "                if values_list < 0.4:\n",
    "                    okay = -1\n",
    "                    list_matrix[i][j] = (-1, list_matrix[i][j][1])\n",
    "                    context_matrix_category[idx][index_label] = 0\n",
    "                    skip_categories_for_this_one.append(i)\n",
    "\n",
    "            \n",
    "            \n",
    "            if original_modifier[index_label]!='0' and original_modifier[index_label] not in our_counted_words['count_words'][idx]:\n",
    "                okay = -1\n",
    "            elif okay == 1 and modifier[index_label] != '0' and our_counted_words['count_words'][idx][original_modifier[index_label]] == 2:\n",
    "                values_domain = np.array([embeddings_index[wd] if wd != modifier[index_label] and (wd not in our_categories_niche_words['niche_plus_cat'] or our_counted_words['count_words'][idx][wd] != 2)  else np.zeros(300) for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(values_domain)\n",
    "                values_list = np.max(np.dot(embeddings_index[original_modifier[index_label]], values_domain.T))\n",
    "                if values_list < 0.4:\n",
    "                    okay = -1\n",
    "\n",
    "            if okay == 1 and core[index_label] != '0' and original_core[index_label] not in our_counted_words['count_words'][idx]:\n",
    "                okay = -1\n",
    "            \n",
    "            elif okay == 1 and core[index_label] != '0' and original_core[index_label] in our_counted_words and our_counted_words['count_words'][idx][original_core[index_label]] == 2:\n",
    "                values_domain = np.array([embeddings_index[wd] if wd != original_core[index_label] and (wd not in our_categories_niche_words['niche_plus_cat'] or our_counted_words['count_words'][idx][wd] != 2)  else np.zeros(300) for wd in our_words['new_col2'][idx]], dtype=np.float32, order='C')\n",
    "                faiss.normalize_L2(values_domain)\n",
    "                values_list = np.max(np.dot(embeddings_index[original_core[index_label]], values_domain.T))\n",
    "                if values_list < 0.4:\n",
    "                    okay = -1\n",
    "            \n",
    "\n",
    "            if (okay==1):\n",
    "                list_matrix[i][j] = (list_matrix[i][j][0]+0.30, list_matrix[i][j][1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if list_matrix[i][0][0] - list_matrix[i][j][0] <= 0.1:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "        \n",
    "        \n",
    "        \n",
    "    final_list.append(list111)\n",
    "    \n",
    "    elements_filtered.append(row_filtered)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index_elements2 = [-1] * len(our_words['new_col2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Filtering and Anchor-Based Label Selection\n",
    "\n",
    "In the final label selection step, we focus on **reinforcing matches around the most meaningful tokens** and filtering out noisy or weak candidates.\n",
    "\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Anchor Term Selection**:\n",
    "   - For each company:\n",
    "     - Sort label candidates by similarity score.\n",
    "     - Pick the label with the **highest number of strong matches** (cosine similarity > **0.5**) with:\n",
    "       - Business-important words (`strongest_value_words[idx]`)\n",
    "       - Niche/category terms (`our_categories_niche_words['niche_plus_cat'][idx]`)\n",
    "     - If no matches > 0.5 exist, allow threshold to drop to **0.4**.\n",
    "\n",
    "2. **Fallback to Contextual Support**:\n",
    "   - If there are ties or uncertainty:\n",
    "     - Prefer labels supported by:\n",
    "       - `context_matrix`\n",
    "       - `context_matrix_niche` + high `similarity_to_niche`\n",
    "       - `context_matrix_category` + high `similarity_to_cat`\n",
    "       - `first_sentence_matrix`\n",
    "\n",
    "3. **Fallback to Mean Cosine Similarity**:\n",
    "   - Still unresolved?\n",
    "     - Compute **mean cosine similarity** for each label vs. strong/niche tokens.\n",
    "     - Choose the label with the **highest average similarity** as anchor.\n",
    "\n",
    "4. **Candidate Filtering**:\n",
    "   - Once an anchor is selected:\n",
    "     - Discard any other label with:\n",
    "       - Cosine similarity < **0.55** to anchor, unless structurally related (e.g. modifier/core match).\n",
    "\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- Keeps labels **centered around the most relevant and specific tokens**.\n",
    "- Prioritizes **semantic closeness** and **contextual fit** over weak textual overlap.\n",
    "- Produces **tighter, more meaningful matches** for each company profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_rows = []\n",
    "final_list = []\n",
    "array_list = []\n",
    "list_best_cat = []\n",
    "last_index_elements2 = [-1] * len(our_words['new_col2'])\n",
    "for idx in range(len(our_words['new_col2'])):\n",
    "    list_matrix[idx].sort(reverse=True)\n",
    "\n",
    "    max_value11 = list_matrix[idx][0][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    if(len(elements_filtered[idx])==1 or max_value11 - list_matrix[idx][1][0]>=0.1):\n",
    "        last_index_elements2[idx] = list_matrix[idx][1][1]\n",
    "        continue\n",
    "\n",
    "    rows = []\n",
    "    rows_emb = []\n",
    "\n",
    "    list111 = []\n",
    "    row_filtered=[]\n",
    "    mean_values = []\n",
    "    mean_values_total = []\n",
    "\n",
    "    row_embeddings = []\n",
    "    our_mean_embeddings = []\n",
    "    rows_emb4_5 = []\n",
    "\n",
    "    set_values = set(strong_values_all[idx])\n",
    "    set_values.update(our_categories_niche_words['niche_plus_cat'][idx])\n",
    "    list_val = set_values\n",
    "   \n",
    "    \n",
    "    if list_val == []:\n",
    "        continue\n",
    "    embeddings_value_all = np.array([embeddings_index.get(val, np.zeros(300)) if val in weak_values_all[idx]  else np.zeros(300) for val in our_words['new_col2'][idx]],  dtype=np.float32, order='C')\n",
    "    embeddings_value = np.array([embeddings_index.get(val, np.zeros(300)) for val in list_val],  dtype=np.float32, order='C')\n",
    "\n",
    "    \n",
    "    \n",
    "    if list_val == set():\n",
    "        continue\n",
    "    faiss.normalize_L2(embeddings_value)\n",
    "   \n",
    "    for j in range(len(elements_filtered[idx])):\n",
    "        list111.append(our_classes['label'][list_matrix[idx][j][1]])\n",
    "        row_filtered.append(list_matrix[idx][j])\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "\n",
    "        our = 0\n",
    "        our_values = np.dot(embeddings_index[original_domain[index_label]].reshape(1,-1), embeddings_value.T)[0]\n",
    "\n",
    "        score_b = 1\n",
    "        score_a = 0\n",
    "        embedding_val = None\n",
    "        penalizer_genericity = 1.0\n",
    "        our_cores_value = np.zeros(300)\n",
    "        if len(list_elements[index_label])==1:\n",
    "           embedding_val = embeddings_index[original_domain[index_label]]\n",
    "           our_values_all = np.dot(embeddings_index[original_domain[index_label]], embeddings_value_all.T)\n",
    "\n",
    "           if len(original_domain[index_label].split(\"_\"))>1 and original_domain[index_label] == domain[index_label]:\n",
    "        \n",
    "                our_values_temp = 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "\n",
    "                our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "               \n",
    "                if (len(our_values_45_temp) > len(our_values_45)) or (len(our_values_5_temp) > len(our_values_5)):\n",
    "                    our_values = our_values_temp\n",
    "\n",
    "        elif len(list_elements[index_label])==2:\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.3\n",
    "                score_b = 0.7\n",
    "        \n",
    "                our_values = our_values * 0.3 + 0.7 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.2 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.425 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T) + 0.375 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T) \n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "               \n",
    "                    if (len(our_values_5_temp) > len(our_values_5) or len(our_values_45_temp) > len(our_values_45)):\n",
    "                        our_values = our_values_temp\n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.3\n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_b = 0.1\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "                penalizer_genericity = 0.7\n",
    "                \n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.5\n",
    "                score_b = 0.5\n",
    "             \n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.3 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.4 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T)[0] + 0.3 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T)[0] \n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_45_temp) > len(our_values_45) or len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "                if len(original_domain[index_label].split(\"_\"))>1:\n",
    "                    our_values_temp = 0.3 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.4 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + 0.3 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_5_temp) > len(our_values_5) or len(our_values_45_temp) > len(our_values_45):\n",
    "                        our_values = our_values_temp\n",
    "            \n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "               \n",
    "                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_a = 0.9\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_a = 0.7\n",
    "                score_b= 1 - score_a\n",
    "    \n",
    "               \n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_core[index_label]].reshape(1,-1), embeddings_value.T)[0]\n",
    "                if len(original_domain[index_label].split(\"_\"))>1:\n",
    "                    if len(core[index_label].split(\"::\"))>1:\n",
    "                        score_c1 = 0.1\n",
    "                        score_a1 = 0.4\n",
    "                        score_b1 = 0.5\n",
    "                    elif len(core[index_label].split(\"__\"))>1:\n",
    "                        score_c1 = 0.2\n",
    "                        score_a1 = 0.35\n",
    "                        score_b1 = 0.45\n",
    "\n",
    "                    our_values_temp = score_a1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + score_b1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + score_c1 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "                    \n",
    "                    if len(our_values_5_temp) > len(our_values_5) or len(our_values_45_temp) > len(our_values_45):\n",
    "                        our_values = our_values_temp\n",
    "            \n",
    "            our_cores_value = np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            embedding_val = (score_a/(score_a+score_b)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b)) * embeddings_index[original_core[index_label]]\n",
    "            our_values_all = np.dot(embeddings_index[original_domain[index_label]], embeddings_value_all.T) * (score_a/(score_a+score_b))  + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value_all.T)\n",
    "\n",
    "        elif len(list_elements[index_label])==3:\n",
    "            score_a = 0.3\n",
    "            score_b = 0.4\n",
    "            score_c = 0.3\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.2\n",
    "                score_b = 0.425\n",
    "                score_c = 0.375                \n",
    "                our_values = our_values * 0.2 + 0.425 * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T)[0] + 0.375 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)[0]\n",
    "  \n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_c = 0.1\n",
    "                    score_a = 0.235\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_c = 0.2\n",
    "                    score_a = 0.2\n",
    "                score_b = 1 - score_a - score_c\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T)[0] + + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)[0]\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label] and len(core[index_label].split(\"::\"))>1:\n",
    "                score_a = 0.425\n",
    "                score_c = 0.1\n",
    "                score_b = 1 - score_a - score_c\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.5\n",
    "                    score_a = 0.3 \n",
    "                    score_c = 1 - score_a-score_b\n",
    "                else:\n",
    "                    score_b = 0.45\n",
    "                    score_a = 0.35 \n",
    "                    score_c = 1 - score_a - score_b\n",
    "            \n",
    "            our_cores_value = np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            our_values = our_values * (score_a/(score_a+score_b+score_c)) + (score_b/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + (score_c/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            our_values_all = np.dot(embeddings_index[original_domain[index_label]], embeddings_value_all.T) * (score_a/(score_a+score_b+score_c)) + (score_b/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value_all.T) + (score_c/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value_all.T)\n",
    "\n",
    "            embedding_val = (score_a/(score_a+score_b+score_c)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b+score_c)) * embeddings_index[original_modifier[index_label]]+ (score_c/(score_a+score_b+score_c)) * embeddings_index[original_core[index_label]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        range_list_val = list(range(len(our_values)))\n",
    "\n",
    "        length_no6 = list(filter(lambda x: (x * penalizer_genericity) >= 0.6, our_values)) \n",
    "        length_no = list(filter(lambda x: (x * penalizer_genericity) >= 0.5, our_values)) \n",
    "        length_no4_5 = list(filter(lambda x: (x * penalizer_genericity) >= 0.45, our_values)) \n",
    "\n",
    "        if core[index_label] != original_core[index_label]:\n",
    "            range_list_val = list(filter(lambda x: our_cores_value[x]<0.6 or our_cores_value[x] <= our_values[x], range_list_val))\n",
    "\n",
    "            our_values_temp = []\n",
    "            for val in range_list_val:\n",
    "                our_values_temp.append(our_values[val])\n",
    "            our_values = our_values_temp.copy()\n",
    "\n",
    "\n",
    "        max_value = 0\n",
    "      \n",
    "        \n",
    "        length_no6 = list(filter(lambda x: (x * penalizer_genericity) >= 0.6, our_values)) \n",
    "        length_no = list(filter(lambda x: (x * penalizer_genericity) >= 0.5, our_values)) \n",
    "        length_no4_5 = list(filter(lambda x: (x * penalizer_genericity) >= 0.45, our_values)) \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "       \n",
    "        length_no4 = list(filter(lambda x: (x * penalizer_genericity) >= 0.4, our_values))\n",
    "\n",
    "\n",
    "        if len(length_no) > 0: \n",
    "            mean_values_total.append(np.mean(list(map(lambda x: x, length_no))))\n",
    "        else:\n",
    "            mean_values_total.append(0)\n",
    "\n",
    "        our=len(length_no)\n",
    "        rows.append(our)\n",
    "        rows_emb.append(length_no)\n",
    "        rows_emb4_5.append(length_no4_5)\n",
    "        our_mean_embeddings = our_values\n",
    "        list_elements2 = our_mean_embeddings\n",
    "        row_embeddings.append(embedding_val)\n",
    "\n",
    "\n",
    "    max_values = 0\n",
    "    mean_max_values = -1\n",
    "    index_x = -1\n",
    "    max_index=-1\n",
    "    indices = []\n",
    "\n",
    "    for idx4, ivv in enumerate(rows):\n",
    "\n",
    "        if max_values < ivv and mean_values_total[idx4] >= 0.085:\n",
    "            max_values = ivv\n",
    "            index_x = idx4\n",
    "            indices = [idx4]\n",
    "        elif max_values == ivv and mean_values_total[idx4] >= 0.085:\n",
    "            indices.append(idx4)\n",
    "    \n",
    "\n",
    "    indices_value = indices.copy()\n",
    "    if max_values != 0:\n",
    "        indices_elements_copy = []\n",
    "        for idx4 in indices:\n",
    "            \n",
    "            if context_matrix[idx][list_matrix[idx][idx4][1]]>0.45 or (context_matrix_niche[idx][list_matrix[idx][idx4][1]]>0.45 and similarity_to_niche[idx][idx] > 0.3) or (context_matrix_category[idx][list_matrix[idx][idx4][1]]>0.45 and similarity_to_cat[idx][idx]>0.4) or first_sentence_matrix[idx][list_matrix[idx][idx4][1]]>0.45:\n",
    "                indices_elements_copy.append(idx4)\n",
    "                index_x = idx4\n",
    "            \n",
    "        if indices_elements_copy != []:\n",
    "            indices_value = indices_elements_copy.copy()\n",
    "    \n",
    "\n",
    "    if max_values==0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    max_len_4_5_values = -1\n",
    "    list_indices_temp = []\n",
    "    if len(indices_value) > 1:\n",
    "        for idx3 in indices_value:\n",
    "            mean = 0\n",
    "            ivv = list_matrix[idx][idx3]\n",
    "            if len(rows_emb[idx3])!=0:\n",
    "                mean_val = np.mean(rows_emb[idx3])\n",
    "            length_4_5 = len(rows_emb4_5[idx3])\n",
    "        \n",
    "            if (length_4_5 > max_len_4_5_values):\n",
    "                max_len_4_5_values = length_4_5\n",
    "                index_x = idx3\n",
    "                list_indices_temp = [idx3]\n",
    "            elif (length_4_5 == max_len_4_5_values):\n",
    "                list_indices_temp.append(idx3)\n",
    "\n",
    "        if len(list_indices_temp) == 0:\n",
    "            indices_value = list_indices_temp.copy()\n",
    "    if len(indices_value) > 1:\n",
    "        mean_val = -1\n",
    "        for idx3  in indices_value:\n",
    "            ivv = list_matrix[idx][idx3]\n",
    "            if len(rows_emb4_5[idx3]) > 0:\n",
    "                mean_ = np.mean(rows_emb4_5[idx3])\n",
    "\n",
    "            else:\n",
    "                mean_ = 0\n",
    "            if mean_ > mean_val:\n",
    "                index_x = idx3\n",
    "                mean_val = mean_\n",
    "\n",
    "    list_best_cat.append(index_x)\n",
    "\n",
    "    for j in range(len(rows)):\n",
    "        max_new=-1\n",
    "        element = cosine_similarity(row_embeddings[j].reshape(1,-1), row_embeddings[index_x].reshape(1,-1))\n",
    "        mean = -1\n",
    "        if element.size > 0:\n",
    "            mean = np.mean(element)\n",
    "        else:\n",
    "            mean = 0\n",
    "        values_6 = list(filter(lambda x: x>=0.6, rows_emb[j]))\n",
    "        length_4_5 = len(rows_emb4_5[j]) + len(values_6)\n",
    "\n",
    "        is_a_label_subset_of_another_label = original_modifier[list_matrix[idx][index_x][1]] == original_domain[list_matrix[idx][j][1]] and original_core[list_matrix[idx][index_x][1]] == original_core[list_matrix[idx][j][1]]\n",
    "        is_a_label_subset_of_another_label = is_a_label_subset_of_another_label or original_core[list_matrix[idx][index_x][1]] == original_domain[list_matrix[idx][j][1]] and original_modifier[list_matrix[idx][index_x][1]] == \"0\" and original_modifier[list_matrix[idx][j][1]] == \"0\"\n",
    "\n",
    "        if mean <= 0.75 or is_a_label_subset_of_another_label: \n",
    "            list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "            array_list.append(idx)\n",
    "        else:\n",
    "            last_index_elements2[idx] = list_matrix[idx][len(elements_filtered[idx])][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "i=0\n",
    "\n",
    "zz=0\n",
    "ridiculous_number = 0\n",
    "for idx, _ in df.iterrows():\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "    list111_og = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "    row_filtered = []\n",
    "    \n",
    "\n",
    "    for j in range(30):\n",
    "\n",
    "        \n",
    "        if(list_matrix[i][j][1]==last_index_elements2[i]):\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        if max_value - list_matrix[i][j][0] <= 0.15:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "                   \n",
    "        \n",
    "    final_list.append(list111)  \n",
    "     \n",
    "    elements_filtered.append(row_filtered)\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category-Based and Text-Based Anchor Selection\n",
    "\n",
    "This stage filters out semantically weak or ambiguous labels, ensuring that only contextually relevant and structurally meaningful candidates remain. It runs in two passes:\n",
    "\n",
    "\n",
    "### 1. Category-Based Anchor Filtering (Strong and Niche Tokens)\n",
    "\n",
    "Selects an anchor label based on alignment with company’s important tokens (`strong_values_all[idx]`) and metadata (`niche_plus_cat[idx]`).\n",
    "\n",
    "#### Process\n",
    "\n",
    "- **Skip Filtering** if:\n",
    "  - Only one candidate in `elements_filtered[idx]`\n",
    "  - Or top label is clearly dominant (score gap ≥ 0.1)\n",
    "\n",
    "- **Embedding Preparation**:\n",
    "  - For each candidate label:\n",
    "    - Build a **weighted embedding** using `domain`, `core`, and `modifier`\n",
    "    - Adjust weights based on:\n",
    "      - Structural overlap (e.g., `domain = core`)\n",
    "      - Genericity (e.g., `__`, `::`)\n",
    "      - Compound splitting (e.g., `agriculture_machinery`)\n",
    "\n",
    "- **Similarity Evaluation**:\n",
    "  - Cosine ≥ 0.475 → `row_values`\n",
    "  - Cosine ≥ 0.4 → `row_values_4`\n",
    "  - Mean ≥ 0.3 → `mean_values`\n",
    "  - Mean ≥ 0.475 → `mean_values_total`\n",
    "\n",
    "- **Anchor Selection**:\n",
    "  - Choose label with:\n",
    "    - Most matches ≥ 0.475\n",
    "    - `mean_values_total ≥ 0.085`\n",
    "    - Contextual alignment via:\n",
    "      - `context_matrix`\n",
    "      - `context_matrix_niche`\n",
    "      - `context_matrix_category`\n",
    "      - `first_sentence_matrix`\n",
    "\n",
    "- **Tie-Breaking**:\n",
    "  1. Prefer high contextual support\n",
    "  2. Then highest `mean_values`\n",
    "  3. Then most matches ≥ 0.4\n",
    "  4. Then highest `mean` in `row_values_4`\n",
    "\n",
    "#### Filtering\n",
    "\n",
    "- Discard labels with:\n",
    "  - Cosine to anchor < 0.75  \n",
    "  - Unless structurally related (e.g., shared domain/core)\n",
    "\n",
    "\n",
    "### 2. Text-Based Anchor Filtering (All Company Tokens)\n",
    "\n",
    "This step refines label selection by comparing each label candidate against **all company tokens** (excluding only the **very weakest terms**), ensuring structural and semantic alignment beyond top-scoring labels.\n",
    "\n",
    "\n",
    "### When It Runs\n",
    "\n",
    "- Only if the **top 2 label scores differ by less than 0.1**\n",
    "- Skipped if:\n",
    "  - Only one label survives early filters (`elements_filtered[idx]`)\n",
    "  - The top label is clearly dominant\n",
    "\n",
    "### Anchor Selection Logic\n",
    "\n",
    "For each label in `elements_filtered[idx]`:\n",
    "\n",
    "- Compute semantic similarity between the label's domain/core/modifier and **all tokens in `new_col2[idx]`**\n",
    "- Apply component-specific blending:\n",
    "  - One-part labels: use domain embedding\n",
    "  - Two-part: blend domain + core (adjusted by specificity)\n",
    "  - Three-part: weighted blend of domain, modifier, core (penalize genericity)\n",
    "\n",
    "- Retain similarity values:\n",
    "  - `length_no`: tokens with cosine ≥ 0.475\n",
    "  - `length_no2`: tokens with cosine ≥ 0.3\n",
    "  - `length_no4`: tokens with cosine ≥ 0.4\n",
    "\n",
    "- Use contextual support as secondary signal:\n",
    "  - Accept label if any of:\n",
    "    - `context_matrix[idx][label_id] > 0.3`\n",
    "    - `context_matrix_niche[idx][label_id] > 0.5` and `similarity_to_niche[idx][idx] > 0.3`\n",
    "    - `context_matrix_category[idx][label_id] > 0.5` and `similarity_to_cat[idx][idx] > 0.3`\n",
    "    - `first_sentence_matrix[idx][label_id] > 0.3`\n",
    "\n",
    "### Anchor Label Selection (Tie-Breaking Priority)\n",
    "\n",
    "1. Labels supported by **any context matrix ≥ 0.45**\n",
    "2. Label with highest **mean similarity** over values ≥ 0.475\n",
    "3. Label with highest count of values ≥ 0.4\n",
    "4. Label with highest overall mean similarity if tie persists\n",
    "\n",
    "\n",
    "### Filtering Logic (Post-Anchor)\n",
    "\n",
    "- After selecting the best anchor:\n",
    "  - Discard any other label if:\n",
    "    - Cosine similarity to anchor < 0.85\n",
    "    - Mean similarity to company tokens < 0.1\n",
    "    - Structurally redundant (e.g., label is just a weaker modifier/domain variant)\n",
    "    - Any of domain, core, modifier similarity < 0.4\n",
    "\n",
    "- Labels with **maximum cosine > 0.9** and strong values ≥ 0.4 may be re-added from `values_to_reconsider`\n",
    "\n",
    "\n",
    "### Result\n",
    "\n",
    "- `list_matrix[idx]` is updated to keep only:\n",
    "  - A **semantically grounded** label\n",
    "  - **Contextually validated** through BERT matrices\n",
    "  - Structurally sound and specific\n",
    "- Ensures consistency even when multiple label candidates are equally strong in FAISS\n",
    "\n",
    "- Anchors are selected based on structure, semantics, and context.\n",
    "- Ensures that final label:\n",
    "  - Matches important tokens and metadata\n",
    "  - Is supported by contextual embeddings\n",
    "  - Avoids generic or structurally vague alternatives\n",
    "- Produces stable, interpretable, and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rows = []\n",
    "final_list = []\n",
    "\n",
    "for idx in range(len(our_words['new_col2'])):\n",
    "    list_matrix[idx].sort(reverse=True)\n",
    "\n",
    "    max_value11 = list_matrix[idx][0][0]\n",
    "\n",
    "    \n",
    "\n",
    "    if(len(elements_filtered[idx])==1):\n",
    "        continue\n",
    "\n",
    "    if(list_matrix[idx][0][0] - list_matrix[idx][1][0]>=0.1):\n",
    "        continue\n",
    "\n",
    "    rows = []\n",
    "\n",
    "\n",
    "    list111 = []\n",
    "    row_filtered=[]\n",
    "    mean_values = []\n",
    "    mean_values_total = []\n",
    "    row_values = []\n",
    "    row_values_4 = []\n",
    "\n",
    "    row_embeddings = []\n",
    "   \n",
    "    for j in range(len(elements_filtered[idx])):\n",
    "        list111.append(our_classes['label'][list_matrix[idx][j][1]])\n",
    "        row_filtered.append(list_matrix[idx][j])\n",
    "      \n",
    "    \n",
    "    \n",
    "        \n",
    "       \n",
    "        index_label = list_matrix[idx][j][1]\n",
    "\n",
    "        our = 0\n",
    "        our_mean_embeddings = []\n",
    "        \n",
    "        embeddings_value = np.array([embeddings_index[val] if val not in weak_values_all[35][:-(len(weak_values_all[35])//2)] else np.zeros(300) for val in our_words['new_col2'][idx]],  dtype=np.float32, order='C')\n",
    "        our_values = np.dot(embeddings_index[original_domain[index_label]], embeddings_value.T)\n",
    "        score_b = 1\n",
    "        score_a = 0\n",
    "        embedding_val = None\n",
    "\n",
    "            \n",
    "\n",
    "        if len(list_elements[index_label])==1:\n",
    "           embedding_val = embeddings_index[original_domain[index_label]]\n",
    "           our_values = list(map(lambda x: x, our_values))\n",
    "           if len(original_domain[index_label].split(\"__\"))>1:\n",
    "                our_values_temp = 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                if len(our_values_5_temp) > len(our_values_5):\n",
    "                    our_values = our_values_temp\n",
    "\n",
    "\n",
    "        \n",
    "        elif len(list_elements[index_label])==2:\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.3\n",
    "                score_b = 0.7\n",
    "\n",
    "                our_values = our_values * 0.3 + 0.7 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)[0]\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.2 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.425 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T) + 0.375 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "\n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.3\n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_b = 0.1\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "\n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                \n",
    "            \n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.5\n",
    "                score_b = 0.5\n",
    "               \n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.3 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.4 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T) + 0.3 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "                if  len(original_domain[index_label].split(\"__\"))>1:\n",
    "                    our_values_temp = 0.3 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.4 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + 0.3 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "               \n",
    "                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_b = 0.1\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "                score_a = 1 - score_b\n",
    "                \n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_domain[index_label].split(\"__\"))>1:\n",
    "                    if len(core[index_label].split(\"::\"))>1:\n",
    "                        score_c1 = 0.1\n",
    "                        score_a1 = 0.4\n",
    "                        score_b1 = 0.5\n",
    "                    elif len(core[index_label].split(\"__\"))>1:\n",
    "                        score_c1 = 0.2\n",
    "                        score_a1 = 0.35\n",
    "                        score_b1 = 0.45\n",
    "\n",
    "                    our_values_temp = score_a1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + score_b1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + score_c1 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "                \n",
    "            embedding_val = (score_a/(score_a+score_b)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b)) * embeddings_index[original_core[index_label]]\n",
    "      \n",
    "        elif len(list_elements[index_label])==3:\n",
    "            score_a = 0.3\n",
    "            score_b = 0.4\n",
    "            score_c = 0.3\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.2\n",
    "                score_b = 0.425\n",
    "                score_c = 0.375\n",
    "\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.325\n",
    "                    score_c = 0.45\n",
    "                    score_a = 0.225\n",
    "                    our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                else:\n",
    "                    our_values = our_values * 0.2 + 0.425 * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + 0.375 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "\n",
    "                \n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.25\n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_c = 0.1\n",
    "                    score_a = 0.20\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_c = 0.25\n",
    "\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.35\n",
    "                    score_c+= 0.075\n",
    "                    score_a+= 0.075\n",
    "                else:\n",
    "                    score_b = 1 - score_a - score_c\n",
    "\n",
    "                our_values = our_values * (score_a/(score_a+score_b+score_c)) + (score_b/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + (score_c/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "                    score_a = 0.35\n",
    "                    score_c = 0.35\n",
    "                \n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.35\n",
    "                score_b = 0\n",
    "                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_c = 0.1\n",
    "\n",
    "                    score_a = 0.4\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_c = 0.2\n",
    "                    score_a = 0.35\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.35\n",
    "                    score_a = 1 - score_a-score_b\n",
    "                else:\n",
    "                    score_b = 1 - score_a - score_c\n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "\n",
    "            embedding_val = (score_a/(score_a+score_b+score_c)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b+score_c)) * embeddings_index[original_modifier[index_label]]+ (score_c/(score_a+score_b+score_c)) * embeddings_index[original_core[index_label]]\n",
    "        \n",
    "     \n",
    "        \n",
    "        elements = our_values\n",
    "        length_no = []\n",
    "        length_no4 = []\n",
    "        length_no2 = []\n",
    "        length_no_avg = []\n",
    "        if context_matrix[idx][list_matrix[idx][j][1]] >= 0.075 or np.max(context_matrix[idx]) <= 0.25:\n",
    "            length_no = list(filter(lambda x: x >= 0.475 , elements)) \n",
    "            length_no2 = list(filter(lambda x: x >= 0.3 , elements))\n",
    "            length_no4 = list(filter(lambda x: x >= 0.4 , elements))\n",
    "            length_no_avg = list(filter(lambda x: x <=0.45 , elements))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if length_no2 != list():\n",
    "            mean_values.append(np.mean(length_no2))\n",
    "        else:\n",
    "            mean_values.append(np.mean(np.zeros(300)))\n",
    "        if length_no != list():\n",
    "            mean_values_total.append(np.mean(length_no))\n",
    "        else:\n",
    "            mean_values_total.append(np.mean(np.zeros(300)))\n",
    "\n",
    "\n",
    "        our+=len(length_no)\n",
    "        rows.append(our)\n",
    "        row_values_4.append(length_no4)\n",
    "        row_values.append(length_no)\n",
    "\n",
    "        row_embeddings.append(embedding_val)\n",
    "    max_values = 0\n",
    "    mean_max_values = -1\n",
    "    index_x = -1\n",
    "    max_index=-1\n",
    "\n",
    "    \n",
    "    max_mean_5 = -1\n",
    "    indices_value = []\n",
    "    final_indices_val = -1\n",
    "    values_to_reconsider = []\n",
    "    for idx4, ivv in enumerate(rows):\n",
    "\n",
    "        if max_values < ivv and mean_values_total[idx4] >= 0.085:\n",
    "            \n",
    "            max_values = ivv\n",
    "            index_x = idx4\n",
    "            mean_val_mid = sum(row_values[idx4]) / len(row_values[idx4])\n",
    "            \n",
    "            if max_mean_5 < mean_val_mid:\n",
    "                max_mean_5 = mean_val_mid\n",
    "            indices_value = [idx4]\n",
    "        elif max_values == ivv and mean_values_total[idx4] >= 0.085 :\n",
    "            indices_value.append(idx4)\n",
    "\n",
    "\n",
    "    for idx4, ivv in enumerate(rows):\n",
    "        if index_x != idx4 and  mean_values_total[idx4] >= 0.085 and np.max(row_values[idx4]) > 0.9 and len(row_values_4[idx4]) >= len(row_values_4[index_x]):\n",
    "            indices_value.append(idx4)\n",
    "            values_to_reconsider.append(idx4)\n",
    "\n",
    "    if max_mean_5 < 0.65 and max_values != 0:\n",
    "        indices_elements_copy = []\n",
    "        for idx4 in indices_value:\n",
    "            \n",
    "            if context_matrix[idx][list_matrix[idx][idx4][1]]>0.45 or (context_matrix_niche[idx][list_matrix[idx][idx4][1]]>0.45 and similarity_to_niche[idx][idx] > 0.3) or (context_matrix_category[idx][list_matrix[idx][idx4][1]]>0.45 and similarity_to_cat[idx][idx]) or first_sentence_matrix[idx][list_matrix[idx][idx4][1]]>0.45:\n",
    "                indices_elements_copy.append(idx4)\n",
    "        if indices_elements_copy != []:\n",
    "            indices_value = indices_elements_copy.copy()\n",
    "\n",
    "    if max_mean_5 < 0.65 or len(indices_value)>1:\n",
    "        indices_value_copy = []\n",
    "\n",
    "        for idx4 in indices_value:\n",
    "            ivv = len(row_values_4[idx4])\n",
    "\n",
    "            if max_values < ivv and mean_values_total[idx4] >= 0.085:\n",
    "                max_values = ivv\n",
    "                index_x = idx4\n",
    "                indices_value_copy=[idx4]\n",
    "            elif max_values == ivv and mean_values_total[idx4] >= 0.085:\n",
    "                indices_value_copy.append(idx4)\n",
    "            elif idx4 in values_to_reconsider:\n",
    "                indices_value_copy.append(idx4)\n",
    "        if indices_value_copy != []:\n",
    "            indices_value = indices_value_copy.copy()\n",
    "\n",
    "    if max_values!=0 and len(indices_value) > 1:\n",
    "        indices_value_copy = []\n",
    "        max_values = len(list(row_values[index_x]))\n",
    "        mean_max_values = 0\n",
    "\n",
    "        for idx3 in indices_value:\n",
    "\n",
    "            row_values_final = list(row_values[idx3])\n",
    "            ivv = len(list(row_values[idx3]))\n",
    "            mean_val = 0\n",
    "            if len(row_values_final) != 0:\n",
    "                mean_val = np.mean(row_values_final)            \n",
    "            \n",
    "            if (max_values == ivv and mean_val > mean_max_values):\n",
    "                mean_max_values = mean_val\n",
    "                index_x = idx3\n",
    "                indices_value_copy= [idx3]\n",
    "        \n",
    "        for idx3, ivv in enumerate(rows):\n",
    "            if idx3 in values_to_reconsider:\n",
    "                indices_value_copy.append(idx3)\n",
    "        \n",
    "        if indices_value_copy != []:\n",
    "            indices_value = indices_value_copy.copy()\n",
    "    \n",
    "    if max_values == 0 or len(indices_value) > 1:\n",
    "        \n",
    "    \n",
    "        mean_max_values = -1\n",
    "        index_x=-1\n",
    "        if max_values==0:\n",
    "            indices_value = list(range(len(row_values_4)))  \n",
    "        max_values = len(row_values_4[0])\n",
    "        for idx3, _ in enumerate(indices_value):\n",
    "            mean_val = 0\n",
    "            \n",
    "            \n",
    "            if len(row_values_4[idx3]) != 0:\n",
    "                mean_val = np.mean(row_values_4[idx3])\n",
    "            \n",
    "            if (max_values == len(row_values_4[idx3]) and mean_val > mean_max_values):\n",
    "                mean_max_values = mean_val\n",
    "                max_values = len(row_values_4[idx3])\n",
    "                index_x = idx3\n",
    "            \n",
    "            \n",
    "    for j in range(len(rows)):\n",
    "        max_new=-1\n",
    "        element = cosine_similarity(row_embeddings[j].reshape(1,-1), row_embeddings[index_x].reshape(1,-1))\n",
    "        mean = -1\n",
    "        if element.size > 0:\n",
    "            mean = np.mean(element)\n",
    "        else:\n",
    "            mean = 0\n",
    "\n",
    "        mean_val = 0\n",
    "        if len(row_values[j]) > 1:\n",
    "            mean_val = np.mean(row_values[j])\n",
    "        \n",
    "        is_a_label_subset_of_another_label = original_modifier[list_matrix[idx][index_x][1]] == original_domain[list_matrix[idx][j][1]] and original_core[list_matrix[idx][index_x][1]] == original_core[list_matrix[idx][j][1]]\n",
    "        \n",
    "        is_a_label_subset_of_another_label = is_a_label_subset_of_another_label or original_core[list_matrix[idx][index_x][1]] == original_domain[list_matrix[idx][j][1]] and original_modifier[list_matrix[idx][index_x][1]] == \"0\" and original_modifier[list_matrix[idx][j][1]] == \"0\" and original_core[list_matrix[idx][j][1]]==\"0\"\n",
    "        \n",
    "        is_a_label_subset_of_another_label = is_a_label_subset_of_another_label or original_domain[list_matrix[idx][index_x][1]] == original_core[list_matrix[idx][j][1]] and original_modifier[list_matrix[idx][index_x][1]] == \"0\" and original_modifier[list_matrix[idx][j][1]] == \"0\" and original_core[list_matrix[idx][index_x][1]]==\"0\"\n",
    "\n",
    "        \n",
    "        if (mean <= 0.85 and not is_a_label_subset_of_another_label):\n",
    "            list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "            array_list.append(idx)\n",
    "        else:\n",
    "            \n",
    "            if mean >=0.85 and mean < 0.99 or is_a_label_subset_of_another_label:\n",
    "                min_value = 1\n",
    "                min_value2 = 1\n",
    "                min_value3 = 1\n",
    "                min_value = np.max(np.dot(embeddings_index[original_domain[list_matrix[idx][index_x][1]]], embeddings_value.T))\n",
    "                if original_modifier[list_matrix[idx][index_x][1]] != \"0\":\n",
    "                    min_value2 = np.max(np.dot(embeddings_index[original_modifier[list_matrix[idx][index_x][1]]], embeddings_value.T))\n",
    "                if original_core[list_matrix[idx][j][1]] != \"0\":\n",
    "                    min_value3 = np.max(np.dot(embeddings_index[original_core[list_matrix[idx][j][1]]], embeddings_value.T))\n",
    "                if len(original_core[list_matrix[idx][j][1]].split(\"_\"))>1:\n",
    "                    min_value11 = np.min(np.dot(embeddings_index[original_core[list_matrix[idx][index_x][1]].split(\"_\")[0]], embeddings_value.T))\n",
    "                    min_value1= np.min(np.dot(embeddings_index[original_core[list_matrix[idx][index_x][1]].split(\"_\")[1]], embeddings_value.T))  \n",
    "                    min_value = np.min([min_value, min_value11, min_value1])\n",
    "                if len(original_domain[list_matrix[idx][j][1]].split(\"_\"))>1:\n",
    "                    min_value22 = np.min(np.dot(embeddings_index[original_domain[list_matrix[idx][j][1]].split(\"_\")[0]], embeddings_value.T))\n",
    "                    min_value2= np.min(np.dot(embeddings_index[original_domain[list_matrix[idx][j][1]].split(\"_\")[1]], embeddings_value.T))  \n",
    "                    min_value = np.min([min_value, min_value22, min_value2])\n",
    "\n",
    "                min_value = np.min([min_value, min_value2, min_value3])\n",
    "                \n",
    "                if min_value < 0.4:\n",
    "                    list_matrix[idx][j] = (-1, list_matrix[idx][j][1])\n",
    "                    array_list.append(idx)\n",
    "            last_index_elements2[idx] = list_matrix[idx][len(elements_filtered[idx])][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "no_elements=0\n",
    "no_elements_not_okay = 0\n",
    "\n",
    "for i in range(len(df['description'])):\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "\n",
    "    row_filtered = []\n",
    "\n",
    "    for j in range(30):\n",
    "\n",
    "        if(list_matrix[i][j][1]==last_index_elements2[i]):\n",
    "            break\n",
    "\n",
    "        if max_value - list_matrix[i][j][0] <= 0.1:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    if (list_matrix[i][0][0]) < 0.30:\n",
    "        no_elements_not_okay +=1\n",
    "    \n",
    "    elements_filtered.append(row_filtered)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "i=0\n",
    "\n",
    "zz=0\n",
    "ridiculous_number = 0\n",
    "for idx, _ in df.iterrows():\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "    row_filtered = []\n",
    "    \n",
    "\n",
    "    for j in range(30):\n",
    "\n",
    "        \n",
    "        if(list_matrix[i][j][1]==last_index_elements2[i]):\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        if max_value - list_matrix[i][j][0] <= 0.10:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "                   \n",
    "        \n",
    "    final_list.append(list111)    \n",
    "    elements_filtered.append(row_filtered)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_text(our_matrix, idx, magic_number):\n",
    "\n",
    "\n",
    "    our_matrix[idx].sort(reverse=True)\n",
    "\n",
    "    max_value11 = our_matrix[idx][0][0]\n",
    "\n",
    "\n",
    "\n",
    "    rows = []\n",
    "\n",
    "\n",
    "    list111 = []\n",
    "    row_filtered=[]\n",
    "    mean_values = []\n",
    "    mean_values_total = []\n",
    "    row_values = []\n",
    "    row_values_4 = []\n",
    "\n",
    "    row_embeddings = []\n",
    "   \n",
    "    for j in range(magic_number):\n",
    "        list111.append(our_classes['label'][list_matrix[idx][j][1]])\n",
    "        row_filtered.append(list_matrix[idx][j])\n",
    "      \n",
    "    \n",
    "    \n",
    "        \n",
    "       \n",
    "        index_label = list_matrix[idx][j][1]\n",
    "\n",
    "        our = 0\n",
    "        our_mean_embeddings = []\n",
    "        \n",
    "        embeddings_value = np.array([embeddings_index[val] if val not in weak_values_all[35][:-(len(weak_values_all[35])//2)] else np.zeros(300) for val in our_words['new_col2'][idx]],  dtype=np.float32, order='C')\n",
    "        our_values = np.dot(embeddings_index[original_domain[index_label]], embeddings_value.T)\n",
    "        score_b = 1\n",
    "        score_a = 0\n",
    "        embedding_val = None\n",
    "\n",
    "            \n",
    "\n",
    "        if len(list_elements[index_label])==1:\n",
    "           embedding_val = embeddings_index[original_domain[index_label]]\n",
    "           our_values = list(map(lambda x: x, our_values))\n",
    "           if len(original_domain[index_label].split(\"__\"))>1:\n",
    "                our_values_temp = 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                if len(our_values_5_temp) > len(our_values_5):\n",
    "                    our_values = our_values_temp\n",
    "\n",
    "\n",
    "        \n",
    "        elif len(list_elements[index_label])==2:\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.3\n",
    "                score_b = 0.7\n",
    "\n",
    "                our_values = our_values * 0.3 + 0.7 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)[0]\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.2 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.425 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T) + 0.375 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "\n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.3\n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_b = 0.1\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "\n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                \n",
    "            \n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.5\n",
    "                score_b = 0.5\n",
    "               \n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.3 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.4 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T) + 0.3 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "                if  len(original_domain[index_label].split(\"__\"))>1:\n",
    "                    our_values_temp = 0.3 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.4 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + 0.3 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "               \n",
    "                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_b = 0.1\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "                score_a = 1 - score_b\n",
    "                \n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_domain[index_label].split(\"__\"))>1:\n",
    "                    if len(core[index_label].split(\"::\"))>1:\n",
    "                        score_c1 = 0.1\n",
    "                        score_a1 = 0.4\n",
    "                        score_b1 = 0.5\n",
    "                    elif len(core[index_label].split(\"__\"))>1:\n",
    "                        score_c1 = 0.2\n",
    "                        score_a1 = 0.35\n",
    "                        score_b1 = 0.45\n",
    "\n",
    "                    our_values_temp = score_a1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + score_b1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + score_c1 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "                \n",
    "            embedding_val = (score_a/(score_a+score_b)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b)) * embeddings_index[original_core[index_label]]\n",
    "      \n",
    "        elif len(list_elements[index_label])==3:\n",
    "            score_a = 0.3\n",
    "            score_b = 0.4\n",
    "            score_c = 0.3\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.2\n",
    "                score_b = 0.425\n",
    "                score_c = 0.375\n",
    "\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.325\n",
    "                    score_c = 0.45\n",
    "                    score_a = 0.225\n",
    "                    our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                else:\n",
    "                    our_values = our_values * 0.2 + 0.425 * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + 0.375 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "\n",
    "                \n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.25\n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_c = 0.1\n",
    "                    score_a = 0.20\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_c = 0.25\n",
    "\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.35\n",
    "                    score_c+= 0.075\n",
    "                    score_a+= 0.075\n",
    "                else:\n",
    "                    score_b = 1 - score_a - score_c\n",
    "\n",
    "                our_values = our_values * (score_a/(score_a+score_b+score_c)) + (score_b/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + (score_c/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "                    score_a = 0.35\n",
    "                    score_c = 0.35\n",
    "                \n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.35\n",
    "                score_b = 0\n",
    "                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_c = 0.1\n",
    "\n",
    "                    score_a = 0.4\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_c = 0.2\n",
    "                    score_a = 0.35\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.35\n",
    "                    score_a = 1 - score_a-score_b\n",
    "                else:\n",
    "                    score_b = 1 - score_a - score_c\n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "\n",
    "            embedding_val = (score_a/(score_a+score_b+score_c)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b+score_c)) * embeddings_index[original_modifier[index_label]]+ (score_c/(score_a+score_b+score_c)) * embeddings_index[original_core[index_label]]\n",
    "        \n",
    "        \n",
    "        elements = our_values\n",
    "        length_no = list(filter(lambda x: x >= 0.475 , elements)) \n",
    "\n",
    "        \n",
    "        length_no2 = list(filter(lambda x: x >= 0.3 , elements))\n",
    "        length_no4 = list(filter(lambda x: x >= 0.4 , elements))\n",
    "      \n",
    "\n",
    "\n",
    "        if length_no2 != list():\n",
    "            mean_values.append(np.mean(length_no2))\n",
    "        else:\n",
    "            mean_values.append(np.mean(np.zeros(300)))\n",
    "        if length_no != list():\n",
    "            mean_values_total.append(np.mean(length_no))\n",
    "        else:\n",
    "            mean_values_total.append(np.mean(np.zeros(300)))\n",
    "\n",
    "\n",
    "        our+=len(length_no)\n",
    "        rows.append(our)\n",
    "        row_values_4.append(length_no4)\n",
    "        row_values.append(length_no)\n",
    "\n",
    "        row_embeddings.append(embedding_val)\n",
    "    max_values = 0\n",
    "    mean_max_values = -1\n",
    "    index_x = -1\n",
    "    max_index=-1\n",
    "\n",
    "    \n",
    "    max_mean_5 = -1\n",
    "    indices_value = []\n",
    "    final_indices_val = -1\n",
    "    values_to_reconsider = []\n",
    "\n",
    "    for idx4, ivv in enumerate(range(len(row_values))):\n",
    "\n",
    "        if max_values < ivv and mean_values_total[idx4] >= 0.085:\n",
    "            \n",
    "            max_values = ivv\n",
    "            index_x = idx4\n",
    "           \n",
    "            mean_val_mid = sum(row_values[idx4]) / len(row_values[idx4])\n",
    "            \n",
    "            if max_mean_5 < mean_val_mid:\n",
    "                max_mean_5 = mean_val_mid\n",
    "            indices_value = [idx4]\n",
    "        elif max_values == ivv and mean_values_total[idx4] >= 0.085 :\n",
    "            indices_value.append(idx4)\n",
    "\n",
    "\n",
    "    for idx4, ivv in enumerate(row_values):\n",
    "        if index_x != idx4 and  mean_values_total[idx4] >= 0.085 and np.max(row_values[idx4]) > 0.9 and len(row_values_4[idx4]) >= len(row_values_4[index_x]):\n",
    "            indices_value.append(idx4)\n",
    "            values_to_reconsider.append(idx4)\n",
    "\n",
    "    if max_mean_5 < 0.65 and max_values != 0:\n",
    "        indices_elements_copy = []\n",
    "        for idx4 in indices_value:\n",
    "            \n",
    "            if context_matrix[idx][list_matrix[idx][idx4][1]]>0.45 or (context_matrix_niche[idx][list_matrix[idx][idx4][1]]>0.45 and similarity_to_niche[idx][idx] > 0.3) or (context_matrix_category[idx][list_matrix[idx][idx4][1]]>0.45 and similarity_to_cat[idx][idx]) or first_sentence_matrix[idx][list_matrix[idx][idx4][1]]>0.45:\n",
    "                indices_elements_copy.append(idx4)\n",
    "        if indices_elements_copy != []:\n",
    "            indices_value = indices_elements_copy.copy()\n",
    "\n",
    "    if max_mean_5 < 0.65 or len(indices_value)>1:\n",
    "        indices_value_copy = []\n",
    "\n",
    "        for idx4 in indices_value:\n",
    "            ivv = len(row_values_4[idx4])\n",
    "\n",
    "            if max_values < ivv and mean_values_total[idx4] >= 0.085:\n",
    "                max_values = ivv\n",
    "                index_x = idx4\n",
    "                indices_value_copy=[idx4]\n",
    "            elif max_values == ivv and mean_values_total[idx4] >= 0.085:\n",
    "                indices_value_copy.append(idx4)\n",
    "            elif idx4 in values_to_reconsider:\n",
    "                indices_value_copy.append(idx4)\n",
    "        if indices_value_copy != []:\n",
    "            indices_value = indices_value_copy.copy()\n",
    "\n",
    "    if max_values!=0 and len(indices_value) > 1:\n",
    "        indices_value_copy = []\n",
    "        max_values = len(list(row_values[index_x]))\n",
    "        mean_max_values = 0\n",
    "\n",
    "        for idx3 in indices_value:\n",
    "\n",
    "            row_values_final = list(row_values[idx3])\n",
    "            ivv = len(list(row_values[idx3]))\n",
    "            mean_val = 0\n",
    "            if len(row_values_final) != 0:\n",
    "                mean_val = np.mean(row_values_final)            \n",
    "            \n",
    "            if (max_values == ivv and mean_val > mean_max_values):\n",
    "                mean_max_values = mean_val\n",
    "                index_x = idx3\n",
    "                indices_value_copy= [idx3]\n",
    "        \n",
    "        for idx3, ivv in enumerate(rows):\n",
    "            if idx3 in values_to_reconsider:\n",
    "                indices_value_copy.append(idx3)\n",
    "        \n",
    "        if indices_value_copy != []:\n",
    "            indices_value = indices_value_copy.copy()\n",
    " \n",
    "    if max_values == 0 or len(indices_value) > 1:\n",
    "        \n",
    "    \n",
    "        mean_max_values = -1\n",
    "        index_x=-1\n",
    "        if max_values==0:\n",
    "            indices_value = list(range(len(row_values_4)))  \n",
    "        max_values = len(row_values_4[0])\n",
    "        for idx3, _ in enumerate(indices_value):\n",
    "            mean_val = 0\n",
    "            \n",
    "            \n",
    "            if len(row_values_4[idx3]) != 0:\n",
    "                mean_val = np.mean(row_values_4[idx3])\n",
    "            \n",
    "            if (max_values == len(row_values_4[idx3]) and mean_val > mean_max_values):\n",
    "                mean_max_values = mean_val\n",
    "                max_values = len(row_values_4[idx3])\n",
    "                index_x = idx3\n",
    "\n",
    "    return index_x, row_values[index_x], row_values_4[index_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Label Recovery and Semantic Filtering Strategy\n",
    "\n",
    "This module implements a robust fallback mechanism to recover meaningful labels when initial predictions are weak (score ≤ 0.30) or overly filtered. It operates through multiple backup matrices and semantic checks to re-rank and reinforce candidate labels using contextual embeddings and structural logic.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "To maintain stable and contextually accurate label predictions, especially when the top scoring candidate is filtered out or deemed unreliable.\n",
    "\n",
    "\n",
    "### Fallback Recovery Pipeline\n",
    "\n",
    "When `list_matrix[i][0][0] ≤ 0.30` or when the candidate list is too large or noisy, the following recovery layers are applied in order:\n",
    "\n",
    "1. **old_list_matrix**  \n",
    "   The less filtered version than the current one. If it includes a valid high-scoring candidate, it is used. Otherwise, fallback continues.\n",
    "\n",
    "2. **list_matrix_original_2**  \n",
    "   A moderately-filtered version, used if the previous matrix fails to yield a suitable result.\n",
    "\n",
    "3. **list_matrix_original**  \n",
    "   A stable baseline. If no valid candidates are found yet, this version is used.\n",
    "\n",
    "4. **Last Resort**  \n",
    "   If all above fail, the first valid label (based on contextual and category thresholds) is selected as a fallback.\n",
    "\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "**`filter_by_text()`**\n",
    "Originally designed to filter candidates across the entire dataset, this function is now applied at row level to select **a single most semantically relevant label** for a specific instance. It uses domain-core-modifier decomposition and token similarity scores to prioritize meaningful matches.\n",
    "\n",
    "**`find_best_indices_cat()`**\n",
    "Previously used globally for category-aware filtering, this function now operates at row level. It identifies the best label aligned with the company’s category and niche by scoring semantic similarity against known business-relevant terms.\n",
    "\n",
    "\n",
    "### Semantic Validation and Scoring\n",
    "\n",
    "Each fallback step involves:\n",
    "\n",
    "- Comparing embeddings of label terms with input tokens.\n",
    "- Verifying modifier, domain, and core relevance through cosine similarity.\n",
    "- Penalizing generic or ambiguous terms using a weighted scheme.\n",
    "- Enforcing contextual similarity thresholds to prevent mislabeling.\n",
    "\n",
    "\n",
    "### Outcome\n",
    "\n",
    "This multi-stage approach ensures:\n",
    "\n",
    "- Recovery of relevant labels even under heavy filtering.\n",
    "- Suppression of generic or poorly-aligned terms.\n",
    "- More stable and meaningful predictions, consistent with the company’s context and domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_indices_cat(our_matrix, idx, magic_number):\n",
    "    our_matrix[idx].sort(reverse=True)\n",
    "\n",
    "    max_value11 = list_matrix[idx][0][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    if(max_value11 - list_matrix[idx][1][0]>=0.1):\n",
    "        return 0\n",
    "\n",
    "    rows = []\n",
    "    rows_emb = []\n",
    "\n",
    "    list111 = []\n",
    "    row_filtered=[]\n",
    "    mean_values_total = []\n",
    "\n",
    "    row_embeddings = []\n",
    "    rows_emb4_5 = []\n",
    "\n",
    "    set_values = set(strong_values_all[idx])\n",
    "    set_values.update(our_categories_niche_words['niche_plus_cat'][idx])\n",
    "    list_val = set_values\n",
    "   \n",
    "    \n",
    "    if list_val == []:\n",
    "        return -1\n",
    "    embeddings_value = np.array([embeddings_index.get(val, np.zeros(300)) for val in list_val],  dtype=np.float32, order='C')\n",
    "    if list_val == set():\n",
    "        return -1\n",
    "    \n",
    "    faiss.normalize_L2(embeddings_value)\n",
    "   \n",
    "    for j in range(magic_number):\n",
    "        list111.append(our_classes['label'][list_matrix[idx][j][1]])\n",
    "        row_filtered.append(list_matrix[idx][j])\n",
    "        index_label = list_matrix[idx][j][1]\n",
    "\n",
    "        our = 0\n",
    "        our_values = np.dot(embeddings_index[original_domain[index_label]].reshape(1,-1), embeddings_value.T)[0]\n",
    "\n",
    "        score_b = 1\n",
    "        score_a = 0\n",
    "        embedding_val = None\n",
    "        penalizer_genericity = 1.0\n",
    "        \n",
    "        if len(list_elements[index_label])==1:\n",
    "           embedding_val = embeddings_index[original_domain[index_label]]\n",
    "           \n",
    "           if len(original_domain[index_label].split(\"_\"))>1 and original_domain[index_label] == domain[index_label]:\n",
    " \n",
    "                our_values_temp = 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.5 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T)\n",
    "                our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "\n",
    "                our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "               \n",
    "                if (len(our_values_45_temp) > len(our_values_45)) or (len(our_values_5_temp) > len(our_values_5)):\n",
    "                    our_values = our_values_temp\n",
    "\n",
    "        elif len(list_elements[index_label])==2:\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.3\n",
    "                score_b = 0.7\n",
    "        \n",
    "                our_values = our_values * 0.3 + 0.7 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.2 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.425 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T) + 0.375 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T) \n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "               \n",
    "                    if (len(our_values_5_temp) > len(our_values_5) or len(our_values_45_temp) > len(our_values_45)):\n",
    "                        our_values = our_values_temp\n",
    "\n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                score_a = 0.3\n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_b = 0.1\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.3\n",
    "                penalizer_genericity = 0.7\n",
    "                \n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.5\n",
    "                score_b = 0.5\n",
    "             \n",
    "                our_values = our_values * (score_a/(score_a+score_b)) + (score_b/(score_a+score_b)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                if len(original_core[index_label].split(\"_\")) > 1:\n",
    "                    our_values_temp =  0.3 * np.dot(embeddings_index[original_domain[index_label]],  embeddings_value.T)+ 0.4 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[0]], embeddings_value.T)[0] + 0.3 * np.dot(embeddings_index[original_core[index_label].split(\"_\")[1]], embeddings_value.T)[0] \n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_45_temp) > len(our_values_45) or len(our_values_5_temp) > len(our_values_5):\n",
    "                        our_values = our_values_temp\n",
    "                if len(original_domain[index_label].split(\"_\"))>1:\n",
    "                    our_values_temp = 0.3 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + 0.4 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + 0.3 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "\n",
    "                    if len(our_values_5_temp) > len(our_values_5) or len(our_values_45_temp) > len(our_values_45):\n",
    "                        our_values = our_values_temp\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "               \n",
    "                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_a = 0.9\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_a = 0.7\n",
    "                score_b= 1 - score_a\n",
    "               \n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_core[index_label]].reshape(1,-1), embeddings_value.T)[0]\n",
    "                if len(original_domain[index_label].split(\"_\"))>1:\n",
    "                    if len(core[index_label].split(\"::\"))>1:\n",
    "                        score_c1 = 0.1\n",
    "                        score_a1 = 0.4\n",
    "                        score_b1 = 0.5\n",
    "                    elif len(core[index_label].split(\"__\"))>1:\n",
    "                        score_c1 = 0.2\n",
    "                        score_a1 = 0.35\n",
    "                        score_b1 = 0.45\n",
    "\n",
    "                    our_values_temp = score_a1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[0]], embeddings_value.T) + score_b1 * np.dot(embeddings_index[original_domain[index_label].split(\"_\")[1]], embeddings_value.T) + score_c1 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "                    our_values_45 = list(filter(lambda x: x>=0.45, our_values))\n",
    "                    our_values_45_temp = list(filter(lambda x: x>=0.45, our_values_temp))\n",
    "                    our_values_5 = list(filter(lambda x: x>=0.5, our_values))\n",
    "                    our_values_5_temp = list(filter(lambda x: x>=0.5, our_values_temp))\n",
    "                    \n",
    "                    if len(our_values_5_temp) > len(our_values_5) or len(our_values_45_temp) > len(our_values_45):\n",
    "                        our_values = our_values_temp\n",
    "                \n",
    "            embedding_val = (score_a/(score_a+score_b)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b)) * embeddings_index[original_core[index_label]]\n",
    "\n",
    "        elif len(list_elements[index_label])==3:\n",
    "            score_a = 0.3\n",
    "            score_b = 0.4\n",
    "            score_c = 0.3\n",
    "            if  original_domain[index_label] != domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                score_a = 0.2\n",
    "                score_b = 0.425\n",
    "                score_c = 0.375                \n",
    "                our_values = our_values * 0.2 + 0.425 * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T)[0] + 0.375 * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)[0]\n",
    "  \n",
    "            elif original_domain[index_label] != domain[index_label] and original_core[index_label] != core[index_label]:                \n",
    "                if len(core[index_label].split(\"::\"))>1:\n",
    "                    score_c = 0.1\n",
    "                    score_a = 0.235\n",
    "                elif len(core[index_label].split(\"__\"))>1:\n",
    "                    score_c = 0.2\n",
    "                    score_a = 0.2\n",
    "                score_b = 1 - score_a - score_c\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] == core[index_label]:\n",
    "                our_values = our_values * score_a + score_b * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T)[0] + + score_c * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)[0]\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label] and len(core[index_label].split(\"::\"))>1:\n",
    "                score_a = 0.425\n",
    "                score_c = 0.1\n",
    "                score_b = 1 - score_a - score_c\n",
    "            elif original_domain[index_label] == domain[index_label] and original_core[index_label] != core[index_label]:\n",
    "                if len(modifier[index_label].split(\"__\"))>1:\n",
    "                    score_b = 0.5\n",
    "                    score_a = 0.3 \n",
    "                    score_c = 1 - score_a-score_b\n",
    "                else:\n",
    "                    score_b = 0.45\n",
    "                    score_a = 0.35 \n",
    "                    score_c = 1 - score_a - score_b\n",
    "            \n",
    "            our_values = our_values * (score_a/(score_a+score_b+score_c)) + (score_b/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_modifier[index_label]], embeddings_value.T) + (score_c/(score_a+score_b+score_c)) * np.dot(embeddings_index[original_core[index_label]], embeddings_value.T)\n",
    "            embedding_val = (score_a/(score_a+score_b+score_c)) * embeddings_index[original_domain[index_label]] +  (score_b/(score_a+score_b+score_c)) * embeddings_index[original_modifier[index_label]]+ (score_c/(score_a+score_b+score_c)) * embeddings_index[original_core[index_label]]\n",
    "\n",
    "       \n",
    "     \n",
    "        length_no = list(filter(lambda zz: zz>=0.45, our_values))\n",
    "        \n",
    "\n",
    "        length_no = list(filter(lambda x: (x * penalizer_genericity) >= 0.5, our_values)) \n",
    "\n",
    "        length_no4_5 = list(filter(lambda x: (x * penalizer_genericity) >= 0.45, our_values)) \n",
    "\n",
    "\n",
    "        if len(length_no) > 0: \n",
    "            mean_values_total.append(np.mean(list(map(lambda x: x, length_no))))\n",
    "        else:\n",
    "            mean_values_total.append(0)\n",
    "\n",
    "        our+=len(length_no)\n",
    "        rows.append(our)\n",
    "        rows_emb.append(length_no)\n",
    "        rows_emb4_5.append(length_no4_5)\n",
    "        row_embeddings.append(embedding_val)\n",
    "\n",
    "    max_values = 0\n",
    "    index_x = -1\n",
    "\n",
    "    for idx4, ivv in enumerate(rows):\n",
    "        if max_values < ivv and mean_values_total[idx4] >= 0.085:\n",
    "            max_values = ivv\n",
    "            index_x = idx4\n",
    "    list_best_cat.append(index_x)\n",
    "\n",
    "    if max_values==0:\n",
    "        return -1\n",
    "\n",
    "\n",
    "    max_len_4_5_values = -1\n",
    "    list_indices = []\n",
    "    for idx3, ivv in enumerate(rows):\n",
    "        if len(rows_emb[idx3])!=0:\n",
    "            mean_val = np.mean(rows_emb[idx3])\n",
    "        if (max_values == ivv and len(rows_emb4_5[idx3]) > max_len_4_5_values):\n",
    "            max_len_4_5_values = len(rows_emb4_5[idx3])\n",
    "            index_x = idx3\n",
    "            list_indices.append(idx3)\n",
    "        elif (max_values == ivv and len(rows_emb4_5[idx3]) == max_len_4_5_values):\n",
    "            list_indices.append(idx3)\n",
    "    \n",
    "    if len(list_indices) > 1:\n",
    "        mean_val = -1\n",
    "        for idx3, ivv in enumerate(list_indices):\n",
    "            \n",
    "            if len(rows_emb4_5[idx3]) > 0:\n",
    "                mean_ = np.mean(rows_emb4_5[idx3])\n",
    "\n",
    "            \n",
    "            else:\n",
    "                mean_ = 0\n",
    "            if mean_ > mean_val:\n",
    "                index_x = idx3\n",
    "                mean_val = mean_\n",
    "    \n",
    "    \n",
    "\n",
    "    return index_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "i=0\n",
    "\n",
    "zz=0\n",
    "ridiculous_number = 0\n",
    "for idx, _ in df.iterrows():\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "    row_filtered = []\n",
    "\n",
    "\n",
    "    for j in range(30):\n",
    "        \n",
    "        if(list_matrix[i][j][1]==last_index_elements2[i] and j!=0):\n",
    "            break        \n",
    "        \n",
    "        \n",
    "        if max_value - list_matrix[i][j][0] <= 0.10:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "        if list_matrix[i][j+1][0] < 0.30:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    final_list.append(list111)\n",
    "    \n",
    "    ind_cat = -1\n",
    "    list1 = [11]\n",
    "    list2 = [11]\n",
    "    list_matrix_original[i].sort(reverse=True)\n",
    "    list_matrix_original_2[i].sort(reverse=True)\n",
    "    old_list_matrix[idx].sort(reverse=True)\n",
    "\n",
    "    if(list_matrix[i][0][0]<=0.30 and old_list_matrix[i][0][0]>0.30) or len(list111)>5:\n",
    "        if i not in skip_categories_for_this_one:\n",
    "            ind_cat = find_best_indices_cat(old_list_matrix, i, 5)\n",
    "            list_matrix_temp = old_list_matrix[i].copy()\n",
    "        else:\n",
    "                \n",
    "            for j in range(220):\n",
    "                if context_matrix_niche[i][old_list_matrix[i][j][1]]>=0.65 or context_matrix_niche[i][old_list_matrix[i][j][1]]>=0.46:\n",
    "                    old_list_matrix[i][j] = (-1, old_list_matrix[j][1][1])\n",
    "            old_list_matrix[idx].sort(reverse=True)\n",
    "\n",
    "            \n",
    "\n",
    "        if(ind_cat==-1):\n",
    "            ind_cat,list1, list2 = filter_by_text(old_list_matrix, i, 5)\n",
    "        temp_list_element = old_list_matrix[i].copy()\n",
    "        if list1!=[] or list2 != []:\n",
    "            for j in range(220):\n",
    "                if j!=ind_cat:\n",
    "                    temp_list_element[j] = (-1, temp_list_element[j][1])\n",
    "            list_matrix[i] = temp_list_element.copy()        \n",
    "   \n",
    "    if (list1==[] and list2==[]) or (list_matrix[idx][0][0]<=0.30 and list_matrix_original_2[idx][0][0]>0.30):\n",
    "        ind_cat = -1\n",
    "        if i not in skip_categories_for_this_one:\n",
    "            ind_cat = find_best_indices_cat(list_matrix_original_2, i, 5)\n",
    "            list_matrix_temp = list_matrix_original_2[idx].copy()\n",
    "        else:\n",
    "\n",
    "            for j in range(220):\n",
    "                if context_matrix_niche[i][list_matrix_original_2[idx][j][1]]>=0.65 or context_matrix_niche[i][list_matrix_original_2[idx][j][1]]>=0.46:\n",
    "                    list_matrix_original_2[i][j] = (-1, list_matrix_original_2[i][j][1])\n",
    "            list_matrix_original_2[idx].sort(reverse=True)\n",
    "            \n",
    "        if(ind_cat==-1):\n",
    "            ind_cat, list1, list2 = filter_by_text(list_matrix_original_2, i, 5)\n",
    "        temp_list_element = list_matrix_original_2[i].copy()\n",
    "        if list1!=[] or list2 != []:\n",
    "            for j in range(220):\n",
    "                if j!=ind_cat:\n",
    "                    temp_list_element[j] = (-1, temp_list_element[j][1])\n",
    "            list_matrix[i] = temp_list_element.copy()\n",
    "        \n",
    "  \n",
    "    if (list1==[] and list2==[]) or (list_matrix[i][0][0]<=0.30 and list_matrix_original[i][0][0]>0.30):\n",
    "        ind_cat = -1\n",
    "        if i not in skip_categories_for_this_one:\n",
    "            ind_cat = find_best_indices_cat(list_matrix_original, i, 5)\n",
    "            list_matrix_temp = list_matrix_original[i].copy()\n",
    "            \n",
    "        else:\n",
    "            listxzz=[]\n",
    "            for j in range(220):\n",
    "                \n",
    "                \n",
    "                if context_matrix_niche[i][list_matrix_original[i][j][1]]>=0.65 or context_matrix_niche[i][list_matrix_original[i][j][1]]>=0.46:\n",
    "                    list_matrix_original[i][j] = (-1, list_matrix_original[i][j][1])\n",
    "                \n",
    "                list_matrix_original[idx].sort(reverse=True)\n",
    "\n",
    "            for element in our_categories_niche_words['niche_plus_cat'][i]:\n",
    "                if element in our_words['new_col2'][i]:\n",
    "                    our_words['new_col2'][i].remove(element)\n",
    "        \n",
    "        if(ind_cat==-1):\n",
    "            ind_cat, list1, list2 = filter_by_text(list_matrix_original, i, 5)\n",
    "        temp_list_element = list_matrix_original[i].copy()\n",
    "        if list1!=[] or list2 != []:\n",
    "            for j in range(220):\n",
    "                if j!=ind_cat:\n",
    "                    temp_list_element[j] = (-1, temp_list_element[j][1])\n",
    "            list_matrix[i] = temp_list_element.copy()\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        if list1 == [] and list2 == []:\n",
    "            max_value = -1\n",
    "            index_v = -1\n",
    "            \n",
    "            for j in range(10):\n",
    "                if max_value < context_matrix[i][list_matrix_original[i][j][1]] and list_matrix_original[i][j][0] >=0.3:\n",
    "                    max_value = context_matrix[i][list_matrix_original[i][j][1]]\n",
    "                    index_v = j\n",
    "            \n",
    "\n",
    "            for j in range(220):\n",
    "                if j!=index_v:\n",
    "                    temp_list_element[j] = (-1, temp_list_element[j][1])\n",
    "           \n",
    "            \n",
    "            list_matrix[i] = temp_list_element.copy()\n",
    "                        \n",
    "                \n",
    "    elements_filtered.append(row_filtered)\n",
    "\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "elements_filtered = []\n",
    "\n",
    "i=0\n",
    "\n",
    "zz=0\n",
    "ridiculous_number = 0\n",
    "\n",
    "for idx, _ in df.iterrows():\n",
    "    list_matrix[i].sort(reverse=True)\n",
    "    \n",
    "    list111 = []\n",
    "    list111_old = []\n",
    "\n",
    "    if list_matrix[i] == []:\n",
    "        final_list.append([])\n",
    "        i+=1\n",
    "        continue\n",
    "\n",
    "    max_value = list_matrix[i][0][0]\n",
    "\n",
    "    \n",
    "    row_filtered = []\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(min(30, len(list_matrix[i]))):\n",
    "        \n",
    "        if(list_matrix[i][j][1]==last_index_elements2[i] and j!=0):\n",
    "            break\n",
    "        \n",
    "        if max_value - list_matrix[i][j][0] <= 0.1:\n",
    "            list111.append(our_classes['label'][list_matrix[i][j][1]])\n",
    "            row_filtered.append(list_matrix[i][j])\n",
    "        if list_matrix[i][j+1][0] < 0.30:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    final_list.append(list111)\n",
    "\n",
    "    elements_filtered.append(row_filtered)\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['insurance_label'] = final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/challenge_ml_insurance_completed_full_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Final Predictions\n",
    "\n",
    "After generating the final results, we insert them back into the **valid rows** of our dataset by adding a new column.\n",
    "\n",
    "The completed results, including the labels predicted, are saved to a file named **`challenge_ml_insurance_completed.csv`** for further use or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"context_matrix.txt\"):\n",
    "    os.remove(\"context_matrix.txt\")\n",
    "if os.path.exists(\"context_sentence_embedding.txt\"):\n",
    "    os.remove(\"context_sentence_embedding.txt\")\n",
    "if os.path.exists(\"label_embeddings.txt\"):\n",
    "    os.remove(\"label_embeddings.txt\")\n",
    "if os.path.exists(\"vec_embeddings.txt\"):\n",
    "    os.remove(\"vec_embeddings.txt\")\n",
    "if os.path.exists(\"tokens_vector.csv\"):\n",
    "    os.remove(\"tokens_vector.csv\")\n",
    "if os.path.exists(\"strongest_words2.csv\"):\n",
    "    os.remove(\"strongest_words2.csv\")\n",
    "if os.path.exists(\"sectors.csv\"):\n",
    "    os.remove(\"sectors.csv\")\n",
    "if os.path.exists(\"sectors.csv\"):\n",
    "    os.remove(\"sectors.csv\")\n",
    "if os.path.exists(\"our_current_categories.csv\"):\n",
    "    os.remove(\"our_current_categories.csv\")\n",
    "if os.path.exists(\"counted_elements.csv\"):\n",
    "    os.remove(\"counted_elements.csv\")\n",
    "if os.path.exists(\"average_tf_idf_per_word.txt\"):\n",
    "    os.remove(\"average_tf_idf_per_word.txt\")\n",
    "if os.path.exists(\"most_important_words.txt\"):\n",
    "    os.remove(\"most_important_words.txt\")\n",
    "if os.path.exists(\"match_words_with_tf_idf.txt\"):\n",
    "    os.remove(\"match_words_with_tf_idf.txt\")\n",
    "if os.path.exists(\"correlated_terms.txt\"):\n",
    "    os.remove(\"correlated_terms.txt\")\n",
    "if os.path.exists(\"new_label_with_categories.csv\"):\n",
    "    os.remove(\"new_label_with_categories.csv\")\n",
    "if os.path.exists(\"first_sentence_matrix_total1.txt\"):\n",
    "    os.remove(\"first_sentence_matrix_total1.txt\")\n",
    "if os.path.exists(\"cat_niche_embeddings.txt\"):\n",
    "    os.remove(\"cat_niche_embeddings.txt\")\n",
    "\n",
    "if os.path.exists(\"counted_categories.csv\"):\n",
    "    os.remove(\"counted_categories.csv\")\n",
    "if os.path.exists(\"niche_matrix.txt\"):\n",
    "    os.remove(\"niche_matrix.txt\")\n",
    "if os.path.exists(\"category_matrix.txt\"):\n",
    "    os.remove(\"category_matrix.txt\")\n",
    "if os.path.exists(\"similarity_desc_to_category.txt\"):\n",
    "    os.remove(\"similarity_desc_to_category.txt\")\n",
    "if os.path.exists(\"similarity_desc_to_niche.txt\"):\n",
    "    os.remove(\"similarity_desc_to_niche.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
