{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as pp\n","from sklearn.utils import shuffle\n","import multiprocessing\n","from functools import partial\n","import string\n","from joblib import Parallel, delayed\n","import stanza\n","import copy\n","from spacy.lang.en import stop_words as stop_words\n","import nltk\n","from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import os\n","from collections import Counter\n","from textblob import TextBlob\n"]},{"cell_type":"markdown","metadata":{},"source":["### Context Similarity & Embedding Extraction (Unsupervised Approach)\n","\n","In this step, we apply an **unsupervised approach** to evaluate and assign the most relevant labels to each company, without relying on external texts in order to help with the classification.\n","\n","We extract the **context similarity matrix** using `SentenceTransformer` for each company columns(except sector). Additionally, we compute the **vector embeddings** of the most important tokens from each company — selected through various methods, which we will explore further.\n","\n","We also generate **label embeddings** for all taxonomy labels, filtering out less relevant terms (eg .services) and keeping only the most representative ones. For each label, we also extract and embed the **strongest word** which has the strongest corelation to any company.\n","\n","These embeddings allow us to compare companies with multiple candidate labels and use the strongest words as a **tie-breaking criterion**, especially when label scores are close or ambiguous.\n","\n","All of this logic is part of a broader workflow and is used within another program called **`classifier2_continuation`**, where we continue the classification process using these extracted signals."]},{"cell_type":"markdown","metadata":{},"source":["Here we are removing each file, to not not do it ourselves manually"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["if os.path.exists(\"context_matrix.txt\"):\n","    os.remove(\"context_matrix.txt\")\n","if os.path.exists(\"context_sentence_embedding.txt\"):\n","    os.remove(\"context_sentence_embedding.txt\")\n","if os.path.exists(\"label_embeddings.txt\"):\n","    os.remove(\"label_embeddings.txt\")\n","if os.path.exists(\"vec_embeddings.txt\"):\n","    os.remove(\"vec_embeddings.txt\")\n","if os.path.exists(\"tokens_vector.csv\"):\n","    os.remove(\"tokens_vector.csv\")\n","if os.path.exists(\"new_label_with_categories\"):\n","    os.remove(\"new_label_with_categories\")\n","if os.path.exists(\"most_specific_term.csv\"):\n","    os.remove(\"most_specific_term.csv\")\n","if os.path.exists(\"match_words_with_tf_idf.txt\"):\n","    os.remove(\"match_words_with_tf_idf.txt\")\n","if os.path.exists(\"counted_elements.csv\"):\n","    os.remove(\"counted_elements.csv\")\n","if os.path.exists(\"sectors.csv\"):\n","    os.remove(\"sectors.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["**Company Data**\n","\n","This is the dataset containing the **company information** that we will use throughout the project for classification."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"../inputs/ml_insurance_challenge.csv\")\n","num_cores = multiprocessing.cpu_count()"]},{"cell_type":"markdown","metadata":{},"source":["### Curated ConceptNet Embeddings with Numberbatch\n","\n","We use **Numberbatch** from **ConceptNet** to help generate word embeddings. The original Numberbatch file was approximately **20GB**, which made it extremely slow to load. Additionally, it was difficult to determine if a term came from a specific language.\n","\n","To improve both **performance** and **accuracy**, we created a **curated version** of Numberbatch. In this version, we extracted only the **foreign terms relevant to company titles**, allowing us to better match multilingual input.\n","\n","This helped us achieve **higher precision** when assigning labels to companies based on their descriptions."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from common_functions_f import generate_noun_for_adj, open_filtered_assertions_file, generate_embedings_index"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import numpy as np\n","embeddings_index =  generate_embedings_index()"]},{"cell_type":"markdown","metadata":{},"source":["**New Columns for Tokenization**\n","\n","These are the **new columns** we generated and prepared specifically for the **tokenization process**.  \n","They combine or transform existing fields (e.g., `description`, `business_tags`, `category` etc.) to provide a cleaner and more consistent input for the NLP tasks."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","df[\"description\"] = df[\"description\"].fillna(\"\")\n","df[\"sector\"] = df[\"sector\"].fillna(\"\")\n","df[\"niche\"] = df[\"niche\"].fillna(\"\")\n","df[\"category\"] = df[\"category\"].fillna(\"\")\n","df['new_col'] = df['sector']\n","df['new_col2'] = df['sector']\n","df2 = copy.deepcopy(df)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import re"]},{"cell_type":"markdown","metadata":{},"source":["### Handling CamelCase Words\n","\n","Many entries in the dataset—especially **company names**—contain **CamelCase formatting** (e.g., `SeniorService`).  \n","To improve clarity and ensure better **tokenization and semantic analysis**, we decided to **split CamelCase words into separate tokens** (e.g., `Senior Service`).\n","\n","This helps processes like embedding, similarity comparison, and label matching to perform more accurately."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def camel_case_split(s):\n","    if s.isupper():\n","        return s\n","    s=re.sub(r'[^\\w\\s]',' ',s)\n","\n","    \n","   \n","    modified_string = list(map(lambda x: '_' + x if x.isupper() else x, s))\n","    split_string = ''.join(modified_string).lower().split('_')\n","    split_string = \" \".join(list(filter(lambda x: x != '' and x!=\"-\" and x!=\"_\", split_string)))\n","    return split_string\n","def camel_case_split2(s):\n","    if s.isupper():\n","        return s\n","    if \"-\" in s or \"_\" in s:\n","        return s.lower()\n","    modified_string = list(map(lambda x: '_' + x if x.isupper() else x, s))\n","    split_string = ''.join(modified_string).lower().split('_')\n","    split_string = \" \".join(list(filter(lambda x: x != '' and x!=\"-\" and x!=\"_\", split_string)))\n","    return split_string"]},{"cell_type":"markdown","metadata":{},"source":["**Acronym Handling**\n","\n","Some **acronyms** in our dataset were found to be **semantically ambiguous** when compared using Numberbatch embeddings.  \n","To address this, we tested the **cosine similarity** between each acronym and its corresponding full form. When the similarity score was low — indicating a mismatch — we replaced the acronym with its **expanded version**, especially for terms that appeared frequently.  \n","This helped improve embedding accuracy and reduced the risk of misclassification during label assignment."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["acronym_dict = {\n","    \"IT\": \"information_technology\",\n","    \"ID\": \"identity\",\n","    \"PP\": \"polypropylene\",\n","    \"CBD\": \"cannabidiol\",\n","    \"covid\" : \"coronavirus\",\n","    \"COVID\": \"coronavirus\",\n","    \"Covid\": \"coronavirus\",\n","    \"Covid-19\": \"coronavirus\",\n","    \"COVID-19\": \"coronavirus\",\n","    \"covid-19\": \"coronavirus\"\n","\n","\n","\n","}\n","\n","def replace_upper_word(words, acronym_dict):\n","    return (list(map(lambda x: acronym_dict[x.replace(\"'\",\"\")] if x.replace(\"'\",\"\") in acronym_dict.keys() else x, words)))"]},{"cell_type":"markdown","metadata":{},"source":["**Text Normalization**\n","\n","As part of our preprocessing, we cleaned individual words by removing unwanted leading or trailing characters.  \n","For example, a word like `\"'paper\"` was normalized to `\"paper\"` to ensure consistency and improve token matching."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def replace_word(text):\n","\n","    if len(text) <= 1:\n","        return text\n","    if text[0] == '-' or text[0]==\"'\":\n","        text = text[1:]\n","    if text[-1] == \"-\" or text[-1]==\"'\":\n","        text = text[:-1]\n","\n","    if \"'s\" not in text and \"'\" in text:\n","        text = text.replace(\"'\",\"\")\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["**Acronym Replacement**\n","\n","To avoid confusion during tokenization and embedding, we replaced acronyms using a predefined dictionary of long forms.  \n","This ensures that terms are represented consistently and accurately, especially when the acronyms could be misunderstood or poorly represented in the embedding space."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["\n","def replace_upper_word_tags(words, acronym_dict):\n","    our_list = []\n","    for i in range(len(words)):\n","        our_list+=(list(map(lambda x: acronym_dict[replace_word(x)] if replace_word(x) in acronym_dict.keys() else replace_word(x).lower(), words[i].split(\" \"))))\n","        if i != len(words)-1:\n","            our_list+=\",\"\n","    return \" \".join(our_list)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def replace_sector_word(word):\n","    return word.lower().replace(\" \", \"_\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def check_punctuation(word):\n","    return all(char in string.punctuation or char == \" \" for char in word)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocessing Strategy\n","\n","To avoid issues during the NLP processing phase, we made a few important preprocessing decisions.\n","\n","- **Business tags** were left as they are, without splitting the terms, so as not to interfere with the NLP model.\n","- For the other columns, we split the terms and **merged the entire text content** from all relevant fields into one combined string per company. This approach is more efficient than running NLP on each column separately, which proved to be slower.\n","\n","We defined **five different processing cases**, one of which is the **company title**. The title is usually found before keywords like `\"is\"`, `\"in\"`, or `\"was\"` in the description. If no such keyword exists, we simply extract the **first five words** as the company name. The company title is given a **high weight**, as it often strongly indicates the company’s category — and we found this to be especially helpful.\n","\n","This is also why we included **foreign words** in the embedding vocabulary — for instance, words like `\"Pintura\"` (Spanish for \"painting\") often appear in company names and carry important meaning for classification.\n","\n","We treated the rest of the fields — **company description, business tags, category, and sector** — as separate components in our embedding and NLP pipeline.\n","\n","Additionally:\n","- We split **description** and **business tags** into lists to ensure no string exceeds the character limits imposed by our NLP model (e.g., < 1,000,000 characters).\n","- We chose **`;`** as a separator between different text segments. This worked significantly better than other options like `\".\"` or `\",\"`, especially when using string index functions and during **lemmatization**, where `\";\"` proved to be reliable for segmenting content clearly."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["relevant_pct = string.punctuation.replace(\",\", \"\").replace(\"_\", \"\").replace(\"-\", \"\").replace(\"'\", \"\")\n","df2=df.copy()\n"," \n","for idx, _ in df2.iterrows():\n","    df2.at[idx, 'business_tags'] = \"\".join(df2.at[idx, 'business_tags'].replace(\"/\", \" \")).translate(str.maketrans('', '', relevant_pct)).lower().split(\"'\")\n","    df2.at[idx, 'business_tags'] = list(filter(lambda x: x if x not in string.punctuation and not check_punctuation(x) else \"\", df2.at[idx, 'business_tags']))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df_old_copy = df.copy()\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import math\n","str_company_titles = \"\"\n","str_company_description_elements = \"\"\n","index_until_company_description_ends = []\n","\n","str_list_company_description_elements = []\n","\n","\n","list_sectors_elements = df['sector'].to_list()\n","set_sectors_elemenets = set(list_sectors_elements)\n","dict_sectors_elements = {i:list_sectors_elements.count(i) for i in set(list_sectors_elements)}\n","dict_sectors_elements2 = set(filter(lambda x: dict_sectors_elements[x]/len(df['description']) <= 0.2, dict_sectors_elements))\n","\n","str_company_business_tags_elements = \"\"\n","str_list_company_business_tags_elements = []\n","index_until_business_tags_elements = []\n","\n","str_company_category_elements = \"\"\n","index_until_category_elements = []\n","rest_comany_description_complete = \"\"\n","\n","str_company_niche_elements = \"\"\n","\n","title_company_complete = \"\"\n","index_until_niche_elements = []\n","all_punctuation_except_hyphen = string.punctuation.replace(\"-\",\"\").replace(\"'\",\"\").replace(\"’\",\"\").replace(\",\", \"\").replace(\".\",\"\").replace(\"/\", \"\")\n","colums = [('description',0), ('business_tags',1), ('category',2), ('niche',3)]\n","for col, idx in colums:\n","    df[col] = df[col].str.replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True)\n","\n","    if(col!='business_tags'):\n","        df[col] = df[col].str.split()\n","\n","df['title_description'] = df['description']\n","\n","title_company_complete_full = \"\"\n","z=0\n","\n","dict_description = {}\n","dict_tags = {}\n","for idx, rows in df.iterrows():\n","    z+=1\n","\n","    df_old_copy.loc[idx, 'sector'] = '' if df.loc[idx, 'sector'] not in dict_sectors_elements2 else replace_sector_word(df.loc[idx,'sector'])\n","\n","\n","\n","    business_tags = rows['business_tags'].split(\"' '\")\n","\n","    business_tags_text = replace_upper_word_tags(business_tags, acronym_dict)\n","    category = rows['category']\n","\n","    category_text = replace_upper_word(category, acronym_dict)\n","\n","    niche = rows['niche']\n","    niche_text = replace_upper_word(niche, acronym_dict)\n","\n","    words_splitted = rows['description']\n","    \n","    max_index = -1\n","    if \"is\" in  words_splitted:\n","        max_index = words_splitted.index(\"is\")\n","    if \"in\" in words_splitted and max_index == -1:\n","        max_index = words_splitted.index(\"in\")\n","    if \"was\" in words_splitted and max_index == -1:\n","        max_index = words_splitted.index(\"was\")\n","\n","    if max_index==-1:\n","        max_index = 5\n","\n","    if max_index > 10:\n","        max_index = 10   \n","    \n","\n","    words_splitted_List = list(map(lambda x: camel_case_split(x), words_splitted[:max_index]))\n","    words_splitted_rest_list = list(map(lambda x: camel_case_split2(x), words_splitted[max_index:]))\n","\n","    company_title =  \" \".join(words_splitted[:max_index])\n","\n","    title_company_complete = \" \".join(words_splitted_List)\n","\n","    title_company_complete_full += title_company_complete+'; '\n","    \n","\n","    str_company_titles+=company_title.replace(\"/\", \" \")+'. '\n","\n","    business_tags_text = business_tags_text.replace(\"-\", \"_\").replace(\" _ \", \"\").replace(\"/\", \" \") + '; '\n","    \n","\n","    if len(str_company_business_tags_elements)+len(business_tags_text.replace(\"/\", \" \")) > 1000000:\n","        str_list_company_business_tags_elements.append(str_company_business_tags_elements)\n","        str_company_business_tags_elements = business_tags_text.replace(\"/\", \" \")\n","    else:\n","        str_company_business_tags_elements += business_tags_text.replace(\"/\", \" \")\n","\n","\n","    niche_text = (\" \".join(niche_text)).replace(\"-\", \"_\").replace(\"'\",\"\").replace('\"',\"\").replace(\" _ \", \"\").lower() + '; '\n","    str_company_niche_elements += niche_text.replace(\"/\", \" \")\n","\n","    category_text = (\" \".join(category_text)).replace(\"-\", \"_\").replace(\"'\",\"\").replace('\"',\"\").replace(\" _ \", \"\").lower() + '; '\n","    str_company_category_elements += category_text.replace(\"/\", \" \")\n","\n","    \n","\n","    word_rest_list_revised = replace_upper_word(words_splitted_rest_list, acronym_dict)\n","\n","    rest_company_description = \" \".join(word_rest_list_revised).replace(\"-\", \"_\").replace(\"'\",\"\").replace('\"',\"\").replace(\" _ \", \"\").lower()+'; '\n","    \n","    \n","    if len(str_company_description_elements)+len(rest_company_description.replace(\"/\", \" \")) > 1000000:\n","        str_list_company_description_elements.append(str_company_description_elements)\n","        str_company_description_elements = rest_company_description.replace(\"/\", \" \")\n","    else:\n","        str_company_description_elements += rest_company_description.replace(\"/\", \" \")\n","\n","\n","str_list_company_description_elements.append(str_company_description_elements)\n","str_list_company_business_tags_elements.append(str_company_business_tags_elements)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Global Token Relevance Filtering with Entity Detection and Label Matching\n","\n","In this step, we expand the relevance check to include **all tokens**, not just those associated with a specific label.\n","\n","#### Entity Detection with English Model:\n","We use spaCy’s **English language model** (`en_core_web_lg`) to process the text and identify named entities and parts of speech across the entire input.\n","\n","#### Relevance Filtering:\n","To ensure that only meaningful tokens are kept:\n","- We compare **each token from the title** against the label embeddings using **cosine similarity**.\n","- If a token has a **similarity score higher than 0.35** to any label, we **consider it relevant** and keep it.\n","- Tokens that don't meet this threshold are discarded to reduce noise.\n","\n","This filtering helps us isolate only the **most relevant terms**, which are likely to carry strong semantic meaning for classification.\n","\n","#### Weighting Important Terms:\n","Tokens that pass the relevance threshold are considered **crucial features**, and we apply an increased weighting of **×4** to them during scoring and embedding aggregation. This gives them greater influence in the final label selection process."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import numpy as np\n","import spacy\n","nlp = spacy.load('en_core_web_lg') "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["nlp_niche = nlp(str_company_niche_elements)\n","nlp_category = nlp(str_company_category_elements)\n","\n","nlp_business_tags = []\n","nlp_description = []"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["nlp_description = [None] * len(str_list_company_description_elements)\n","nlp_business_tags = [None] * len(str_list_company_business_tags_elements)"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Global Variables for Parallelization**\n","\n","We use **global variables** in our implementation to support **parallelization**, allowing multiple processes to access shared data efficiently.  \n","This design choice helps us significantly **speed up computation**"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import threading\n","\n","def get_nlp(no, number):\n","    if no == 0:\n","        global nlp_business_tags\n","        nlp_business_tags[number] = nlp(str_list_company_business_tags_elements[number])\n","    else:\n","        global nlp_description\n","        nlp_description[number] = nlp(str_list_company_description_elements[number])\n","    return None\n","        "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["threads = []\n","for i in range(len(str_list_company_description_elements)):\n","    thread = threading.Thread(target = get_nlp, args=(1,i))\n","    threads.append(thread)\n","    thread.start()\n","\n","for i in range(len(str_list_company_business_tags_elements)):\n","    thread = threading.Thread(target = get_nlp, args=(0,i))\n","    threads.append(thread)\n","    \n","    thread.start()\n","\n","\n","for thread in threads:\n","    thread.join()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Index Tracking for Business Tags Tokenization\n","\n","We track the **indexes marking the end of each company's `business_tags`** in order to properly manage how we tokenize them later. Alongside that, we maintain the index of the `text_nlp_business_tags` list, which contains the preprocessed NLP tokens for all business tags.\n","\n","This indexing system helps us during the **tokenization step for each column**, especially when we need to segment the input correctly per company.\n","\n","Whenever we reach a new company index in our list, we:\n","- Move to the corresponding entry in the `text_nlp_business_tags` list\n","- **Reset the start and end values**\n","- Repeat the process until we have iterated through all tags\n","\n","This logic ensures that each company’s business tags are correctly aligned with their corresponding tokenized representation. This logic is applied also to the nlp_description, and it is simpler for category, niche and title."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["j=0\n","\n","\n","text_nlp_business_tags = [el.lemma_ for el in nlp_business_tags[j]]\n","\n","start=0\n","end_index = text_nlp_business_tags[start:].index(\";\")\n","text_list=[]\n","\n","\n","index_end_business_tags = []\n","index_group_business_tags = []\n","list_nlp_business_tags = []\n","z=0\n","s=-1\n","while j<len(nlp_business_tags):\n","    text_list=[]\n","    index_end_business_tags.append(end_index)\n","    index_group_business_tags.append(j)\n","    \n","    if end_index+1 >= len(nlp_business_tags[j]):\n","        j+=1\n","        start=0\n","        if j < len(nlp_business_tags):\n","            text_nlp_business_tags = [el.text for el in nlp_business_tags[j]]\n","            end_index=text_nlp_business_tags[start:].index(\";\")+1        \n","    else:\n","        start=end_index+1\n","        end_index+= text_nlp_business_tags[start:].index(\";\")+1\n","    z+=1\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["text_nlp_category = [el.text for el in nlp_category]\n","\n","start=0\n","end_index = text_nlp_category[start:].index(\";\")+1\n","text_list=[]\n","\n","\n","index_end_category = []\n","list_nlp_cateogry = []\n","z=1\n","s=-1\n","while z<len(df['category']):\n","    text_list=[]\n","    index_end_category.append(end_index)    \n","    start=end_index\n","    end_index+= text_nlp_category[start:].index(\";\")+1\n","    z+=1\n","index_end_category.append(end_index)    \n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["j=0\n","\n","\n","text_nlp_niche = [el.text for el in nlp_niche]\n","\n","start=0\n","end_index = text_nlp_niche[start:].index(\";\")+1\n","text_list=[]\n","\n","\n","index_end_niche = []\n","list_nlp_niche = []\n","z=1\n","s=-1\n","\n","while z<len(df['niche']):\n","    text_list=[]\n","    index_end_niche.append(end_index)    \n","    start=end_index\n","    end_index+= text_nlp_niche[start:].index(\";\")+1\n","    z+=1\n","index_end_niche.append(end_index)    \n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["j=0\n","\n","\n","text_nlp_description = [el.text for el in nlp_description[j]]\n","\n","start=0\n","end_index = text_nlp_description[start:].index(\";\")\n","text_list=[]\n","\n","\n","index_end_description = []\n","index_group_description = []\n","list_nlp_description = []\n","z=0\n","s=-1\n","while z<len(df['description']) and j<len(nlp_description):\n","    text_list=[]\n","    index_end_description.append(end_index)\n","    index_group_description.append(j)\n","    \n","    if end_index+1 >= len(nlp_description[j]):\n","        j+=1\n","        start=0\n","        if j < len(nlp_description):\n","            text_nlp_description = [el.text for el in nlp_description[j]]\n","            end_index= text_nlp_description[start:].index(\";\")+1\n","\n","\n","            \n","    else:\n","        start=end_index+1\n","        end_index+= text_nlp_description[start:].index(\";\")+1\n","    z+=1"]},{"cell_type":"markdown","metadata":{},"source":["This our function for generating ngrams, it checks if it is in our embeddings_index"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def treat_composite_words(element):\n","    new_list = []\n","    for word in element:\n","        if word[0].count(\"_\") > 0 and word[0] not in embeddings_index.keys():\n","            for word_split in word[0].split(\"_\"):\n","                if len(word_split) > 1 and word_split in embeddings_index.keys() and word_split not in stop_words.STOP_WORDS:\n","                    new_list.append((word_split, word[1]))\n","        else:\n","            new_list.append(word)\n","    return new_list"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from nltk.util import ngrams\n"," \n","def new_ngrams(element, n, dict_excl=None, dict_incl=None, row_no2=None):\n","    for our_grams in ngrams(element, n):\n","        \n","        is_only__containing_stop_words = all(val in stop_words.STOP_WORDS for val in our_grams)\n","\n","        \n","        does_contain_irrevant_words = False\n","        if dict_excl != None and dict_incl!=None:\n","            does_contain_irrevant_words = any((val in dict_excl and val not in stop_words.STOP_WORDS and val not in dict_incl) for val in our_grams)\n","       \n","        if is_only__containing_stop_words:\n","            continue\n","        \n","        if does_contain_irrevant_words:\n","            continue\n","\n","    \n","        element = \"_\".join(our_grams)\n","    \n","        if element not in embeddings_index.keys() or (dict_excl !=None and element in dict_excl):\n","            continue\n","    \n","        yield  element\n","\n","\n","       "]},{"cell_type":"markdown","metadata":{},"source":["### Label Tokenization and Cleaning Process\n","\n","In this step, we perform **tokenization and preprocessing** on the label text to prepare it for further analysis and embedding.\n","\n","#### Cleaning Steps:\n","- We remove **punctuation**, except for **hyphens (`_`)** and **apostrophes (`'`)** when they appear **within** words.\n","- Apostrophes are **removed** if they appear at the **start or end** of a word (e.g., `'data` → `data`).\n","- We also apply **lemmatization** to reduce words to their base forms (e.g., `\"services\"` → `\"service\"`).\n","- After that, we apply **n-gram generation** to capture important multi-word expressions.\n","\n","#### TF-IDF Filtering:\n","Once tokenization is complete, we compute a **TF-IDF matrix** over the processed label terms.\n","\n","To improve the quality of the label embeddings and reduce noise, we filter out **generic or low-signal terms**. Specifically, we eliminate terms that have a **TF-IDF score lower than 0.3**, as they are considered too common or uninformative for distinguishing between labels.\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO: Pandarallel will run on 11 workers.\n","INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"]}],"source":["import nltk\n","from nltk.corpus import wordnet as wn\n","from pandarallel import pandarallel\n","\n","our_classes = pd.read_csv(\"../inputs/insurance_taxonomy - insurance_taxonomy.csv\")\n","our_classes_vector = our_classes['label']\n","pandarallel.initialize()\n","\n","def our_classes_lemmitize(tokens):\n","   ok=0\n","   original_terms = tokens\n","   merged_token = \" \".join(tokens).lower().replace(\"-\", \"_\").replace(\" _ \", \" \")\n","   \n","   tokens = nlp(merged_token)\n","\n","\n","   \n","   element = list(map(lambda x: replace_word(x.text.lower()), tokens))\n","   elements_words_stop_new = stop_words.STOP_WORDS\n","   elements_words_stop_new.add(\"new\")\n","   elements_words_stop_new.add(\"single\")\n","   elements_words_stop_new.add(\"multi\")\n","\n","   if \"well\" in elements_words_stop_new:\n","      elements_words_stop_new.remove(\"well\")\n","   \n","\n","   tokens = []\n","   for x in element:\n","      \n","      if x not in elements_words_stop_new and x !=\"s\":\n","         if x not in embeddings_index.keys():\n","            joined_word = \"\".join(x.split(\"_\"))\n","            if joined_word not in embeddings_index.keys():\n","               for word_split in x.split(\"_\"):\n","                  \n","                  if word_split not in elements_words_stop_new and word_split !=\"s\" and word_split in embeddings_index.keys():\n","                     tokens.append(word_split)\n","            else:\n","               tokens.append(joined_word)\n","         else:\n","            tokens.append(x)\n","   element = tokens\n","          \n","   element = element +  list(new_ngrams(element,2)) +  list(new_ngrams(element,3))\n","   return element"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["all_punctuation_except_hyphen = string.punctuation.replace(\"-\",\"\").replace(\"'\",\"\").replace(\"’\",\"\").replace(\",\", \"\").replace(\".\",\"\").replace(\"/\", \"\")\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["our_classes['rows_number'] = range(0, len(our_classes['label']))\n","\n","our_classes['label'] = our_classes['label'].replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()\n","our_classes['label'] = [(list(filter(lambda x: x if (\"'\" not in x or (x[0]==\"'\" or x[-1]==\"'\") or \"'s\" in x) and (x.isnumeric() == False) else '', el))) for el in our_classes['label']] \n","\n","our_labels_final = our_classes.apply(lambda x: our_classes_lemmitize(x['label']),axis=1)\n","our_classes['new_label'] = our_labels_final\n","\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction import DictVectorizer\n","from scipy.sparse import csr_matrix\n","from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","our_classes['new_label'] = [list(map(lambda x:x[2:] if len(x) > 2 and x[0:2]==\"__\" else x, label_word)) for label_word in our_classes['new_label']]\n","our_classes['new_label2'] = [Counter(elements_label) for elements_label in our_classes['new_label']]\n","\n","\n","v_label = DictVectorizer(sparse=True)\n","X_label = v_label.fit_transform(our_classes['new_label2'])\n","feature_names_label = v_label.get_feature_names_out()\n","X_csr_label = csr_matrix(X_label)\n","tfidf_transformer_label =TfidfTransformer(smooth_idf=True,use_idf=True)\n","tf_idf_label = tfidf_transformer_label.fit_transform(X_label)\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["match_label_with_tf_idf_valuess = {}\n","match_sum_with_tf_idf_valuess = {}\n","match_no_with_tf_idf_valuess = {}\n","\n","for k in range(0, len(our_classes['new_label2'])):\n","    row_tfidf = X_label[k].toarray()[0]\n","    nonzero_indices = np.nonzero(row_tfidf)[0]\n","    g=0\n","    for j in nonzero_indices:\n","        g+=1\n","        a = round(tf_idf_label[k,j],5)    \n","        match_label_with_tf_idf_valuess[(feature_names_label[j], k)] = a\n","\n","        if (feature_names_label[j], k) not in match_sum_with_tf_idf_valuess.keys():\n","            match_sum_with_tf_idf_valuess[feature_names_label[j]] = a\n","            match_no_with_tf_idf_valuess[feature_names_label[j]] = 1\n","        else:\n","            match_sum_with_tf_idf_valuess[feature_names_label[j]] +=a\n","            match_no_with_tf_idf_valuess[feature_names_label[j]] +=1\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def get_plural_tf_idf_value(word, idx):\n","    if word+\"s\" in match_sum_with_tf_idf_valuess.keys():\n","        return match_sum_with_tf_idf_valuess[word+'s']/match_no_with_tf_idf_valuess[word+'s']\n","    elif word+\"es\" in match_label_with_tf_idf_valuess.keys():\n","        return match_sum_with_tf_idf_valuess[word+'es']//match_no_with_tf_idf_valuess[word+'es']\n","    return match_label_with_tf_idf_valuess[(word, idx)]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["generic_core_terms = {\n","    \"operations\",\n","    'manufacturing',\n","    'application',\n","    'construction',\n","    'consulting',\n","    'planning',\n","    'processing',\n","    'development',\n","    'management',\n","    'installation',\n","    'maintenance'\n","\n","}"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["our_classes['new_label'] =  [ (list(filter(lambda x:x if x  in generic_core_terms or (get_plural_tf_idf_value(x, idx) >= 0.3 or (len(row['new_label'])==1)) else \"\", row['new_label']))) for idx, (_, row) in enumerate(our_classes.iterrows())]"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["label_embeddings = []\n","\n","global rows_\n","def get_sentence_embeddings_labels(word_list):\n","    \n","    embeddings_ = [\n","    embeddings_index.get(word,np.zeros(300))\n","        for word in word_list\n","    ]\n","\n","    embeddings_ = np.mean(embeddings_, axis=0)\n","    \n","    if (len(embeddings_)) < 400:\n","        embeddings_= np.pad(embeddings_, (0, (400-len(embeddings_))), mode = 'constant')\n","\n","    return embeddings_\n","\n","label_embeddings = np.array(our_classes.apply(lambda x: get_sentence_embeddings_labels(x['new_label']), axis=1).to_list())\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Filtering Irrelevant Terms, Locations, and Person Names\n","\n","To improve the quality of our label and company text representations, we also remove **irrelevant terms**, such as **location names** and **person names**, which are not useful for our classification task.\n","\n","#### Named Entity Filtering:\n","We use **spaCy's multilingual model** (`xx_ent_wiki_sm`) to detect and remove:\n","- **Named entities** identified as locations or persons for multiple languagesx\n","\n","#### Additional Heuristics:\n","- If a term is identified as a **person or location** and **contains a hyphen** (e.g., `\"Cafe-Cafe\"`), we double-check its relevance. These might be **misclassified**, so we verify them against our embeddings.\n","- We check whether each term is present in the **`embeddings_index`**:\n","  - If it is **not found**, it's assumed to be irrelevant or out-of-vocabulary.\n","  - If it is found but has a **low cosine similarity score** to any label embedding, we also filter it out.\n","\n","#### Storage:\n","All terms that meet the criteria for removal are added to a set called:\n","```python\n","is_person_or_location_or_irrelevant_terms"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["nlp2 = spacy.load('xx_ent_wiki_sm') \n","nlp_title_ours2 = nlp2(title_company_complete_full.lower())"]},{"cell_type":"markdown","metadata":{},"source":["### Filtering Irrelevant Named Entities\n","\n","We filter out terms that may be wrongly assigned as named entities (`ORG`, `PRODUCT`) but are irrelevant for our labels.  \n","Words are checked using their embeddings: if a word has low similarity to label embeddings or is unknown, it is added to a set of irrelevant terms.\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["is_person_or_location_or_irrevalent_terms = set()\n","\n","for i in nlp_title_ours2.ents:\n","    if i.label_ == \"ORG\" or i.label_ == \"PRODUCT\":\n","        for sp in i.text.split(\" \"):\n","            if len(sp) == 1:\n","                is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","                continue\n","            if sp in embeddings_index.keys():\n","                word_e = embeddings_index[sp.lower()]\n","\n","                if (len(word_e)) < 400:\n","                        word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                word_e = np.array(word_e).reshape(1, -1)\n","\n","                similarities = []\n","                sim = cosine_similarity(word_e, label_embeddings)\n","\n","                if max(sim[0]) < 0.2:\n","                    is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","            else:\n","                is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","\n","                                        \n","    else:\n","        if len(i.text.split(\"-\"))>1:\n","\n","            split_words = re.split(r'[-\\s]+', i.text)\n","            for sp in split_words:\n","\n","                if sp in embeddings_index.keys():\n","                    if len(sp) == 1:\n","                        is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","                        continue\n","                    word_e = embeddings_index[sp.lower()]\n","\n","                    if (len(word_e)) < 400:\n","                            word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                    word_e = np.array(word_e).reshape(1, -1)\n","\n","                    similarities = []\n","                    sim = cosine_similarity(word_e, label_embeddings)\n","\n","                    if max(sim[0]) < 0.2:\n","                        is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","                else:\n","                    is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","\n","                    \n","        else:\n","            is_person_or_location_or_irrevalent_terms.add(i.text.lower())\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Enhanced Filtering of Irrelevant Named Entities\n","\n","We refine entity filtering by analyzing both individual words and compound phrases from business-related tags.  \n","The goal is to discard terms that are irrelevant or weakly associated with our label embeddings.\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["is_person_or_location_or_irrevalent_terms2 = set()\n","ignore_in_compond_word = set()\n","\n","for nlp_bt in nlp_business_tags:\n","    for i in nlp_bt.ents:\n","            \n","        if i.label_ == \"ORG\" or i.label_ == \"PRODUCT\":\n","            for sp in i.text.split(\" \"):\n","                if len(sp) == 1:\n","                    is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                    continue\n","                if sp not in embeddings_index.keys():\n","                    is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                  \n","        else:\n","            if (len(i.text.split(\"-\"))>1 or len(i.text.split(\" \"))>1) and not(i.label_ == \"CARD\" or i.label_ == \"ORDINAL\" or i.label_ == \"TIME\" or i.label_ == \"DATE\"):\n","                split_words = re.split(r'[-\\s]+', i.text)\n","                for sp in split_words:\n","\n","                    if sp in embeddings_index.keys():\n","                        if len(sp) == 1:\n","                            is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                            continue\n","                        word_e = embeddings_index[sp.lower()]\n","\n","                        if (len(word_e)) < 400:\n","                                word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                        word_e = np.array(word_e).reshape(1, -1)\n","\n","                        similarities = []\n","                        sim = cosine_similarity(word_e, label_embeddings)\n","                        \n","                        if(max(sim[0])< 0.35):\n","                            word_ctx =  nlp(sp)\n","                            if len(word_ctx.ents) > 0 and not(word_ctx.ents[0].label_ == \"ORG\" or word_ctx.ents[0].label_ == \"PRODUCT\"):\n","                                is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","\n","                    else:\n","\n","                        is_person_or_location_or_irrevalent_terms2.add(sp.lower())                        \n","            else:\n","\n","\n","                word_e = embeddings_index.get(i.text.lower(), np.zeros(300))\n","\n","                if (len(word_e)) < 400:\n","                        word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                word_e = np.array(word_e).reshape(1, -1)\n","                sim = cosine_similarity(word_e, label_embeddings)\n","                pattern_float = r\"^-?\\d+\\.\\d+$\"\n","                if max(sim[0]) >= 0.30 and (not i.text.isnumeric() and not re.match(pattern_float, i.text)) and i.label_ != \"DATE\":\n","                    continue\n","\n","                if max(sim[0]) >= 0.30 and (not i.text.isnumeric() and not re.match(pattern_float, i.text)) and i.label_ == \"DATE\":\n","                    ignore_in_compond_word.add(i.text)\n","\n","                is_person_or_location_or_irrevalent_terms2.add(i.text.lower().replace(\" \", \"_\"))\n","                list_text = list(new_ngrams(i.text.lower().replace(\"_\", \" \").replace(\"-\", \" \").split(\" \"), 2))\n","                for j in list_text:\n","                    if(j not in embeddings_index.keys()):\n","                        is_person_or_location_or_irrevalent_terms2.add(j.lower())\n","                        continue\n","                    word_e = embeddings_index[j.lower()]\n","\n","                    if (len(word_e)) < 400:\n","                            word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                    word_e = np.array(word_e).reshape(1, -1)\n","                    sim = cosine_similarity(word_e, label_embeddings)\n","\n","                    if max(sim[0]) < 0.3:\n","                        if (max(sim[0])) > 0.25:\n","                            ignore_in_compond_word.add(j.lower())\n","                            \n","                        is_person_or_location_or_irrevalent_terms2.add(j.lower())\n","                    else:\n","                        if(max(sim[0])< 0.35):\n","                            word_ctx =  nlp(j)\n","\n","                            if len(word_ctx.ents) > 0 and not(word_ctx.ents[0].label_ == \"PERSON\" or word_ctx.ents[0].label_ == \"ORG\" or word_ctx.ents[0].label_ == \"PRODUCT\"):\n","                                is_person_or_location_or_irrevalent_terms2.add(j.lower())\n","\n","                if(i.label_ == \"LOC\" or i.label_==\"GPE\"):\n","                    i_final=i.text.lower().replace(\" \", \"_\")\n","                    for el in i_final.split(\"_\"):\n","                        if(el not in embeddings_index.keys()):\n","                            is_person_or_location_or_irrevalent_terms2.add(el.lower())\n","                            continue\n","                        word_e = embeddings_index[el.lower()]\n","\n","                        if (len(word_e)) < 400:\n","                                word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                        word_e = np.array(word_e).reshape(1, -1)\n","                        sim = cosine_similarity(word_e, label_embeddings)\n","                        if max(sim[0]) < 0.3:\n","                            if (max(sim[0])) > 0.25:\n","                                ignore_in_compond_word.add(el.lower())\n","\n","                            is_person_or_location_or_irrevalent_terms2.add(el.lower())\n","                        else:\n","                            if(max(sim[0])< 0.35):\n","                                word_ctx =  nlp(el)\n","\n","                                if len(word_ctx.ents) > 0 and not(word_ctx.ents[0].label_ == \"PERSON\" or word_ctx.ents[0].label_ == \"ORG\" or word_ctx.ents[0].label_ == \"PRODUCT\"):\n","                                    is_person_or_location_or_irrevalent_terms2.add(el.lower())\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Filtering Irrelevant Terms from Descriptions\n","\n","We apply the same logic to entity extraction from **description fields**, ensuring only meaningful terms are kept for label prediction."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["for nlp_desc in nlp_description:\n","    for i in nlp_desc.ents:\n","     \n","        if i.label_ == \"ORG\" or i.label_ == \"PRODUCT\":\n","            for sp in i.text.split(\" \"):\n","\n","                if len(sp) == 1:\n","                    is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                    continue\n","                if sp not in embeddings_index.keys():\n","                    is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                  \n","        else:\n","            if len(i.text.split(\"-\"))>1 and not(i.label_ == \"CARD\" or i.label_ == \"ORDINAL\" or i.label_ == \"TIME\" or i.label_ == \"DATE\"):\n","                split_words = re.split(r'[-\\s]+', i.text)\n","                for sp in split_words:\n","                    if sp in embeddings_index.keys():\n","                        if len(sp) == 1:\n","                            is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                            continue\n","                        word_e = embeddings_index[sp.lower()]\n","\n","                        if (len(word_e)) < 400:\n","                                word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                        word_e = np.array(word_e).reshape(1, -1)\n","\n","                        similarities = []\n","                        sim = cosine_similarity(word_e, label_embeddings)\n","                        if max(sim[0]) < 0.3:\n","                            if (max(sim[0])) > 0.25:\n","                                ignore_in_compond_word.add(sp.lower())\n","                            is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","                        else:\n","                            if(max(sim[0])< 0.35):\n","                                word_ctx =  nlp(sp)\n","                                if len(word_ctx.ents) > 0 and not(word_ctx.ents[0].label_ == \"PERSON\" or word_ctx.ents[0].label_ == \"ORG\" or word_ctx.ents[0].label_ == \"PRODUCT\"):\n","                                    sim1 = cosine_similarity(word_e, label_embeddings)\n","                                    is_person_or_location_or_irrevalent_terms2.add(sp.lower())\n","\n","                    else:\n","                        is_person_or_location_or_irrevalent_terms2.add(sp.lower())                        \n","            else:\n","                word_e = embeddings_index.get(i.text.lower(), np.zeros(300))\n","\n","                if (len(word_e)) < 400:\n","                        word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                word_e = np.array(word_e).reshape(1, -1)\n","                sim = cosine_similarity(word_e, label_embeddings)\n","                pattern_float = r\"^-?\\d+\\.\\d+$\"\n","                val = max(sim[0])\n","\n","                if max(sim[0]) >= 0.30 and (not i.text.isnumeric() and not re.match(pattern_float, i.text)) and i.label_ != \"DATE\":\n","                    continue\n","                \n","\n","                if max(sim[0]) >= 0.30 and (not i.text.isnumeric() and not re.match(pattern_float, i.text)) and i.label_ == \"DATE\":\n","                    ignore_in_compond_word.add(i.text)\n","\n","                is_person_or_location_or_irrevalent_terms2.add(i.text.lower().replace(\" \", \"_\"))\n","                \n","                list_text = list(new_ngrams(i.text.lower().replace(\"_\", \" \").replace(\"-\", \" \").split(\" \"), 2))\n","                for j in list_text:\n","                    if(j not in embeddings_index.keys()):\n","                        is_person_or_location_or_irrevalent_terms2.add(j.lower())\n","                        continue\n","                    word_e = embeddings_index[j.lower()]\n","                    word_e_list = [np.pad(embeddings_index.get(wd, np.zeros(300)), (0, (400-len(embeddings_index.get(wd, np.zeros(300))))), mode = 'constant') for wd in j.split(\"_\")]\n","\n","                    if (len(word_e)) < 400:\n","                            word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                    word_e = np.array(word_e).reshape(1, -1)\n","                    sim = cosine_similarity(word_e, label_embeddings)\n","                    sim_full = cosine_similarity(word_e_list, label_embeddings)\n","                    sim_full_to_list = []\n","                    max_v = -1.0\n","                    index_v = -1\n","                    for idx, zzz in enumerate(sim_full):\n","                        cur_element_max = np.max(zzz)\n","                        if cur_element_max > max_v:\n","                            index_v = idx\n","                            max_v = cur_element_max\n","\n","\n","                    if max_v > 0.4:\n","                        lit_g = j.split(\"_\")[index_v]\n","\n","                \n","                    if max(sim[0]) < 0.3:\n","                        if (max(sim[0])) > 0.25:\n","                            ignore_in_compond_word.add(j.lower())\n","\n","\n","                        is_person_or_location_or_irrevalent_terms2.add(j.lower())\n","                    else:\n","                        if(max(sim[0])< 0.35):\n","                            word_ctx =  nlp(j)\n","                            if len(word_ctx.ents) > 0 and not(word_ctx.ents[0].label_ == \"PERSON\" or word_ctx.ents[0].label_ == \"ORG\" or word_ctx.ents[0].label_ == \"PRODUCT\"):\n","                                is_person_or_location_or_irrevalent_terms2.add(j.lower())\n","\n","                if(i.label_ == \"LOC\" or i.label_==\"GPE\"):\n","                   \n","                    i_final=i.text.lower().replace(\" \", \"_\")\n","                    for el in i_final.split(\"_\"):\n","                        if(el not in embeddings_index.keys()):\n","                            is_person_or_location_or_irrevalent_terms2.add(el.lower())\n","                            continue\n","                        word_e = embeddings_index[el.lower()]\n","\n","                        if (len(word_e)) < 400:\n","                                word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","                        word_e = np.array(word_e).reshape(1, -1)\n","                        sim = cosine_similarity(word_e, label_embeddings)\n","                        if max(sim[0]) < 0.3:\n","                            if (max(sim[0])) > 0.25:\n","                                ignore_in_compond_word.add(el.lower())\n","                            \n","                            is_person_or_location_or_irrevalent_terms2.add(el.lower())\n","                        else:\n","                            if(max(sim[0])< 0.35):\n","\n","                                word_ctx =  nlp(el)\n","                                if len(word_ctx.ents) > 0 and not(word_ctx.ents[0].label_ == \"PERSON\" or word_ctx.ents[0].label_ == \"ORG\" or word_ctx.ents[0].label_ == \"PRODUCT\"):\n","                                    \n","                                    is_person_or_location_or_irrevalent_terms2.add(el.lower())"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["nlp_title_ours = nlp(title_company_complete_full.lower())"]},{"cell_type":"markdown","metadata":{},"source":["### Description Text Cleaning with Named Entity Filtering\n","\n","When processing the **company descriptions**, we apply a more refined filtering step to remove **unnecessary or irrelevant terms** that do not contribute meaningful information for classification.\n","\n","#### Named Entity Filtering:\n","Using spaCy's entity recognition, we filter out tokens that are identified as any of the following entity types:\n","- **Person**\n","- **Location**\n","- **Geopolitical Entity (GPE)**\n","- **Cardinal** (e.g., numbers like \"three\", \"100\")\n","- **Ordinal** (e.g., \"first\", \"second\")\n","\n","These entities are often not useful for identifying the company’s industry or services, and are removed to **clean the text** and reduce semantic noise.\n","\n","This helps ensure that the remaining tokens focus on **core business-related terms**, improving the quality of embeddings and downstream label matching."]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["dict_gpe_company_elements = {}\n","\n","for i in nlp_description:\n","    for el in i.ents:\n","        if (el.label_ != \"ORG\" and el.label_ != \"PRODUCT\"):\n","         for words in el.text.split(\" \"):\n","            dict_gpe_company_elements[words.lower()] = 1"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["def replace_word_2(text, text_original, next_word, next_word_original, previous_word, previous_word_original, row_no2=None):\n","\n","    \n","    if len(text) <= 1:\n","        return text\n","    if text[0] == '-':\n","        text_original = text_original[1:]\n","        text = text[1:]\n","    if text[-1] == \"-\":\n","        text_original = text_original[:-1]\n","        text = text[:-1]\n","\n","    if \"'s\" not in text and \"'\" in text:\n","        text_original = text_original.replace(\"'\", \"\")\n","        text = text.replace(\"'\",\"\")\n","\n","    \n","    if text_original in embeddings_index.keys():\n","        \n","        if next_word != None and text != None and next_word_original in embeddings_index.keys() and f\"{text}_{next_word}\" not in embeddings_index.keys() and f\"{text_original}_{next_word_original}\" in embeddings_index.keys():\n","            return text_original\n","        \n","        if previous_word != None and text != None and previous_word_original in embeddings_index.keys() and f\"{previous_word}_{text}\" not in embeddings_index.keys() and f\"{previous_word_original}_{text_original}\" in embeddings_index.keys():\n","            return text_original\n","        \n","\n","        if len(text) > 2 and text[-2:]==\"um\" and text_original[-1]==\"a\":\n","            return text_original\n","    \n","\n","\n","    if text in embeddings_index.keys():\n","        return text\n","    return text_original"]},{"cell_type":"markdown","metadata":{},"source":["**Data Cleaning**\n","\n","To ensure consistency and avoid issues during processing, we removed all rows containing **missing values**.  \n","Additionally, we excluded the `sector` column from our analysis, as it provided minimal value for the classification task and did not contribute meaningful signals for label assignment."]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["colums = [('description',0), ('business_tags',1), ('category',2), ('niche',3)]"]},{"cell_type":"markdown","metadata":{},"source":["**Token Filtering**\n","\n","During tokenization, we retained only **adjectives**, **verbs**, and **nouns** to improve clarity and reduce noise.  \n","This focused filtering ensures that we capture the most meaningful and descriptive terms, while eliminating less relevant tokens such as stopwords, conjunctions, or determiners."]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["def replace_tag(element, tokens, original_text, row_no):\n","\n","    \n","    \n","    \n","    match tokens:\n","        case 'CD':\n","            res = any(char.isalpha() for char in element)\n","            if res:\n","                tokens = 'a'\n","                return tokens\n","            return 'n'\n","        case 'JJ':\n","            tokens = 'a'\n","            return tokens\n","        case 'NN' | 'NNS' | 'NNP' | 'NNPS':\n","            tokens = 'n'\n","            return tokens\n","        case 'VB' | 'VBG' | 'VBD' | 'VBZ' | 'VBN' | 'VBP':\n","            return 'v'\n","        case _:\n","            if(element not in embeddings_index.keys()):\n","                return 'n'\n","            tokens = 'z'\n","            return tokens"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["import faiss\n","def remove_irrelevalent_ngrams_with_stop_words(wd):\n","    is_containing_stop_words = any(val in stop_words.STOP_WORDS for val in wd.split(\"_\"))\n","    if wd.split(\"_\")[0] in stop_words.STOP_WORDS:\n","        return False\n","\n","    if not is_containing_stop_words:\n","        return True\n","\n","    word_e = embeddings_index[wd.lower()]\n","    if (len(word_e)) < 400:\n","        word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","    word_e = word_e.reshape(1,-1).astype(np.float32, order='C')\n","    faiss.normalize_L2(word_e)\n","\n","    le1 = np.array(label_embeddings, dtype=np.float32, order='C')\n","    faiss.normalize_L2(le1)\n","\n","    el = max(np.dot(word_e, le1.T)[0])\n","\n","    if el >= 0.30:\n","        return True\n","    return False"]},{"cell_type":"markdown","metadata":{},"source":["### Company Text Cleaning and Filtering\n","\n","Just like we did for the labels, we also applied a similar cleaning process to the **company descriptions**. To make this more efficient, we used **index and end lists** to manage the token positions for each company, which helps us process only the relevant text segments efficiently.\n","\n","#### Cleaning Steps:\n","- We removed **irrelevant terms** such as:\n","  - **Punctuation**\n","  - Previously identified terms stored in `dict_gpe_company_elements`\n","  - **Stopwords** (common words with little semantic value)\n","- We kept only tokens that are:\n","  - **Adjectives**\n","  - **Nouns**\n","  - **Verbs**\n","\n","This ensures we focus only on the most **informative words** in each company’s description, improving both the semantic quality and classification performance."]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["def get_relevant_words(current_list):\n","    word_embedding_list1 = np.array([np.pad(embeddings_index.get(wd.lower(), np.zeros(300)), (0, label_embeddings[0].shape[0] - len(embeddings_index.get(wd.lower(), np.zeros(300)))), mode='constant') for wd in current_list])\n","    \n","    we1 = np.array(word_embedding_list1, dtype=np.float32, order='C')\n","    faiss.normalize_L2(we1)\n","\n","    le1 = np.array(label_embeddings, dtype=np.float32, order='C')\n","    faiss.normalize_L2(le1)\n","\n","    el = np.dot(we1, le1.T)\n","\n","    word_list = [wd for wd in current_list]\n","    current_row = current_list.copy()\n","    len_current_row_original = len(current_row)\n","    \n","        \n","    current_row = current_list.copy()\n","    threshold=0.3\n","\n","    for index in range(len(current_list)):\n","        max_val_word = max(el[index])\n","        if max_val_word <= threshold:\n","            current_row.remove(word_list[index])\n","            \n","    \n","    return current_row"]},{"cell_type":"markdown","metadata":{},"source":["### Handling Acronyms and Fallback Term Mapping\n","\n","We use external assertions to improve term recognition:\n","\n","- **Acronyms**: We detect acronyms by matching short forms to their expanded versions based on initials and verifying similarity using embeddings. Special cases like \"abbreviation for\", \"short name for\", and \"acronym for\" are also handled.\n","- **Plural and Fallback Mapping**: The `convertPluralToSingular` mapping (name slightly misleading) is used not only for converting plurals to singulars but also as a fallback to find alternate forms of words missing from the embeddings.\n","- **Fallback for Missing Terms**: If a word is not found in the embeddings, we first attempt to map it using `convertPluralToSingular` before further processing.\n","\n","This enhances entity extraction by recovering better word matches when embeddings alone are insufficient."]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["import csv\n","import time\n","antonyms_words = set()\n","convertPluralToSingular = {}\n","\n","with open(\"../inputs/filtered_assertions.txt\") as f:\n","    rows = f.read().split(\"\\n\")\n","    for row in rows:\n","        if row==\"\":\n","            continue\n","        words = row.split(\" \")\n","\n","        if (words[0]==\"/r/IsA\"):\n","            word1 = words[1]\n","            word2 = words[2]\n","            if (len(word1) == 2 or len(word1) == 3) and not word1.isnumeric() and len(word2.split(\"_\"))>1:\n","                a, b = word2.split(\"_\")[0], word2.split(\"_\")[1]\n","                c = None\n","                if len(word1) == 3:\n","                    if len(word2.split(\"_\"))>2:\n","                        c = word2.split(\"_\")[2]\n","                if word1 in embeddings_index.keys() and word2 in embeddings_index.keys() and a[0]==word1[0] and b[0]==word1[1] and (c == None or c[0] == word1[2]):\n","                    current_val = np.dot(embeddings_index[word1].reshape(1,-1), embeddings_index[word2].reshape(1,-1).T)\n","                    if (current_val > 0.5):\n","                        acronym_dict[word1] = word2\n","                elif word1 in embeddings_index.keys() and len(word2.split(\"abbreviation_for_\")) > 1 and word2.split(\"abbreviation_for_\")[1] in embeddings_index.keys():\n","                    word2 = word2.split(\"abbreviation_for_\")[1]\n","                    current_val = (np.dot(embeddings_index[word1].reshape(1,-1), embeddings_index[word2].reshape(1,-1).T))\n","                    if (current_val > 0.5):\n","                        acronym_dict[word1] = word2\n","                elif word1 in embeddings_index.keys() and len(word2.split(\"short_name_for_\")) > 1 and word2.split(\"short_name_for_\")[1] in embeddings_index.keys():\n","                    word2 = word2.split(\"short_name_for_\")[1]\n","                    current_val = (np.dot(embeddings_index[word1].reshape(1,-1), embeddings_index[word2].reshape(1,-1).T))\n","                    if (current_val > 0.5):\n","                        acronym_dict[word1] = word2\n","                elif word1 in embeddings_index.keys() and len(word2.split(\"acronym_for_\")) > 1 and word2.split(\"acronym_for_\")[1] in embeddings_index.keys():\n","                    word2 = word2.split(\"acronym_for_\")[1]\n","                    current_val = (np.dot(embeddings_index[word1].reshape(1,-1), embeddings_index[word2].reshape(1,-1).T))\n","                    if (current_val > 0.5):\n","                        acronym_dict[word1] = word2\n","        if (words[0]==\"/r/FormOf\"):\n","            word1 = words[1]\n","            word2 = words[2]\n","            convertPluralToSingular[word1] = word2\n","        "]},{"cell_type":"markdown","metadata":{},"source":["### Token Processing and Lemmatization\n","\n","The `lemmatize_f` function processes text from different columns (description, business tags, category, niche) to create clean, relevant token sets for further analysis.\n","\n","### Main Steps\n","\n","- **Lemmatization**: Extract base forms of words using spaCy, filtering by POS tags (`noun`, `adjective`, `verb`).\n","- **Sentence Splitting** (for descriptions): Focuses only on the first sentence for key term extraction.\n","- **Bigram and Trigram Generation**: Creates 2-word and 3-word combinations while filtering out irrelevant terms.\n","- **Acronym and Composite Handling**: Replaces detected acronyms and treats compound words consistently.\n","- **Stopword and Irrelevant Term Removal**: Filters out known irrelevant, short, numeric, and stopword tokens.\n","- **First Sentence Extraction**: For descriptions, focuses on words from the first sentence after the title, assuming they represent the most important content. These important words are specifically filtered and saved (in `most_important_words` / `elements_add_important_words`).\n","\n","- **Fallback Corrections**:\n","  - Corrects British vs American spelling differences (`er` ↔ `re`, `ou` ↔ `o`).\n","  - Uses `convertPluralToSingular` for additional fallback mapping if needed.\n","- **Final Output**: Returns a frequency counter of the cleaned and corrected tokens.\n","\n","### Goal\n","\n","Extract meaningful, context-aware tokens that are robust across different text fields for downstream tasks like classification or matching."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["df2['new_business_tags_lemmatized'] = df2['business_tags'].copy()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["\n","elements_add_important_words = []\n","\n","end_of_first_sentence = []\n","def lemmatize_f(tokens, col, row_no, row_no2, description, business_tags, category):\n","    pattern_float = r\"^-?\\d+\\.\\d+$\"\n","    elements3 = None\n","    if col == \"description\":\n","        global nlp_description\n","        global index_end_description\n","        global index_group_description\n","        global dict_gpe_company_elements\n","        start_from_here = -1\n","        if row_no == 0 or (index_group_description[row_no]!=0 and index_group_description[row_no-1]!=index_group_description[row_no]):\n","            start_from_here = 0\n","        else:\n","            start_from_here = index_end_description[row_no-1]+1\n","        tok = nlp_description[index_group_description[row_no]][start_from_here:index_end_description[row_no]]\n","       \n","        elements = [i.lemma_ for i in tok]\n","        g=0\n","\n","        if \".\" in elements:\n","            g+=1\n","            \n","            if len(elements) != elements.index('.')+1 and (elements[elements.index('.')+1]==','):\n","                end_of_first_sentence.append(elements[elements.index('.')+1:].index('.'))\n","                elements = elements[0:elements[elements.index('.')+1:].index('.')]\n","            else:\n","                end_of_first_sentence.append(elements.index('.'))\n","                elements =elements[0:elements.index('.')]\n","                \n","        \n","\n","        elements3 = list(filter(lambda x: x not in embeddings_index.keys() and len(x.split(\"_\"))>1, elements))\n","        \n","        elements2 = list(map(lambda x: (replace_word_2(tok[x].lemma_, tok[x].text, tok[x+1].lemma_, tok[x+1].text, None, None),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)) if x==0 else (replace_word_2(tok[x].lemma_, tok[x].text, None, None, tok[x-1].lemma_, tok[x-1].text),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)) if x==len(tok)-1 else (replace_word_2(tok[x].lemma_, tok[x].text, tok[x+1].lemma_, tok[x+1].text, tok[x-1].lemma_, tok[x-1].text),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)), range(len(tok[0:len(elements)]))))\n","\n","        \n","        tokens_raw_elements = list(map(lambda x: x[0], elements2))\n","\n","        list_bigrams_elements = list(new_ngrams(tokens_raw_elements, 2, is_person_or_location_or_irrevalent_terms2,ignore_in_compond_word, row_no2))\n","        list_trigrams_elements = list(new_ngrams(tokens_raw_elements, 3, is_person_or_location_or_irrevalent_terms2,ignore_in_compond_word, row_no2))\n","        elements2 = treat_composite_words(elements2)\n","        \n","        \n","        elements3_copy = []\n","        for an_element in elements3:\n","\n","            for split_word in an_element.split(\"_\"):\n","                elements3_copy.append(split_word)\n","        elements3 = elements3_copy\n","        elements3 = list(map(lambda x: replace_upper_word([x], acronym_dict)[0] if x.isupper() else replace_upper_word([x.lower()], acronym_dict)[0], elements3))\n","        elements3= list(filter(lambda x: x.lower() not in is_person_or_location_or_irrevalent_terms2 and x.lower() not in stop_words.STOP_WORDS and not x.isnumeric(), elements3))\n","\n","\n","\n","        elements2 = list(filter(lambda x: (x[0].lower(),x[1]) if x[0].lower() not in stop_words.STOP_WORDS and x[0].lower() in embeddings_index.keys()  and (x[1]=='n' or x[1]=='a' or x[1] == 'v') and (not x[0].isnumeric() and not re.match(pattern_float, x[0])) else '', elements2))\n","        \n","\n","        \n","        elements2 = list(filter(lambda x: x[0].lower() not in is_person_or_location_or_irrevalent_terms2, elements2))\n","        elements2 = list(map(lambda x: replace_upper_word([x[0]], acronym_dict)[0] if x[0].isupper() else replace_upper_word([x[0].lower()], acronym_dict)[0], elements2))\n","\n","        elements2.extend(list(list_bigrams_elements))\n","        elements2.extend(list(list_trigrams_elements))\n","        if elements3 != []:\n","            elements3 = get_relevant_words(elements3)\n","        elements2.extend(elements3)\n","        elements_add_important_words.append(elements2)\n","        \n","\n","    \n","\n","    elif col == \"business_tags\":\n","        global nlp_business_tags\n","        global index_end_business_tags\n","        global index_group_business_tags\n","        start_from_here = -1\n","        if row_no == 0 or (index_group_business_tags[row_no]!=0 and index_group_business_tags[row_no-1]!=index_group_business_tags[row_no]):\n","            start_from_here = 0\n","        else:\n","            start_from_here = index_end_business_tags[row_no-1]+1\n","        tok =[element for element in nlp_business_tags[index_group_business_tags[row_no]][start_from_here:index_end_business_tags[row_no]]]\n","    elif col == \"category\":\n","        global nlp_category\n","        global index_end_category\n","        start_from_here = -1\n","        if row_no == 0:\n","            start_from_here = 0\n","        else:\n","            start_from_here = index_end_category[row_no-1]\n","        tok = [element for element in nlp_category[start_from_here:index_end_category[row_no]]]\n","    else:\n","        global nlp_niche\n","        global index_end_niche\n","        start_from_here = -1\n","        if row_no == 0:\n","            start_from_here = 0\n","        else:\n","            start_from_here = index_end_niche[row_no-1]\n","        tok = [element for element in nlp_niche[start_from_here:index_end_niche[row_no]]]\n","    \n","    if len(tok)>1:\n","        tokens = list(map(lambda x:  (replace_word_2(tok[x].lemma_, tok[x].text, tok[x+1].lemma_, tok[x+1].text, None, None),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)) if x==0 else (replace_word_2(tok[x].lemma_, tok[x].text, None, None, tok[x-1].lemma_, tok[x-1].text),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)) if x==len(tok)-1 else (replace_word_2(tok[x].lemma_, tok[x].text, tok[x+1].lemma_, tok[x+1].text, tok[x-1].lemma_, tok[x-1].text),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)), range(len(tok))))\n","        \n","    elif len(tok)==1:\n","        tokens = list(map(lambda x:  (replace_word_2(tok[x].lemma_, tok[x].text, None, None, None, None),replace_tag(tok[x].lemma_, tok[x].tag_, tok[x].text, row_no)), range(len(tok))))\n","    else:\n","        tokens = []\n","\n","   \n","    \n","    tokens_raw = list(map(lambda x: x[0], tokens))\n","  \n","    list_bigrams = list(new_ngrams(tokens_raw, 2, is_person_or_location_or_irrevalent_terms2,ignore_in_compond_word, row_no2))\n","   \n","    list_trigrams = list(new_ngrams(tokens_raw, 3, is_person_or_location_or_irrevalent_terms2,ignore_in_compond_word, row_no2))\n","    \n"," \n","    list_bigrams = (list(filter(lambda x: x if remove_irrelevalent_ngrams_with_stop_words(x) else '', list_bigrams)))\n","    list_trigrams = (list(filter(lambda x: x if remove_irrelevalent_ngrams_with_stop_words(x) else '', list_trigrams)))\n","    tokens = treat_composite_words(tokens)\n","    tokens = list(filter(lambda x: (x[0].lower(),x[1]) if x[0].lower() not in stop_words.STOP_WORDS and x[0].lower() in embeddings_index.keys()  and (x[1]=='n' or x[1]=='a' or x[1] == 'v') and (col!=\"description\" or x[0].lower() not in is_person_or_location_or_irrevalent_terms2) and (not x[0].isnumeric() and not re.match(pattern_float, x[0])) else '', tokens))\n","    tokens = list(filter(lambda x: x[0].lower() not in is_person_or_location_or_irrevalent_terms2, tokens))\n","\n","\n","    tokens = list(map(lambda x: replace_upper_word([x[0]], acronym_dict)[0] if x[0].isupper() else replace_upper_word([x[0].lower()], acronym_dict)[0], tokens))\n","    full_list = None\n","    if elements3!=None:\n","        full_list = tokens + list_bigrams + list_trigrams + elements3\n","    else:\n","        full_list = tokens + list_bigrams + list_trigrams\n","\n","    tokens1 = tokens.copy()\n","\n","    \n","    if col==\"business_tags\":\n","        tokens1.extend(description)\n","    if col==\"category\":\n","        tokens1.extend(description)\n","        tokens1.extend(business_tags)\n","    if col==\"niche\":\n","        tokens1.extend(description)\n","        tokens1.extend(business_tags)\n","        tokens1.extend(category)\n","\n","\n","        \n","    for idx, wd in enumerate(full_list):\n","        \n","        if wd.replace(\"er\", \"re\") in tokens1 and wd.replace(\"er\",\"re\")!=wd and cosine_similarity(embeddings_index[wd.replace(\"er\",\"re\")].reshape(1,-1), embeddings_index[wd].reshape(1,-1))>0.85:\n","            full_list[idx] = wd.replace(\"er\", \"re\")\n","        elif wd.replace(\"ou\", \"o\") in tokens1 and wd.replace(\"ou\",\"o\")!=wd and cosine_similarity(embeddings_index[wd.replace(\"ou\",\"o\")].reshape(1,-1), embeddings_index[wd].reshape(1,-1))>0.85:\n","            full_list[idx] = wd.replace(\"ou\", \"o\")\n","        elif wd in convertPluralToSingular and convertPluralToSingular[wd] in tokens1 and cosine_similarity(embeddings_index[convertPluralToSingular[wd]].reshape(1,-1), embeddings_index[wd].reshape(1,-1))>0.75:\n","            \n","            full_list[idx] =convertPluralToSingular[wd]\n","\n","    \n","    counter_element = Counter(full_list)\n","    \n","\n","    return counter_element\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["all_punctuation_except_hyphen = string.punctuation.replace(\"-\",\"\").replace(\"'\",\"\").replace(\"’\",\"\")\n","df['rows'] = range(0, len(df))\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["df['rows_33'] = [idx for idx, _ in df.iterrows() ]"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["df['most_important_words'] = df['description']"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["start_from_here = -1\n","row_no=1\n","if row_no == 0 or (index_group_business_tags[row_no]!=0 and index_group_business_tags[row_no-1]!=index_group_business_tags[row_no]):\n","    start_from_here = 0\n","else:\n","    start_from_here = index_end_business_tags[row_no-1]+1\n","el_a = [element for element in nlp_business_tags[index_group_business_tags[1]][start_from_here:index_end_business_tags[1]]]\n","copy_a = el_a.copy()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","for col, idx in colums:\n","    df[col] = df.apply(lambda x: lemmatize_f(x[col], col, x['rows'], x['rows_33'], x['description'], x['business_tags'], x['category']), axis =1)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["z=0\n","for idx, _ in df.iterrows():\n","    df.at[idx, 'most_important_words'] = elements_add_important_words[z]\n","    z+=1"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["def check_if_word_is_okay(x, elements_counter):\n","    elements = list(filter(lambda z: z in x, set(elements_counter)))\n","    return elements\n","    "]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["dict_associate_index_with = {}\n","z=0\n","for idx, _ in df.iterrows():\n","    dict_associate_index_with[z]=idx\n","    z+=1"]},{"cell_type":"markdown","metadata":{},"source":["### Extracting Important Words from Titles\n","\n","This part processes the titles to extract the most meaningful words, aiming to support downstream tasks like classification.\n","\n","### Main Steps\n","\n","- **Title Tokenization**: Titles are split based on the semicolon (`;`) separator.\n","- **Word Filtering**: Remove stopwords, irrelevant terms, punctuation, and very short words.\n","- **Word Validation**:\n","  - If a word is missing in embeddings, validate it using associated fields (description, niche, category, business tags).\n","  - Ensure words are semantically meaningful by checking their cosine similarity with label embeddings (threshold ≥ 0.35).\n","- **Important Word Collection**: \n","  - Filtered and validated words are collected for each title.\n","  - Importance is emphasized by weighting (repeating the Counter multiple times).\n","- **Final Output**: \n","  - The list `titles_important_word` stores a Counter for each title, containing its most important extracted words.\n","\n","### Goal\n","\n","Capture the most relevant words from titles, cleaned and validated, to enhance the quality of feature extraction."]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["from functools import reduce\n","titles_important_word = [None] * len(df['description'])\n","nlp_title_ours_list = [title.lemma_ for title in nlp_title_ours]\n","s=-1\n","z=0\n","word_no = -1\n","\n","while len(nlp_title_ours_list)>word_no+1:\n","    word_no += nlp_title_ours_list[s+1:].index(\";\") + 1\n","    \n","\n","    word_companies_list = nlp_title_ours_list[s+1:word_no]\n","    word_companies_list = [x for x in word_companies_list if x not in stop_words.STOP_WORDS and x not in is_person_or_location_or_irrevalent_terms and len(x) > 1 and x not in string.punctuation]\n","    word_companies_list = [item for sublist in word_companies_list for item in (sublist if isinstance(sublist, list) else [sublist])]\n","\n"," \n","\n","    list_titles=[]\n","    wd_list = []\n","    for sp in word_companies_list:\n","        if sp not in embeddings_index.keys():\n","            wd_list.extend(check_if_word_is_okay(sp.lower(), df['description'][dict_associate_index_with[z]]+df['niche'][dict_associate_index_with[z]]+df['category'][dict_associate_index_with[z]]+df['business_tags'][dict_associate_index_with[z]]))\n","        else:\n","            wd_list.append(sp.lower())\n","    word_companies_list = wd_list  \n","    for sp in word_companies_list:\n","\n","        if sp in is_person_or_location_or_irrevalent_terms:\n","             continue\n","\n","        if len(sp) == 1:\n","            is_person_or_location_or_irrevalent_terms.add(sp.lower())\n","            continue\n","             \n","        word_e = embeddings_index[sp.lower()]\n","\n","        if (len(word_e)) < 400:\n","                word_e= np.pad(word_e, (0, (400-len(word_e))), mode = 'constant')\n","\n","        word_e = np.array(word_e).reshape(1, -1)\n","\n","        similarities = []\n","        sim = cosine_similarity(word_e, label_embeddings)\n","        pattern_float = r\"^-?\\d+\\.\\d+$\"\n","        if max(sim[0]) >= 0.35 and (not sp.isnumeric() and not re.match(pattern_float, sp)):\n","            list_titles.append(sp)\n","    \n","\n","    s=word_no\n","    title_counter = Counter(list_titles)\n","    titles_important_word[z] = title_counter+title_counter+title_counter+title_counter\n","    z+=1\n"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["df['title_description'] = df['description']\n","\n","df.loc[:, 'title_description'] = titles_important_word"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["df_temp = df.copy()"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["df['description'] +=df['title_description']"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["df['new_col'] = df['description']+df['business_tags']+ df['category'] + df['niche']"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction import DictVectorizer\n","from scipy.sparse import csr_matrix\n","from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","def get_tfidf_sparse_matrix(col_val):\n","    v = DictVectorizer(sparse=True)\n","    X = v.fit_transform(col_val)\n","    feature_names = v.get_feature_names_out()\n","    X_csr = csr_matrix(X)\n","    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","    tf_idf = tfidf_transformer.fit_transform(X)\n","    return X_csr, feature_names, tf_idf, X\n","X_csr, feature_names, tf_idf, X = get_tfidf_sparse_matrix(df['new_col'])\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["\n","def get_dictionary_for_tfidif(X_csr, feature_names, tf_idf, X):\n","    num_threads = 8\n","    threads = []\n","    size = X_csr.shape[0]//num_threads\n","\n","    global match_words_with_tf_idf_valuess\n","    match_words_with_tf_idf_valuess={}\n","\n","    global match_average_words_with_tf_idf_valuess\n","    match_average_words_with_tf_idf_valuess={}\n","\n","    global match_number_words_with_tf_idf_valuess\n","    match_number_words_with_tf_idf_valuess={}\n","\n","    g=[]\n","    global zzz\n","    zzz = 0\n","\n","    def get_matrix(i, start, end):\n","        for k in range(start, end):\n","            row_tfidf = X[k].toarray()[0]\n","            nonzero_indices = np.nonzero(row_tfidf)[0]\n","            g=0\n","            for j in nonzero_indices:\n","                g+=1\n","                a = round(tf_idf[k,j],5)\n","                \n","                match_words_with_tf_idf_valuess[(feature_names[j], k)] = a\n","                if feature_names[j] not in match_average_words_with_tf_idf_valuess.keys():\n","                    match_average_words_with_tf_idf_valuess[feature_names[j]]=a\n","                    match_number_words_with_tf_idf_valuess[feature_names[j]]=1\n","                else:\n","                    match_average_words_with_tf_idf_valuess[feature_names[j]]+=a\n","                    match_number_words_with_tf_idf_valuess[feature_names[j]]+=1\n","\n","\n","        return match_words_with_tf_idf_valuess\n","\n","    for i in range(num_threads):\n","        start = i * size\n","        end = (i+1) * size if i != num_threads -1 else X_csr.shape[0]\n","        thread = threading.Thread(target = get_matrix, args=(i, start, end))\n","        threads.append(thread)\n","        thread.start()\n","\n","    for thread in threads:\n","        thread.join()\n","\n","get_dictionary_for_tfidif(X_csr, feature_names, tf_idf, X)\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["df2['tags_for_bt'] = df2['business_tags'].copy()\n","for idx, _ in df.iterrows():\n","    list_elements_all = []\n","    determine_tag_all = []\n","    start_from_here = -1\n","    if idx == 0 or (index_group_business_tags[idx]!=0 and index_group_business_tags[idx-1]!=index_group_business_tags[idx]):\n","        start_from_here = 0\n","    else:\n","        start_from_here = index_end_business_tags[idx-1]+1\n","    tok =[element for element in nlp_business_tags[index_group_business_tags[idx]][start_from_here:index_end_business_tags[idx]]]\n","    list_elements = []\n","    determine_tag = []\n","    for i in tok:\n","        if i.lemma_ == \",\" and list_elements!=[]:\n","            list_elements_all.append(list_elements)\n","            determine_tag_all.append(determine_tag)\n","            list_elements = []\n","            determine_tag = []\n","        elif i.lemma_ not in stop_words.STOP_WORDS and i.lemma_ in match_average_words_with_tf_idf_valuess.keys() and ((match_average_words_with_tf_idf_valuess[i.lemma_]/match_number_words_with_tf_idf_valuess[i.lemma_])>0.075 or i.lemma_ in set_sectors_elemenets):\n","            list_elements.append(i.lemma_)\n","            determine_tag.append(i.tag_)\n","    if list_elements != []:\n","        list_elements_all.append(list_elements)\n","        determine_tag_all.append(determine_tag)\n","    \n","    df2.at[idx, 'tags_for_bt'] = determine_tag_all\n","    df2.at[idx, 'new_business_tags_lemmatized'] = list_elements_all"]},{"cell_type":"markdown","metadata":{},"source":["### TF-IDF Filtering for Company Data\n","\n","As we did with the labels, we also apply a **TF-IDF matrix transformation** to the company text. This helps us identify the most informative terms based on their frequency and uniqueness across the dataset."]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["set_sectors = set(list(map(lambda x: x.replace(\" \", \"_\",), list_sectors_elements)))\n","\n","most_used_sector_terms = set()\n","most_used_sector_terms = set()\n","\n","df_temp['description'] = [ (Counter({k:v for k,v in rows['description'].items() if match_words_with_tf_idf_valuess[(k, idx)] >=0.06 and k not in most_used_sector_terms })) for idx, (_, rows) in enumerate(df_temp.iterrows()) ]\n","df_temp['title_description'] = [ (Counter({k:v for k,v in rows['title_description'].items() if match_words_with_tf_idf_valuess[(k, idx)] >=0.06 and k not in most_used_sector_terms })) for idx, (_, rows) in enumerate(df_temp.iterrows()) ]\n","\n","df['description'] = [ (Counter({k:v for k,v in rows['description'].items() if (match_words_with_tf_idf_valuess[(k, idx)]) >=0.04 or  (match_average_words_with_tf_idf_valuess[k]/match_number_words_with_tf_idf_valuess[k])>=0.08 and k not in most_used_sector_terms })) for idx, (_, rows) in enumerate(df.iterrows()) ]\n","df['business_tags'] = [ (Counter({k:v for k,v in rows['business_tags'].items() if (match_words_with_tf_idf_valuess[(k, idx)]) >=0.04 or  (match_average_words_with_tf_idf_valuess[k]/match_number_words_with_tf_idf_valuess[k])>=0.08 and k not in set_sectors_elemenets  })) for idx, (_, rows) in enumerate(df.iterrows()) ]\n","df['category'] = [ (Counter({k:v for k,v in rows['category'].items() if ((match_average_words_with_tf_idf_valuess[k]/match_number_words_with_tf_idf_valuess[k]) >=0.04 and k not in most_used_sector_terms)  or len(rows['category'])==1})) for idx, (_, rows) in enumerate(df.iterrows()) ]\n","df['niche'] = [ (Counter({k:v for k,v in rows['niche'].items() if ((match_average_words_with_tf_idf_valuess[k]/match_number_words_with_tf_idf_valuess[k]) >=0.04 and k not in most_used_sector_terms )or len(rows['niche'])==1 })) for idx, (_, rows) in enumerate(df.iterrows()) ]\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["threshold_each_row=[]\n","z=0\n","for idx, row in df.iterrows():\n","    elements=[]\n","    for i in df['new_col'][idx]:\n","        elements.append((match_words_with_tf_idf_valuess[(i, z)], i))\n","    elements.sort()\n","    max_elements = elements[len(elements)//5-1][0]\n","    threshold_each_row.append(max_elements)\n","    z+=1\n","\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["df['most_important_words'] = [ [k for k in rows['most_important_words'] if k in match_average_words_with_tf_idf_valuess.keys() and match_average_words_with_tf_idf_valuess[k]/match_number_words_with_tf_idf_valuess[k]>=0.05 and k not in most_used_sector_terms ] for idx, (_, rows) in enumerate(df.iterrows())]"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["df_temp['most_important_words'] = [set(df.at[idx, 'most_important_words']) for idx, _ in df.iterrows()]"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["match_average_words_with_tf_idf_valuess2 = match_average_words_with_tf_idf_valuess.copy()\n","match_number_words_with_tf_idf_valuess2 = match_number_words_with_tf_idf_valuess.copy()"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["most_important_words = []\n","for i, (_, row) in enumerate(df2.iterrows()):\n","    if \".\" in row['description']:\n","        elements = row['description']\n","        if len(elements) != elements.index('.')+1 and (elements[elements.index('.')+1]==','):\n","            elements = elements[0:elements[elements.index('.')+1:].index('.')]\n","        else:\n","            elements =elements[0:elements.index('.')]\n","        most_important_words.append(f\"{elements}.\")\n","    else:\n","        most_important_words.append(f\"{row['description']}.\")        "]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["import numpy as np\n","import threading\n","import faiss\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","\n","\n","bert_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\") \n","\n","our_classes_vector_embeddings = bert_model.encode(our_classes_vector)\n","\n","texts_vector_description = []\n","company_texts_description = [\n","    f\"{row['description']}.\"\n","    for idx, row in df2.iterrows()\n","]\n","\n","\n","company_texts_category = [\n","    f\"This company operates in the category: {row['category']}\"\n","    for index, row in df2.iterrows()\n","]\n","\n","\n","company_texts_niche= [\n","    f\"This company operates in the niche: {row['niche']}\"\n","    for index, row in df2.iterrows()\n","]\n","\n","company_embeddings_1 = bert_model.encode(company_texts_description, batch_size=32, normalize_embeddings=True) \n","ce1 = company_embeddings_1\n","faiss.normalize_L2(ce1)\n","\n","\n","company_embeddings_first_sentence = bert_model.encode(most_important_words, batch_size=32, normalize_embeddings=True) \n","fist_sentence_emb = company_embeddings_first_sentence\n","faiss.normalize_L2(fist_sentence_emb)\n","\n","ocve = our_classes_vector_embeddings\n","faiss.normalize_L2(ocve)\n","similarity_matrix = np.dot(ce1, ocve.T)\n","company_embeddings_2 = bert_model.encode(company_texts_category, batch_size=32, normalize_embeddings=True) \n","ce2 = company_embeddings_2\n","faiss.normalize_L2(ce1)\n","similarity_matrix2 =  np.dot(ce1, ce2.T)\n","company_embeddings_3 = bert_model.encode(company_texts_niche, batch_size=32, normalize_embeddings=True) \n","ce31 = company_embeddings_3\n","faiss.normalize_L2(ce31)\n","\n","similarity_matrix3 =  np.dot(ce1, ce31.T)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["set_sectors_elemenets = set(list(map(lambda x: x.replace(\" \", \"_\",).lower(), list_sectors_elements)))\n","set_sector_elements = set(filter(lambda x: x!='' , set_sectors_elemenets))"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["similarity_matrix_first_sentence = np.dot(fist_sentence_emb, ocve.T)\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["similarity_matrix_label2 =  np.dot(ce2, ocve.T)\n","similarity_matrix_label3 =  np.dot(ce31, ocve.T)"]},{"cell_type":"markdown","metadata":{},"source":["### Hybrid Similarity: Token-Level and Contextual Embeddings\n","\n","In addition to applying **embeddings to individual tokens** (which we later compare using **FAISS**), we also use **SentenceTransformer** to capture the **contextual meaning** of full text segments.\n","\n","By combining both:\n","- **Token-level similarity** (fast, granular, useful for concept matching)\n","- **Sentence-level/contextual similarity** (semantic understanding of full text)\n","\n","...we can improve the quality and flexibility of our label matching process.\n","\n","We chose this **hybrid approach** because relying on multiple similarity strategies gives us a **more robust and accurate classification**, especially in ambiguous or fuzzy cases."]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["import faiss\n","MAX_TOKENS = max(df['new_col'].apply(len))\n","global rows_\n","\n","def get_sentence_embeddings_for_categories(word_list):\n","    embeddings_ = []\n","    new_list = [word for word in word_list]\n","    \n","    embeddings_ = [\n","        (embeddings_index.get(word, np.zeros(300)\n","    )) for word in new_list]\n","\n","    if (word_list == Counter()):\n","        return [np.zeros(300) for _ in range(150)]\n","    \n","    if (len(embeddings_)) < 150:\n","        for _ in range((150-len(word_list))):\n","            embeddings_.append(np.zeros(300))\n","    \n","    return embeddings_\n","\n","def get_sentence_embeddings_for_categories2(word_list):\n","    embeddings_ = []\n","    new_list = [word for word in word_list]\n","    embeddings_ = [\n","        np.mean([embeddings_index.get(wd, np.zeros(300)\n","    ) for wd in word], axis=0) for word in new_list]\n","\n","    if len(embeddings_) < 70:\n","        for _ in range((70-len(embeddings_))):\n","            embeddings_.append(np.zeros(300) for _ in range(150))\n","    return embeddings_\n","description_embeddings2 = np.array(df.apply(lambda x: get_sentence_embeddings_for_categories(x['description']), axis=1).to_list())\n","niche_embeddings2 = np.array(df.apply(lambda x: get_sentence_embeddings_for_categories(x['niche']), axis=1).to_list())\n","category_embeddings2 = np.array(df.apply(lambda x: get_sentence_embeddings_for_categories(x['category']), axis=1).to_list())\n","business_tags_embeddings2 = np.array(df.apply(lambda x: get_sentence_embeddings_for_categories(x['business_tags']), axis=1).to_list())\n","title_embeddings2 = np.array(df_temp.apply(lambda x: get_sentence_embeddings_for_categories(x['title_description']), axis=1).to_list())\n","\n","description_embeddings_original = np.array(df_temp.apply(lambda x: get_sentence_embeddings_for_categories(x['description']), axis=1).to_list())\n","most_important_words_embeddings = np.array(df_temp.apply(lambda x: get_sentence_embeddings_for_categories(x['most_important_words']), axis=1).to_list())\n"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["def get_sentence_embeddings_for_categories2(word_list):\n","    embeddings_ = []\n","    new_list = [word for word in word_list]\n","    if new_list == [[]]:\n","        for _ in range(100):\n","            embeddings_.append(np.zeros(300))\n","        return embeddings_\n","    embeddings_ = [\n","        np.mean([embeddings_index.get(wd.replace(\"-\",\"_\"), np.zeros(300)\n","    ) for wd  in word], axis=0) for word in new_list]\n","    or_lst = []\n","    \n","    \n","    if len(embeddings_) < 100:\n","        for _ in range((100-len(embeddings_))):\n","            embeddings_.append(np.zeros(300))\n","    \n","\n","    for i in range(100):\n","        or_lst.append(len(embeddings_[i]))\n","    \n","    return embeddings_\n","\n","business_tags_embeddings3_lemmatized = np.array(df2.apply(lambda x: get_sentence_embeddings_for_categories2(x['new_business_tags_lemmatized']), axis=1).to_list())"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["def get_sentence_embeddings_for_categories22(word_list):\n","    embeddings_ = []\n","    new_list = [word for word in word_list]\n","    if new_list == [[]]:\n","        for _ in range(100):\n","            embeddings_.append(np.zeros(300))\n","        return embeddings_\n","    embeddings_ = [\n","        np.array([embeddings_index.get(wd.replace(\"-\",\"_\"), np.zeros(300)\n","    ) for wd  in word]) for word in new_list]\n","\n","\n","    embeddings_ = []\n","   \n","   \n","    for word in new_list:\n","\n","        emb_curr = []\n","        for wd in word:\n","            emb_curr.append(embeddings_index.get(wd.replace(\"-\",\"_\"), np.zeros(300)))\n","        for _ in range(10 - len(emb_curr)):\n","            emb_curr.append(np.zeros(300))\n","        embeddings_.append(np.array(emb_curr))\n","\n","    \n","    \n","    if len(embeddings_) < 65:\n","        for _ in range((65-len(embeddings_))):\n","            emb_curr = []\n","            for __ in range(10):\n","                emb_curr.append(np.zeros(300))\n","            embeddings_.append(np.array(emb_curr))\n","\n","\n","    return embeddings_\n","business_tags_embeddings_full_lemmatized = np.array(df2.apply(lambda x: get_sentence_embeddings_for_categories22(x['new_business_tags_lemmatized']), axis=1).to_list())"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["df_temp['business_tags'] = df['business_tags'].copy()"]},{"cell_type":"markdown","metadata":{},"source":["### Finding Related Words and Handling Context\n","\n","This part builds relationships between words to improve the matching and extraction of meaningful business tags.\n","\n","### Main Steps\n","\n","- **Load ConceptNet-based Relationships**:\n","  - Read relations like `IsA`, `HasContext`, and plural forms.\n","  - Identify very generic terms based on occurrence and WordNet specificity.\n","  \n","- **Expand Related Terms**:\n","  - Use `IsA` and reverse relationships to find similar or related words.\n","  - Calculate cosine similarity between embeddings to filter relevant related terms (threshold ≥ 0.4).\n","\n","- **Contextual Matching**:\n","  - Functions like `generate_list_context` look for contextual relationships using the `HasContext` graph.\n","  - If direct relations are missing, it falls back to checking noun forms or plural-to-singular mappings.\n","\n","- **Finding New Related Words**:\n","  - During matching, if a candidate word does not match strongly enough, nearby related words or contextually related terms are checked.\n","  - Adds correlated terms if sufficient similarity is found through embeddings and context relationships.\n","\n","### Goal\n","\n","To improve matching between business tags and descriptions by leveraging external relationship graphs, embeddings, and contextual knowledge — even when direct matches are weak."]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["import csv\n","import time\n","antonyms_words = set()\n","isA_relationship = {}\n","isA_reverse_relationship = {}\n","\n","hasContext_relationship = {}\n","hasContext_reverse_relationship = {}\n","\n","\n","related_terms_for_antonyms = {}\n","convertPluralToSingular = {}\n","occuranceTermForParent = {}\n","antonyms_words,isA_relationship, isA_reverse_relationship, occuranceTermForParent, convertPluralToSingular, hasContext_relationship, hasContext_reverse_relationship, embeddings_index = open_filtered_assertions_file(antonyms_words,isA_relationship, isA_reverse_relationship, occuranceTermForParent, convertPluralToSingular, hasContext_relationship, hasContext_reverse_relationship, embeddings_index)      "]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["from collections import deque \n","\n","def bfs(dict_terms, start):\n","    visited = []\n","    queue = deque([(start, 0)])\n","\n","    while queue:\n","        node = queue.popleft()\n","        if node[0] not in visited:\n","\n","            visited.append(node[0])\n","        \n","            if node[0] not in dict_terms.keys():\n","                return node[1]                \n","            for neighbor in dict_terms[node[0]]:\n","                if neighbor not in visited:\n","                    queue.append((neighbor, node[1]+1)) \n","    return 0"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["original_terms_for_parent = occuranceTermForParent.copy()\n","occuranceTermForParent = dict(sorted(occuranceTermForParent.items(), key=lambda item: item[1]))\n","occuranceTermForParent = {k: v for k, v in occuranceTermForParent.items() if (v>=70 and  bfs(isA_relationship,k)<=1) or (v >=450 and bfs(isA_relationship,k)<=2) }\n","very_generic_terms = set([k for k, _ in occuranceTermForParent.items()])"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["def get_specificity(term):\n","    synsets = wn.synsets(term)\n","    if not synsets:\n","        return 0\n","\n","    return max([len(path) for path in synsets[0].hypernym_paths()])"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["terms_for_look_for_beginning = {\"prepared\", \"shaped\", \"solid\", \"liquid\", \"long\", \"light\", \"dirty\", \"busy\", \"organized\",\"constructed\", \"processed\"}\n","terms_for_look_for_beginning = {\"prepared\", \"shaped\", \"processed\"}\n","terms_for_look_for_end = {\"object\", \"thing\", \"fluid\", \"thing\", \"matter\"}\n","for original_term in original_terms_for_parent.keys():\n","    \n","    if ((original_term in occuranceTermForParent and occuranceTermForParent[original_term]>4) or (original_term not in occuranceTermForParent.keys())) and len(original_term.split(\"_\")) > 1 and get_specificity(original_term)<4:\n","        is_generic_term = original_term.split(\"_\")[0] in terms_for_look_for_beginning or (original_term.split(\"_\")[-1] in terms_for_look_for_end and get_specificity(\"_\".join(original_term.split(\"_\")[:-1])) >1 and get_specificity(\"_\".join(original_term.split(\"_\")[:-1]))<5) \n","\n","        if is_generic_term:\n","            very_generic_terms.add(original_term)\n"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["related_words_to_a_word_similarity = {}\n","\n","for word in isA_relationship:\n","    set_element = isA_relationship[word]\n","    set_element_temp = set_element.copy()\n","    \n","    for word2 in set_element:\n","        set_is_A_relationship = {}\n","        set_is_A_reverse_relationship = {}\n","\n","        if word2 in isA_relationship.keys():\n","            set_is_A_relationship = isA_relationship[word2]\n","        if word2 in isA_reverse_relationship.keys()  and word2 not in very_generic_terms:\n","            set_is_A_reverse_relationship = isA_reverse_relationship[word2]\n","        set_element_temp.update(set_is_A_relationship)\n","\n","        set_element_temp.update(set_is_A_reverse_relationship)\n","\n","        if word in set_element_temp:\n","            set_element_temp.remove(word)\n","\n","    if set_element_temp !=set():\n","\n","        list_words = list(set_element_temp)\n","        current_word_embedding = (embeddings_index[word]).reshape(1,-1)\n","        faiss.normalize_L2(current_word_embedding)\n","        all_word_embeddings = np.array([embeddings_index[wd] if wd in embeddings_index else np.zeros(300) for wd in list_words]).astype(np.float32, order='C')\n","        faiss.normalize_L2(all_word_embeddings)\n","        our_values = np.dot(current_word_embedding, all_word_embeddings.T)[0]\n","        list_words_zip_for_antonyms = list(filter(lambda x: x[1]>=0.4, list(zip(list_words, our_values))))\n","        set_element_temp = set(map(lambda x: x[0], list_words_zip_for_antonyms))\n","\n","    if word not in related_words_to_a_word_similarity.keys():\n","        related_words_to_a_word_similarity[word] = set_element_temp\n","    else:\n","        set_word_sim = related_words_to_a_word_similarity[word] \n","\n","        set_word_sim.update(set_element_temp)\n","        related_words_to_a_word_similarity[word] = set_word_sim\n","\n","for value_word in related_words_to_a_word_similarity.keys():\n","    list_value_word = related_words_to_a_word_similarity[value_word]\n","    if value_word in isA_reverse_relationship.keys():\n","        for wd in isA_reverse_relationship[value_word]:\n","            list_value_word = set(filter(lambda x: x if wd not in x else '', related_words_to_a_word_similarity[value_word]))\n","            related_words_to_a_word_similarity[value_word] = list_value_word\n","\n"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["def go_further(word_key_label, word):\n","    if word not in related_words_to_a_word_similarity.keys():\n","        return []\n","    return list(filter(lambda x: f\"_{word_key_label}_\" in x or word_key_label == x or f\"{word_key_label}_\" == x[0:(len(word_key_label)+1)] or f\"_{word_key_label}\" in x, related_words_to_a_word_similarity[word]))"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["def words_related_to_key_words(word_key_label, wd):\n","    if (len(wd)==1):\n","        return set()\n","    modify_word  = wd\n","  \n","    \n","    filtered_similarity_list = set()\n","    \n","    if modify_word in related_words_to_a_word_similarity:\n","        filtered_similarity_list = set(filter(lambda x: f\"_{word_key_label}_\" in x or word_key_label == x or f\"{word_key_label}_\" == x[0:(len(word_key_label)+1)] or f\"_{word_key_label}\" in x or go_further(word_key_label, x), related_words_to_a_word_similarity[modify_word]))\n","    if filtered_similarity_list == set() and word_key_label in convertPluralToSingular.keys() and len(convertPluralToSingular[word_key_label].split(\"_\"))>1 and modify_word in related_words_to_a_word_similarity:\n","        word_key_label_new = convertPluralToSingular[word_key_label].split(\"_\")[0]\n","        filtered_similarity_list = set(filter(lambda x: f\"_{word_key_label_new}_\" in x or word_key_label_new == x or f\"{word_key_label_new}_\" == x[0:(len(word_key_label_new)+1)] or f\"_{word_key_label_new}\" in x or go_further(word_key_label_new,x), related_words_to_a_word_similarity[modify_word]))\n","    return filtered_similarity_list\n"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["def is_list_relevant(similar_items, word2, embeddings_index):\n","    if similar_items != list():\n","        similar_items_embeddings = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in similar_items],  dtype= np.float32, order='C')\n","        faiss.normalize_L2(similar_items_embeddings)\n","        values = np.dot(similar_items_embeddings, embeddings_index[word2].T)\n","        range_items = list(filter(lambda x: values[x]>=0.35, range(len(similar_items))))\n","        similar_items_temp = similar_items.copy()\n","        similar_items = [similar_items_temp[i] for i in range_items]\n","    return similar_items"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["def generate_list_context(word1, word2, hasContext_reverse_relationship, embeddings_index, idx=None):\n","    similar_items = []\n","    if word2 in hasContext_reverse_relationship.keys():\n","        similar_items = list(filter(lambda x: f\"_{word1}_\" in x or word1 == x or f\"{word1}_\" == x[0:(len(word1))+1] or f\"_{word1}\" in x, hasContext_reverse_relationship[word2]))\n","        similar_items = is_list_relevant(similar_items, word2, embeddings_index)\n","    noun_for_adj = generate_noun_for_adj(word2, embeddings_index)\n","    \n","    if similar_items == list() and noun_for_adj in hasContext_reverse_relationship.keys():\n","        similar_items = list(filter(lambda x: f\"_{word1}_\" in x or word1 == x or f\"{word1}_\" == x[0:(len(word1)+1)] or f\"_{word1}\" in x, hasContext_reverse_relationship[noun_for_adj]))\n","        similar_items = is_list_relevant(similar_items, word2, embeddings_index)\n","\n","    return similar_items"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["\n","def find_new_related_word(df2, word_description, bt3_emb_full, ind, wd_e, match_words_with_tf_idf_valuess, i, currated_list, hasContext_reverse_relationship, embeddings_index, idx):\n","    our_values = np.dot(bt3_emb_full[ind,:,:], wd_e.T)\n","    maximum_value = np.max(our_values)\n","    global correlated_terms\n","    \n","    if maximum_value < 0.1:\n","        return currated_list\n","        \n","    \n","    for idx2, word in enumerate(df2['new_business_tags_lemmatized'][idx][ind]):\n","        max_value_current = np.max(our_values[idx2])\n","        list_converted = list(our_values[idx2])\n","\n","\n","        index_val = list_converted.index(max_value_current)\n","\n","\n","\n","\n","        \n","        if len(word) == 1  or ((word, i) in match_words_with_tf_idf_valuess.keys() and match_words_with_tf_idf_valuess[(word, i)] <= 0.1) or ((word[:-1], i) in match_words_with_tf_idf_valuess.keys() and match_words_with_tf_idf_valuess[(word[:-1], i)] <= 0.1):\n","            continue\n","        \n","        if max_value_current > 0.5:\n","            currated_list.append([word])\n","            continue\n","        word1 = word\n","        word2 = word_description[index_val]\n","\n","        similar_items = generate_list_context(word1, word2, hasContext_reverse_relationship, embeddings_index, idx)\n","\n","        if similar_items != list():\n","            currated_list.append([word])\n","            correlated_terms.add((word, word_description[index_val]))\n","            correlated_terms.add((word_description[index_val], word))\n","\n","            continue\n","\n","        word2 = word\n","        word1 = word_description[index_val]\n","\n","        similar_items = generate_list_context(word1, word2, hasContext_reverse_relationship, embeddings_index, idx)\n","        \n","\n","        if similar_items != list():\n","            currated_list.append([word])\n","            correlated_terms.add((word, word_description[index_val]))\n","            correlated_terms.add((word_description[index_val], word))\n","            continue\n","        \n","        \n","        list_set_to_consider = list(words_related_to_key_words(word, word_description[index_val]))\n","        list_set_to_consider = is_list_relevant(list_set_to_consider, word_description[index_val], embeddings_index)\n","\n","        if (list_set_to_consider == list() and len(word_description[index_val].split(\"_\"))>1):\n","            list_splitted_words = word_description[index_val].split(\"_\")\n","            list_splitted_emb = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in list_splitted_words],  dtype= np.float32, order='C')\n","            \n","            faiss.normalize_L2(list_splitted_emb)\n","            word1_embeddings = embeddings_index.get(wd, np.zeros(300))\n","            \n","            our_valuezzz = np.dot(list_splitted_emb, word1_embeddings.T)\n","\n","\n","            max_value_current = np.max(our_valuezzz)\n","            list_converted = list(our_valuezzz)\n","            index_val_cur = list_converted.index(max_value_current)\n","\n","            list_set_to_consider = list(words_related_to_key_words(word, word_description[index_val].split(\"_\")[index_val_cur]))\n","            list_set_to_consider = is_list_relevant(list_set_to_consider, word_description[index_val].split(\"_\")[index_val_cur], embeddings_index)\n","\n","            if(list_set_to_consider != list()):\n","                currated_list.append([word])\n","                correlated_terms.add((word, word_description[index_val].split(\"_\")[index_val_cur]))\n","                correlated_terms.add((word_description[index_val].split(\"_\")[index_val_cur], word))\n","                continue\n","        elif list_set_to_consider != list():\n","            currated_list.append([word])\n","            correlated_terms.add((word, word_description[index_val]))\n","            correlated_terms.add((word_description[index_val], word))\n","            continue\n","    \n","    return currated_list"]},{"cell_type":"markdown","metadata":{},"source":["### Refining Business Tags Based on Description and Context\n","\n","This part further refines the `business_tags` for each company by comparing them to the description and niche information using embeddings and relationship graphs.\n","\n","### Main Steps\n","\n","- **Embedding Similarity**:\n","  - Compare business tag embeddings with description embeddings.\n","  - If similarity is too low (< 0.50), attempt to validate or replace tags using contextual relationships and related terms.\n","\n","- **Handling Short Descriptions**:\n","  - If the description is short (few words), instead of comparing each tag individually, we compare the **whole list** of `business_tags` embeddings together.\n","  - `convertPluralToSingular` is used here to convert tags into alternative forms that may exist in the embeddings index, improving matching reliability.\n","\n","- **Context and Related Words**:\n","  - Use `HasContext` and `IsA` graphs to search for related or supportive words when direct similarity fails.\n","  - Split compound words when necessary to find partial matches.\n","\n","- **Fallback to Niche**:\n","  - If matching with the description fails, fallback to matching against the company's niche.\n","  - Companies requiring this fallback are tracked in `do_exception_for_niche_for_these_companies`.\n","\n","- **Final Cleaning**:\n","  - Correct spelling variations (e.g., `er` → `re`, `ou` → `o`).\n","  - Filter out stopwords, punctuation, and irrelevant terms.\n","  - Update the `business_tags` with cleaned words, bigrams, and trigrams.\n","\n","- **Correlations Tracking**:\n","  - Contextually matched pairs are stored in a global `correlated_terms` set for future use.\n","\n","### Goal\n","\n","Ensure that each company has accurate and meaningful business tags, even for short or weak descriptions, by using fallback logic, relationship graphs, and smart term corrections."]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["keys_sector_elements = {}\n","for elements in set_sector_elements:\n","    keys_sector_elements[elements] = [0]"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["which_terms_are_associated_with_value = {}\n","\n","def find_values_that_may_be_related(word, lemmatized_business_tags, bt3_emb, wd_e, idx):\n","    no_values = 0\n","    global which_terms_are_associated_with_value\n","    values_emb = np.dot(bt3_emb, wd_e.T)\n","    max_value = df.at[idx, 'sector']!='' and np.max(np.dot(embeddings_index[word], embeddings_index[df.at[idx, 'sector'].replace(\" \",\"_\").lower()].T))\n","    for i in range(len(lemmatized_business_tags)):\n","\n","\n","       \n","\n","        max_our_values = np.max(values_emb[i])\n","        element = keys_sector_elements.get(word, None)\n","        okay = word in lemmatized_business_tags[i]\n","        \n","        if okay and (max_our_values>0.4 or ((element != None or max_value > 0.6) and max_our_values>0.25)):\n","            \n","         \n","            word_index = lemmatized_business_tags[i].index(word)\n","            if word_index >= len(df2.at[idx,'tags_for_bt'][i]):\n","                continue\n","            find_tag = df2.at[idx,'tags_for_bt'][i][word_index]\n","            if find_tag == \"JJ\" or find_tag == \"JJR\" or find_tag == \"JJS\" or element != None or max_value > 0.6:\n","                if (word, idx) not in which_terms_are_associated_with_value.keys():\n","                    list_val = lemmatized_business_tags[i].copy()\n","                    list_val.remove(word)\n","                    which_terms_are_associated_with_value[(word, idx)]  = list_val\n","                else:\n","                    list_val = which_terms_are_associated_with_value[(word, idx)]\n","                    list_val.extend(lemmatized_business_tags[i].copy())\n","                    list_val.remove(word)\n","                    which_terms_are_associated_with_value[(word, idx)] = list(set(list_val))\n","                no_values += 1\n","    \n","    return no_values"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["i=0\n","ok3=0\n","correlated_terms = set()\n","do_exception_for_niche_for_these_companies = set()\n","for idx, rows in df.iterrows():\n","    de2 = np.array(description_embeddings_original[i], dtype= np.float32, order='C')\n","    faiss.normalize_L2(de2)\n","    bte2 = np.array(business_tags_embeddings2[i], dtype= np.float32, order='C')\n","    faiss.normalize_L2(bte2)\n","\n","\n","\n","    val_business_tags = np.einsum('jk,lk->jl', bte2, de2)\n","    list_business_tags = [bt for bt in rows['business_tags']]\n","    copy_df_business = df['business_tags'][idx].copy()\n","\n","    our_list = [word for word in df['business_tags'][idx]]\n","    our_list_original = [word for word in df_temp['description'][idx]]\n","\n","    maximum_value_list = []\n","    currated_list = []\n","    \n","    if len(df_temp['description'][idx])>3:\n","        bt3_emb_lemma_=  np.array(business_tags_embeddings3_lemmatized[i], dtype= np.float32, order='C')\n","        wd_e = np.array([embeddings_index[wd] for wd in df['description'][idx]], dtype= np.float32, order='C')\n","\n","\n","\n","        \n","        for j in range(len(df['business_tags'][idx])):\n","\n","    \n","        \n","            if (max(val_business_tags[j]) < 0.50):\n","                index_val_single = list(val_business_tags[j]).index(np.max(val_business_tags[j]))\n","\n","             \n","\n","                if np.max(val_business_tags[j]) == 0:\n","                    del copy_df_business[list_business_tags[j]]\n","                    continue\n","\n","\n","                \n","                word1 = list_business_tags[j]\n","                word2 = our_list_original[index_val_single]\n","\n","                similar_items = generate_list_context(word1, word2, hasContext_reverse_relationship, embeddings_index)\n","    \n","                if similar_items != list():\n","                    correlated_terms.add((word1, word2))\n","                    correlated_terms.add((word2, word1))\n","                    continue\n","                word2 = list_business_tags[j]\n","                word1 = our_list_original[index_val_single]\n","\n","                similar_items = generate_list_context(word1, word2, hasContext_reverse_relationship, embeddings_index)\n","                \n","        \n","                if similar_items != list():\n","                    correlated_terms.add((word1, word2))\n","                    correlated_terms.add((word2, word1))\n","                    continue\n","                list_set_to_consider = list(words_related_to_key_words(list_business_tags[j], our_list_original[index_val_single]))\n","                list_set_to_consider = is_list_relevant(list_set_to_consider, our_list_original[index_val_single], embeddings_index)\n","\n","                if (list_set_to_consider == list() and len(our_list_original[index_val_single].split(\"_\"))>1):\n","                    list_splitted_words1 = our_list_original[index_val_single].split(\"_\")\n","                    list_splitted_emb1 = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in list_splitted_words1],  dtype= np.float32, order='C')\n","                    faiss.normalize_L2(list_splitted_emb1)\n","                    word1_embeddings = embeddings_index.get(wd, np.zeros(300))\n","                    \n","                    our_valuezzz = np.dot(list_splitted_emb1, word1_embeddings.T)\n","\n","\n","                    max_value_current = np.max(our_valuezzz)\n","                    list_converted = list(our_valuezzz)\n","                    index_val_current = list_converted.index(max_value_current)\n","\n","                    list_set_to_consider = list(words_related_to_key_words(list_business_tags[j], our_list_original[index_val_single].split(\"_\")[index_val_current]))\n","                    list_set_to_consider = is_list_relevant(list_set_to_consider, our_list_original[index_val_single].split(\"_\")[index_val_current], embeddings_index)\n","                   \n","                    if(list_set_to_consider == list()):\n","                        val = find_values_that_may_be_related(list_business_tags[j], df2['new_business_tags_lemmatized'][idx], bt3_emb_lemma_, wd_e, idx)\n","\n","                        if val == 0:\n","                            del copy_df_business[list_business_tags[j]]\n","                        else:\n","                            copy_df_business[list_business_tags[j]] = val\n","                    else:\n","                        correlated_terms.add((list_business_tags[j], our_list_original[index_val_single].split(\"_\")[index_val_current]))\n","                        correlated_terms.add((our_list_original[index_val_single].split(\"_\")[index_val_current], list_business_tags[j]))\n","                elif list_set_to_consider == list():\n","                    val = find_values_that_may_be_related(list_business_tags[j], df2['new_business_tags_lemmatized'][idx], bt3_emb_lemma_, wd_e, idx)\n","                    if val == 0:\n","                        del copy_df_business[list_business_tags[j]]\n","                    else:\n","                        copy_df_business[list_business_tags[j]] = val\n","                else:\n","                    correlated_terms.add((list_business_tags[j], our_list_original[index_val_single]))\n","                    correlated_terms.add((our_list_original[index_val_single], list_business_tags[j]))\n","\n","                \n","                \n","            \n","\n","            if len(copy_df_business) <= 1:\n","                if (max(val_business_tags[j]) < 0.50):\n","                    del copy_df_business[list_business_tags[j]]\n","            \n","        df.at[idx, 'business_tags'] = copy_df_business\n","\n","\n","    elif len(df_temp['description'][idx]) <=3 and df_temp['business_tags'][idx]!=Counter():\n","\n","            wd_e = np.array([embeddings_index[wd] for wd in df['description'][idx]], dtype= np.float32, order='C')\n","            bt_emb =  np.array([embeddings_index[wd] for wd in df['business_tags'][idx]], dtype= np.float32, order='C')\n","            bt3_emb =  np.array(business_tags_embeddings3_lemmatized[i], dtype= np.float32, order='C')\n","            bt3_emb_full =  np.array(business_tags_embeddings_full_lemmatized[i], dtype= np.float32, order='C')\n","\n","\n","            \n","            faiss.normalize_L2(bt_emb)\n","            faiss.normalize_L2(bt3_emb)\n","\n","            currated_list = []\n","            word_list_new = [wd for wd in df2['new_business_tags_lemmatized'][idx]]\n","\n","            if df['description'][idx] != Counter():\n","                faiss.normalize_L2(wd_e)\n","                word_description = [wd for wd in df['description'][idx]]\n","                list_a = []\n","                for ind in range(len(df2['new_business_tags_lemmatized'][idx])):\n","                    a= max(np.dot(bt3_emb, wd_e.T)[ind])\n","\n","                    maximum_value_list.append(a)\n","\n","                    if (a> 0.35 and len(word_list_new[ind])!=1) or a> 0.5:\n","                        currated_list.append(word_list_new[ind])\n","                    else:\n","                        currated_list3 = currated_list.copy()\n","                        currated_list = find_new_related_word( df2, word_description, bt3_emb_full, ind, wd_e, match_words_with_tf_idf_valuess, i, currated_list3, hasContext_reverse_relationship, embeddings_index, idx)\n","                                                        \n","                    list_a.append(a)\n","\n","            if currated_list == list():\n","                ne2 = np.array(niche_embeddings2[i], dtype= np.float32, order='C')\n","                faiss.normalize_L2(ne2)\n","                val_business_tags = np.einsum('jk,lk->jl', bte2, ne2)\n","                word_business_tags = [wd for wd in df['business_tags'][idx]]\n","                word_niche = [wd for wd in df_temp['niche'][idx]]\n","\n","                n_e = np.array([embeddings_index[wd] for wd in df_temp['niche'][idx]], dtype= np.float32, order='C')\n","\n","                for ind in range(len(df2['new_business_tags_lemmatized'][idx])):\n","                    a= max(np.dot(bt3_emb, ne2.T)[ind])\n","\n","                    maximum_value_list.append(a)\n","\n","                    if (a> 0.35 and len(word_list_new[ind])!=1) or a> 0.5:\n","                        currated_list.append(word_list_new[ind])\n","                    for ind in range(len(df2['new_business_tags_lemmatized'][idx])):\n","                        a= max(np.dot(bt3_emb, n_e.T)[ind])\n","\n","                        maximum_value_list.append(a)\n","\n","                        if (a> 0.35 and len(word_list_new[ind])!=1) or a> 0.5:\n","                            currated_list.append(word_list_new[ind])\n","                        else:\n","                            currated_list3 = currated_list.copy()\n","                            currated_list = find_new_related_word( df2, word_niche, bt3_emb_full, ind, n_e, match_words_with_tf_idf_valuess, i, currated_list3, hasContext_reverse_relationship, embeddings_index, idx)\n","                if(currated_list==list()):\n","                    do_exception_for_niche_for_these_companies.add(idx)\n","\n","            word_list = [wd for wd in df['business_tags'][idx]]\n","            currated_list = sum(currated_list, [])\n","            currated_list = list(map(lambda x: replace_upper_word([x], acronym_dict)[0], currated_list))\n","            for idx1, wd in enumerate(currated_list):\n","\n","                if wd not in embeddings_index.keys() and wd in convertPluralToSingular and convertPluralToSingular[wd] in embeddings_index.keys():\n","                    currated_list[idx1] =convertPluralToSingular[wd]\n","                elif wd.replace(\"er\", \"re\") in df.at[idx, 'description'] and wd.replace(\"er\",\"re\")!=wd and cosine_similarity(embeddings_index[wd.replace(\"er\",\"re\")].reshape(1,-1), embeddings_index[wd].reshape(1,-1))>0.85:\n","                    currated_list[idx1] = wd.replace(\"er\", \"re\")\n","                elif wd.replace(\"ou\", \"o\") in  df.at[idx, 'description'] and wd.replace(\"ou\",\"o\")!=wd and cosine_similarity(embeddings_index[wd.replace(\"ou\",\"o\")].reshape(1,-1), embeddings_index[wd].reshape(1,-1))>0.85:\n","                    currated_list[idx1] = wd.replace(\"ou\", \"o\")\n","                elif wd in convertPluralToSingular and convertPluralToSingular[wd] in  df.at[idx, 'description'] and cosine_similarity(embeddings_index[convertPluralToSingular[wd]].reshape(1,-1), embeddings_index[wd].reshape(1,-1))>0.75:\n","                    currated_list[idx1] =convertPluralToSingular[wd]\n","            \n","            tokens = list(filter(lambda x: x if x.lower() not in stop_words.STOP_WORDS and x.lower() in embeddings_index.keys() and x.lower() not in string.punctuation and  (col!=\"description\" or x.lower() not in is_person_or_location_or_irrevalent_terms2) and (not x[0].isnumeric() and not re.match(pattern_float, x[0])) else '', currated_list))\n","\n","            tokens = list(filter(lambda x: x.lower() not in is_person_or_location_or_irrevalent_terms2, tokens)) \n","            \n","            if tokens != [] or len(df['description'][idx])>3:\n","                df.at[idx, 'business_tags'] = Counter(tokens)  \n","                df.at[idx, 'business_tags'].update(new_ngrams(tokens, 2))\n","                df.at[idx, 'business_tags'].update(new_ngrams(tokens, 3))\n","                \n","                            \n","\n","    i+=1\n","\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["copy_business_tags = df['business_tags'].copy()"]},{"cell_type":"markdown","metadata":{},"source":["### Standardizing and Filtering Business Tags with TF-IDF Scores\n","\n","This section further refines `business_tags` by correcting word forms and validating tags based on TF-IDF scores.\n","\n","### Main Steps\n","\n","- **Word Form Correction**:\n","  - Automatically adjusts common word variations like:\n","    - Removing suffixes (`-ing`, `-ies`, `-es`, `-s`, `-ed`, `-ly`) when appropriate.\n","    - Replaces the original word with its corrected form if it exists in `embeddings_index` and has a valid TF-IDF score.\n","  - Falls back to **lemmatization** if no direct match is found.\n","\n","- **TF-IDF Filtering**:\n","  - Each business tag is validated against `match_words_with_tf_idf_valuess`.\n","  - Tags without a valid TF-IDF score are removed from the business tags list.\n","\n","- **Tracking TF-IDF Extremes**:\n","  - For each company, the minimum and maximum TF-IDF values among their business tags are stored in `min_val_list` and `max_val_list`.\n","\n","- **Fallback for Plural Forms**:\n","  - If a tag's TF-IDF value is missing, attempts are made to look it up through the `convertPluralToSingular` mapping.\n","\n","### Goal\n","\n","Ensure business tags are clean, normalized, and strongly relevant according to TF-IDF importance, improving the quality of downstream text analysis."]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["min_val_list = [2] * len(df['business_tags'])\n","max_val_list = [-2] * len(df['business_tags'])\n","z=0\n","for idx, row in df.iterrows():\n","    min_val = 2\n","    max_val = -2\n","    copy_df_business = df.at[idx, 'business_tags'].copy()\n","    row_new = [wd for wd in df.at[idx, 'business_tags'].most_common()]\n","    for i in row_new:\n","        if (i[0],z) not in match_words_with_tf_idf_valuess.keys():\n","           \n","            \n","            if(len(i[0])>3 and i[0][-3:]==\"ing\") and  i[0][:-3] in embeddings_index.keys() and (i[0][:-3], z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-3]]))\n","                del copy_df_business[i[0]]\n","            elif(len(i[0])>4 and i[0][-3:]==\"ing\") and  i[0][:-3] in embeddings_index.keys() and (i[0][:-4], z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-3]]))\n","                del copy_df_business[i[0]]\n","            elif(len(i[0])>3 and i[0][-3:]==\"ies\") and  i[0][:-3]+\"y\" in embeddings_index.keys()  and (i[0][:-3]+\"y\", z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-3]+\"y\"]))\n","                del copy_df_business[i[0]]\n","            elif(len(i[0])>2 and i[0][-3:]==\"es\") and  i[0][:-2] in embeddings_index.keys()  and (i[0][:-2], z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-2]]))\n","                del copy_df_business[i[0]]\n","            \n","            elif(len(i[0])>1 and i[0][-1:]==\"s\" and i[0][:-1] in embeddings_index.keys()) and (i[0][:-1], z) in match_words_with_tf_idf_valuess.keys():\n","             \n","                copy_df_business.update(Counter([i[0][:-1]]))\n","                del copy_df_business[i[0]]\n","            elif(len(i[0])>2 and i[0][-2:]==\"ed\" and i[0][:-1] in embeddings_index.keys()) and (i[0][:-1], z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-1]]))\n","                del copy_df_business[i[0]]\n","            elif(len(i[0])>2 and i[0][-2:]==\"ly\" and i[0][:-2] in embeddings_index.keys()) and (i[0][:-2], z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-2]]))\n","                del copy_df_business[i[0]]\n","            elif(len(i[0])>3 and i[0][-2:]==\"ly\" and i[0][:-3] in embeddings_index.keys()) and (i[0][:-3], z) in match_words_with_tf_idf_valuess.keys():\n","                copy_df_business.update(Counter([i[0][:-3]]))\n","                del copy_df_business[i[0]]\n","            else:\n","\n","                doc = nlp(i[0])\n","                text_element = doc[0].lemma_\n","                if (text_element, z) in match_words_with_tf_idf_valuess.keys() and text_element in embeddings_index.keys():\n","                    copy_df_business.update(Counter([text_element]))\n","            \n","                del copy_df_business[i[0]]\n","    \n","    df.at[idx, 'business_tags'] = copy_df_business\n","    row_new = [wd for wd in df.at[idx, 'business_tags'].most_common()]\n","    for i in row_new:   \n","        value = None\n","        \n","        \n","        if (i[0], z) in match_words_with_tf_idf_valuess.keys():\n","            value = match_words_with_tf_idf_valuess[(i[0], z)]\n","        else:\n","            del copy_df_business[i[0]]\n","            continue\n","\n","        \n","        if min_val > value:\n","            min_val = value\n","\n","\n","        if max_val < value:\n","            max_val = value\n","    \n","    df.at[idx, 'business_tags'] = copy_df_business\n","    row_new = [wd for wd in df.at[idx, 'business_tags'].most_common()]\n","\n","\n","    for i in row_new:\n","    \n","            \n","        value = None\n","        if (i[0], z) in match_words_with_tf_idf_valuess.keys():\n","            value = match_words_with_tf_idf_valuess[(i[0], z)]\n","        else:\n","            value = match_words_with_tf_idf_valuess[(convertPluralToSingular[i[0]], z)]\n","\n","        if min_val > value:\n","            min_val = value\n","        if max_val < value:\n","            max_val = value\n","\n","    min_val_list[z] = min_val\n","    max_val_list[z] = max_val\n","    z+=1\n"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["df['business_tags'] = [ (Counter({k:v for k,v in rows['business_tags'].items() if match_words_with_tf_idf_valuess[(k, idx)] >=0.06 and k not in most_used_sector_terms  })) for idx, (_, rows) in enumerate(df.iterrows()) ]"]},{"cell_type":"markdown","metadata":{},"source":["### Final Business Tags Filtering Using TF-IDF and Embedding Similarity\n","\n","This final step filters weak business tags by analyzing their TF-IDF scores and embedding similarities compared to stronger words.\n","\n","### Main Steps\n","\n","- **TF-IDF Thresholding**:\n","  - Compute dynamic thresholds (25% and 75%) between the minimum and maximum TF-IDF scores for each company.\n","  - Split business tags into **high TF-IDF words** and **low TF-IDF words** based on these thresholds.\n","\n","- **Similarity Checking**:\n","  - For each low TF-IDF tag:\n","    - Compare its embedding against the high TF-IDF tags.\n","    - If similarity is low (< 0.15), further checks are performed:\n","      - Check if parts of compound words (split by \"_\") match better.\n","      - Check semantic relations using `IsA` relationships.\n","      - Compare to \"most important words\" embeddings if necessary.\n","\n","- **Tag Deletion**:\n","  - If no strong relationship is found after all checks, the low-quality tag is **deleted** from the business tags list.\n","\n","- **Special Handling**:\n","  - Companies flagged in `do_exception_for_niche_for_these_companies` are skipped during this phase.\n","\n","### Goal\n","\n","Aggressively clean business tags by removing weak or loosely related terms, ensuring that final tags are highly relevant and semantically connected to strong description keywords."]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["index_cur = 0\n","for idx, row in df.iterrows():\n","\n","    min25th = (min_val_list[index_cur] + 0.25 * (max_val_list[index_cur]-min_val_list[index_cur]))\n","\n","    max25th = (max_val_list[index_cur] - 0.50 * (max_val_list[index_cur]-min_val_list[index_cur]))\n","    high_tf_idf_word = []\n","    low_tf_idf_word = []\n","\n","    for word in (df['business_tags'][idx]+df['description'][idx]):\n","        if match_words_with_tf_idf_valuess[(word, index_cur)] > max25th:\n","            high_tf_idf_word.append(word)\n","        \n","    copy_df_business = df.at[idx, 'business_tags'].copy()\n","\n","    if idx in do_exception_for_niche_for_these_companies:\n","        index_cur+=1\n","        continue\n","\n","    for word in (df['business_tags'][idx]):    \n","        if match_words_with_tf_idf_valuess[(word, index_cur)] < min25th:\n","            low_tf_idf_word.append(word)\n"," \n","    if low_tf_idf_word != [] and high_tf_idf_word != []:\n","        low_emb = np.array([embeddings_index[wd] for wd in low_tf_idf_word], dtype= np.float32, order='C')\n","        faiss.normalize_L2(low_emb)\n","\n","        high_emb =  np.array([embeddings_index[wd] for wd in high_tf_idf_word], dtype= np.float32, order='C')\n","        faiss.normalize_L2(high_emb)\n","\n","        values_score = np.dot(low_emb, high_emb.T)\n","\n","        for i in range(len(low_tf_idf_word)):\n","            value_maximum = np.max(values_score[i])\n","            index_1 = list(values_score[i]).index(value_maximum)\n","            \n","            if value_maximum < 0.15:\n","\n","                \n","\n","\n","                \n","                value_element_max = None\n","                list_elements = []\n","\n","                \n","\n","                if (len(low_tf_idf_word[i].split(\"_\"))>1):\n","                    list_splitted_words1 = low_tf_idf_word[i].split(\"_\")\n","                    list_splitted_emb1 = np.array([embeddings_index.get(wd, np.zeros(300)) for wd in list_splitted_words1],  dtype= np.float32, order='C')\n","                    faiss.normalize_L2(list_splitted_emb1)\n","                    word1_embeddings = embeddings_index.get(high_tf_idf_word[index_1], np.zeros(300))\n","                    \n","                    our_valuezzz = np.dot(list_splitted_emb1, word1_embeddings.T)\n","\n","\n","                    max_value_current = np.max(our_valuezzz)\n","\n","                    \n","\n","                    \n","                    if max_value_current > 0.25:\n","                        continue\n","\n","\n","\n","                if high_tf_idf_word[index_1] in isA_relationship.keys():\n","                    list_elements_high = [wd2 for wd2 in isA_relationship[high_tf_idf_word[index_1]]]\n","                    value_element_max = np.array([embeddings_index[wd] for wd in list_elements_high], dtype= np.float32, order='C')\n","                    \n","                    faiss.normalize_L2(value_element_max)\n","                else:\n","                    value_element_max = embeddings_index[high_tf_idf_word[index_1]].reshape(1,-1)\n","\n","\n","\n","                current_element = None\n","                if low_tf_idf_word[i]  in isA_relationship.keys():\n","                    list_elements = list(isA_relationship[low_tf_idf_word[i]])\n","\n","                    current_element = np.array([embeddings_index[wd1] for wd1 in list_elements], dtype= np.float32, order='C')\n","                    faiss.normalize_L2(current_element)\n","                   \n","                else:\n","                    current_element = embeddings_index[low_tf_idf_word[i]].reshape(1,-1)\n","                    list_elements = [low_tf_idf_word[i]]\n","                \n","                \n","                \n","                values_score2 = np.dot(current_element, value_element_max.T)\n","                max_v = -1\n","                for j, _ in enumerate(list_elements):\n","                    val_ = np.max(values_score2[j])\n","\n","                    if max_v < val_:\n","                        max_v =val_\n","                if max_v < 0.3:\n","                    miw_e =  np.array(most_important_words_embeddings[index_cur], dtype= np.float32, order='C')\n","                    faiss.normalize_L2(miw_e)\n","                    our_value = np.max(np.dot(low_emb, miw_e.T)[i])\n","\n","                    if our_value > 0.35:\n","                        continue\n","                    del copy_df_business[low_tf_idf_word[i]]\n","\n","    \n","    df.at[idx, 'business_tags'] = copy_df_business\n","    index_cur+=1\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["idx_z=0\n","for lists_x in copy_business_tags:\n","    wd_e = np.array([embeddings_index[wd] for wd in df['description'][idx_z]], dtype= np.float32, order='C')\n","\n","    for x in lists_x:\n","        if x not in df.at[idx_z, 'business_tags']:\n","            words_list = which_terms_are_associated_with_value.get((x, idx_z), [])\n","            if words_list != []:\n","                for word1 in words_list:\n","                    if x in set_sector_elements and np.dot(embeddings_index[x], embeddings_index[df.at[idx_z, 'sector'].replace(\" \",\"_\").lower()].T) < 0.4 and np.max(np.dot(embeddings_index[x], wd_e.T))>0.4:\n","                           df.at[idx_z, 'business_tags'].update(Counter([x]))\n","                    elif word1 in df.at[idx_z, 'business_tags']:    \n","                        df.at[idx_z, 'business_tags'].update(Counter([x]))\n","                        break\n","\n","    idx_z+=1"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["business_tags_embeddings_edited = np.array(df.apply(lambda x: get_sentence_embeddings_for_categories(x['business_tags']), axis=1).to_list())"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[],"source":["generic_domain_terms = {\"commercial\", \"residential\", \"family\", \"general\", \"corporate\", \"industrial\", \"building\", \"ornamental\", \"tilt\", \"precast\"}"]},{"cell_type":"markdown","metadata":{},"source":["### Refining Niche and Category Tags Based on Embedding Similarity\n","\n","This step further cleans the `niche` and `category` fields by comparing their relevance to the description, business tags, and each other using embeddings and adaptive thresholds.\n","\n","### Main Steps\n","\n","- **Embedding Similarity Comparisons**:\n","  - Compare `niche` and `category` embeddings with description and business tags embeddings.\n","  - Compute similarity scores using FAISS and cosine similarity.\n","\n","- **Adaptive Thresholding**:\n","  - Dynamic thresholds (around 0.3–0.375) are set based on overall similarity scores for each company.\n","  - Lower thresholds for generic domain terms to allow stricter filtering.\n","\n","- **Niche Tag Filtering**:\n","  - Remove niche tags with low maximum similarity to description or business tags.\n","  - If some niche terms are borderline, they are reconsidered based on the average similarity across the set.\n","\n","- **Category Tag Filtering**:\n","  - Remove category tags with weak similarity to description, business tags, or niche terms.\n","  - Special fallback handling for companies needing niche-based matching.\n","  \n","- **Special Handling**:\n","  - Companies listed in `do_exception_for_niche_for_these_companies` are treated differently by relying more on niche-based comparisons.\n","  - Words deleted can be reconsidered if the average similarity across terms is still strong.\n","\n","### Goal\n","\n","To ensure that both `niche` and `category` tags are contextually meaningful and strongly aligned with the company’s description and business tags, improving label precision."]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["\n","i=0\n","for idx, rows in df.iterrows():\n","    de3 = np.array(description_embeddings_original[i], dtype= np.float32, order='C')\n","    faiss.normalize_L2(de3)\n","    bte3 = np.array(business_tags_embeddings_edited[i], dtype= np.float32, order='C')\n","    faiss.normalize_L2(bte3)\n","    ce3 = np.array(category_embeddings2[i], dtype=np.float32, order='C')\n","    faiss.normalize_L2(ce3)\n","    ne2 = np.array(niche_embeddings2[i], dtype = np.float32, order='C')\n","    faiss.normalize_L2(ne2)\n","\n","\n","\n","\n","    val_category = np.einsum('jk,lk->jl', ce3, de3)\n","    val_category_bt = np.einsum('jk,lk->jl', ce3, bte3)\n","    val_niche_bt = np.einsum('jk,lk->jl', ne2, bte3)\n","\n","    val_niche = np.einsum('jk,lk->jl', ne2, de3)\n","    list_category = [bt for bt in rows['category']]\n","    list_niche = [bt for bt in rows['niche']]\n","    list_description = [bt for bt in df_temp['description'][idx]]\n","    list_business = [bt for bt in rows['business_tags']]\n","\n","    avg_number_list = []\n","    words_to_consider_again = []\n","\n","    if idx not in do_exception_for_niche_for_these_companies and (df.at[idx, 'business_tags']+df.at[idx, 'description']) != Counter():\n","        threshold=0.3\n","        mean_val = None\n","        if similarity_matrix3[i][i] > 0.2:\n","            threshold=0.3\n","        else:\n","            threshold=0.375\n","\n","        \n","        \n","        copy_df_niche = df['niche'][idx].copy()\n","        sector_word_to_consider = []\n","        for j in range(len(df['niche'][idx])):\n","            val_niche_2 = val_niche[j][0:len(df['description'][idx])]\n","            \n","            if threshold!=0.3:\n","\n","                if len(val_niche_2) != 0:\n","                    mean_val = np.mean(val_niche_2)\n","                else:\n","                    mean_val = 0\n","                \n","            mav_val_niche = max(val_niche[j])\n","            mav_val_niche2 = max(val_niche_bt[j])\n","\n","            \n","            val_niche_2bt = list(filter(lambda x: x!=0, list(val_niche_bt[j])))\n","\n","            list_max_val_nc = []\n","            if df['description'][idx] != '':\n","                list_max_val_nc = list(zip(val_niche_2, range(len(val_niche_2))))\n","            list_max_val_nc2 = list(zip(val_niche_2bt, range(len(val_niche_2bt))))\n","            list_max_val_nc.sort(reverse=True)\n","            list_max_val_nc2.sort(reverse=True)\n","            \n","            list_max_val_nc = list(filter(lambda x: x[1] < len(list_description) and (match_average_words_with_tf_idf_valuess[list_description[x[1]]]/match_number_words_with_tf_idf_valuess[list_description[x[1]]])>0.065,list_max_val_nc))\n","            list_max_val_nc2 = list(filter(lambda x: x[1] < len(list_business) and  (match_average_words_with_tf_idf_valuess[list_business[x[1]]]/match_number_words_with_tf_idf_valuess[list_business[x[1]]])>0.065,list_max_val_nc2))\n","            \n","            list_max_val_nc = list(map(lambda x: x[0], list_max_val_nc))\n","            list_max_val_nc2 = list(map(lambda x: x[0], list_max_val_nc2))\n","            \n","            list_max_val_nc.append(0.0)\n","            list_max_val_nc2.append(0.0)\n","            if list_max_val_nc != []:\n","                mav_val_niche = max(max(list_max_val_nc), max(list_max_val_nc2))\n","            else:\n","                mav_val_niche =  max(list_max_val_nc2)\n","\n","            if df.at[idx, 'sector'] != '':\n","                cos_similarity_sector = np.dot(embeddings_index[list_niche[j]], embeddings_index[df.at[idx,'sector'].lower().replace(\" \", \"_\")].T)\n","            else:\n","                cos_similarity_sector = 0\n","            \n","            avg_number_list.append(mav_val_niche)\n","\n","            if list_niche[j] in generic_domain_terms and threshold==0.3:\n","                threshold-=0.05\n"," \n","            if cos_similarity_sector > 0.85:\n","                sector_word_to_consider.append(list_niche[j])\n","\n","\n","            if(mav_val_niche < threshold and threshold<=0.3) or (threshold>0.3 and (mean_val!=None and mav_val_niche < threshold or mean_val<0.15)):\n","                \n","                if (mav_val_niche < 0.7 or (mav_val_niche < 0.85 and mean_val!=None and mean_val<0.13)):\n","                    del copy_df_niche[list_niche[j]]\n","            elif (mav_val_niche < threshold+0.05 and threshold<=0.3):\n","                del copy_df_niche[list_niche[j]]\n","                words_to_consider_again.append(list_niche[j])\n","            \n","            if list_niche[j] in generic_domain_terms and threshold==0.25:\n","                threshold+=0.05\n","\n","            \n","            df.at[idx, 'niche'] = copy_df_niche\n","\n","    if len(avg_number_list)!=0:\n","        mean_avg_number_list = np.mean(avg_number_list)\n","    else:\n","        mean_avg_number_list = 0\n","    if words_to_consider_again != list() and avg_number_list!=list() and mean_avg_number_list >=0.55:\n","        df.at[idx, 'niche'].update(Counter(words_to_consider_again))\n","    if df.at[idx, 'niche'] != Counter():\n","        sector_word_to_consider = list(filter(lambda x: x not in df.at[idx, 'niche'], sector_word_to_consider))\n","        df.at[idx, 'niche'].update(Counter(sector_word_to_consider))\n","\n","    \n","        \n","    avg_number_list = []\n","    words_to_consider_again = []\n","    if similarity_matrix2[i][i] < 0.5:\n","        threshold=0.3\n","\n","        mean_val = None\n","        max_niche_value = None\n","\n","        stay_in_loop = 1\n","        copy_df_category = df['category'][idx].copy()\n","        mav_val_category = -1\n","        if idx in do_exception_for_niche_for_these_companies:\n","            ne3 = np.array([embeddings_index[wd] for wd in df['niche'][idx]], dtype= np.float32, order='C')\n","            val_category_nc = np.einsum('jk,lk->jl', ce3, ne3)\n","        \n","        if similarity_matrix2[i][i] > 0.2:\n","            threshold=0.3\n","        else:\n","            threshold=0.375\n","            if df['niche'][idx] != Counter():\n","                ne3 = np.array([embeddings_index[wd] for wd in df['niche'][idx]], dtype= np.float32, order='C')\n","                val_category_nc = np.einsum('jk,lk->jl', ce3, ne3)\n","                list_niche = [bt for bt in df.at[idx, 'niche']]\n","            else:\n","                ne3 = np.array([np.zeros(300)], dtype= np.float32, order='C')\n","                val_category_nc = np.einsum('jk,lk->jl', ce3, ne3)\n","                list_niche = [bt for bt in df.at[idx, 'niche']]\n","\n","                \n","            \n","    \n","        for j in range(len(df['category'][idx])):\n","            x, y = None, None\n","            x_list, y_list = [], []\n","\n","\n","            \n","\n","\n","            if idx not in do_exception_for_niche_for_these_companies:\n","                x = val_category_bt[j]\n","                y = val_category[j]\n","\n","                if len(df.at[idx, 'description'])==0:\n","                    y=[0]\n","                \n","                y_list = list_description\n","                if list_description== '':\n","                    y_list = []\n","                x_list = list_business\n","                \n","                mav_val_category = max(max(val_category_bt[j]), max(val_category[j]))\n","            else:\n","\n","                \n","                x = val_category_nc[j]\n","                y = val_category[j]\n","                if len(df['description'][idx])==0:\n","                    x=[0]\n","               \n","                y_list = list_description\n","                if list_description== '':\n","                    y_list = []\n","                \n","                x_list = list_niche\n","                mav_val_category = max(max(val_category_nc[j]), max(val_category[j]))\n","           \n","            x = list(filter(lambda x: x!=0, list(x)))\n","            y = list(filter(lambda x: x!=0, list(y)))\n","            \n","            if threshold!=0.3:\n","                if len(y)!=0:\n","                    mean_val = np.mean(y)\n","                else:\n","                    mean_val = 0\n","\n","                max_niche_value = np.max(val_category_nc[j])\n","    \n","            x1 = list(zip(x, range(len(x))))\n","            y1 = list(zip(y, range(len(y))))\n","            x1.sort(reverse=True)\n","            y1.sort(reverse=True)\n","\n","            \n","            x1 = list(filter(lambda z: z[1] < len(x_list) and (match_average_words_with_tf_idf_valuess[x_list[z[1]]]/match_number_words_with_tf_idf_valuess[x_list[z[1]]])>0.065,x1))\n","            y1 = list(filter(lambda z: z[1] < len(y_list) and  (match_average_words_with_tf_idf_valuess[y_list[z[1]]]/match_number_words_with_tf_idf_valuess[y_list[z[1]]])>0.065,y1))\n","            \n","            x1 = list(map(lambda z: z[0], x1))\n","            y1 = list(map(lambda z: z[0], y1))\n","            \n","            x1.append(0.0)\n","            y1.append(0.0)\n","            mav_val_category = max(max(x1), max(y1))\n","\n","            if list_category[j] in generic_domain_terms and threshold==0.3:\n","                threshold-=0.05\n","\n","            \n","            if(mav_val_category < threshold and threshold<=0.03) or (threshold>0.3 and mean_val!=None and (mav_val_category < threshold or mean_val<0.15)):\n","                if (mav_val_category < 0.7 or (mav_val_category < 0.85 and mean_val!=None and mean_val<0.13)) and (max_niche_value==None or max_niche_value<0.55):\n","                    del copy_df_category[list_category[j]]\n","            elif (mav_val_category < threshold+0.05 and threshold<=0.3):\n","                \n","                del copy_df_category[list_category[j]]\n","                words_to_consider_again.append(list_category[j])\n","            \n","            if list_category[j] in generic_domain_terms and threshold==0.25:\n","                threshold+=0.05\n","        \n","\n","\n","        \n","        df.at[idx, 'category'] = copy_df_category\n","        if len(avg_number_list) != 0:\n","            mean_avg_number_list = np.mean(avg_number_list)\n","        else:\n","            mean_avg_number_list = 0\n","\n","        if words_to_consider_again != list() and avg_number_list!=list() and mean_avg_number_list >=0.55:\n","            df.at[idx, 'category'].update(Counter(words_to_consider_again))\n","\n","    i+=1\n","\n","    "]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["df_temp['business_tags'] = df['business_tags']\n","df['description'] = df_temp['description']\n","df_temp['category'] = df['category']\n","df_temp['niche'] = df['niche']\n","df['most_important_words'] = [Counter(df.at[idx, 'most_important_words']) for idx, _ in df.iterrows()]\n"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["all_overall_embeddings2 = np.array(df.apply(lambda x: get_sentence_embeddings_for_categories(x['description']+x['business_tags']+x['niche']+x['category']+x['most_important_words']), axis=1).to_list())"]},{"cell_type":"markdown","metadata":{},"source":["### Filtering Title Description Tags Based on Embedding Similarity\n","\n","This final step filters out weak title-description words by comparing their embeddings against all available company embeddings.\n","\n","### Main Steps\n","\n","- **Embedding Similarity**:\n","  - Compare `title_description` embeddings to the overall embeddings (`all_overall_embeddings2`) for each company.\n","  - Use cosine similarity to measure relevance.\n","\n","- **Thresholding**:\n","  - Words with maximum similarity below a threshold (0.35) are **deleted** from the `title_description` field.\n","\n","- **Final Cleaning**:\n","  - Only title-description words strongly connected to the overall context are retained.\n","\n","### Goal\n","\n","Ensure that the `title_description` field contains only highly relevant and semantically aligned words, improving overall quality and consistency."]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["\n","i=0\n","for idx, rows in df_temp.iterrows():\n","\n","\n","    allovce = np.array(all_overall_embeddings2[i], dtype= np.float32, order='C')\n","    faiss.normalize_L2(allovce)\n","\n","    te2 = np.array(title_embeddings2[i], dtype= np.float32, order='C')\n","    faiss.normalize_L2(te2)\n","    \n","\n","    val_title = np.einsum('jk,lk->jl', te2, allovce)\n","    list_title = [bt for bt in rows['title_description']]      \n","    stay_in_loop = 1\n","    threshold = 0.35\n","    copy_df_temp_title = df_temp['title_description'][idx].copy()\n","    if list_title != []:\n","        for j in range(len(df_temp['title_description'][idx])):\n","            mav_val_title = max(val_title[j])\n","            if (mav_val_title < threshold):\n","                del copy_df_temp_title[list_title[j]]\n","\n","    df_temp.at[idx, 'title_description'] = copy_df_temp_title\n","\n","    i+=1\n","\n","    "]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["df['description'] = df_temp['description'] + df_temp['title_description']"]},{"cell_type":"markdown","metadata":{},"source":["### Building the Final Combined Feature Column\n","\n","This step creates a new enriched column combining all important fields for each company, preparing the data for the final model.\n","\n","### Main Steps\n","\n","- **Combining Fields**:\n","  - Merge `description`, `business_tags`, `category`, and `niche` fields together.\n","  - `category` and `niche` are repeated to emphasize their importance.\n","  - Add `most_important_words`, giving them extra weight if they aren't already present.\n","\n","- **Weighting Words**:\n","  - The combined list is transformed into a `Counter`, keeping track of how many times each word appears.\n","\n","- **Final Adjustments**:\n","  - Sector information is appended twice to strengthen its presence.\n","  - A final cleaned version (`new_col`) is stored as a **set** (for uniqueness).\n","  - A backup version (`df_old['new_col']`) retains the **full counts** before conversion to set form.\n","\n","### Goal\n","\n","To build a rich, weighted feature representation of each company that combines descriptions, tags, important words, and sector information, ready for downstream machine learning or matching tasks."]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["for idx, _ in df.iterrows():\n","    df.at[idx, 'new_col'] = df.at[idx, 'description']+df.at[idx, 'business_tags']+ df.at[idx, 'category'] + df.at[idx, 'niche'] + df.at[idx, 'category'] + df.at[idx, 'niche']\n","    list_to_add = []\n","    our_current_list = list(df.at[idx, 'new_col'])\n","    for word in df['most_important_words'][idx]:\n","        if word not in our_current_list:\n","            list_to_add.append(word)\n","            list_to_add.append(word)\n","        else:\n","            list_to_add.append(word)\n","    df.at[idx, 'new_col'].update(Counter(list_to_add))"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["df['new_col'] =  [list(map(lambda x: (x,row['new_col'][x]), row['new_col']))  for _, row in df.iterrows()]\n","\n","list_elements = []\n","for idx, row in df.iterrows():\n","    list__ = []\n","    for word in row['new_col']:\n","\n","        if(word[0].isupper()):\n","            list_elements.append(word[0])\n","        b = word[0]\n","    \n","        list__+=([b]*word[1])\n","\n","    list__.append(df_old_copy['sector'][idx])\n","    list__.append(df_old_copy['sector'][idx])\n","    df.at[idx, 'new_col2'] = Counter(list__)\n","    set_List__ = set(list__)\n","    set_List__.discard('')\n","    df.at[idx, 'new_col'] = set_List__"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["df_old = df.copy()\n","df_old['new_col'] = df['new_col2'].copy()\n"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["df_old['count_niche_cat'] = (df['niche']+df['category']).copy()"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["df['new_col2'] = [Counter(elements_label) for elements_label in df['new_col']]"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["X_csr, feature_names, tf_idf, X = get_tfidf_sparse_matrix(df['new_col2'])"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["get_dictionary_for_tfidif(X_csr, feature_names, tf_idf, X)"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["import faiss\n","zz_o = 0\n","list_a = []\n","high_correlation_scores = {}\n","for idx, rows in df.iterrows():\n","    zz_o+=1\n","    list_new_col = rows['new_col'].copy()\n","    threshold = 0.30\n","    continue_iteration = 1    \n","    word_embedding_list1 = np.array([np.pad(embeddings_index.get(wd.lower(), np.zeros(300)), (0, label_embeddings[0].shape[0] - len(embeddings_index.get(wd.lower(), np.zeros(300)))), mode='constant') for wd in df['new_col'][idx]])\n","    we1 = np.array(word_embedding_list1, dtype=np.float32, order='C')\n","    faiss.normalize_L2(we1)\n","\n","    le1 = np.array(label_embeddings, dtype=np.float32, order='C')\n","    faiss.normalize_L2(le1)\n","\n","    el = np.dot(we1, le1.T)\n","\n","    to_continue_founction = 1\n","    word_list = [wd for wd in df['new_col'][idx]]\n","    current_row = rows['new_col'].copy()\n","    len_current_row_original = len(current_row)\n","    \n","    \n","    while to_continue_founction:\n","       \n","        current_row = rows['new_col'].copy()\n","        len_current_row = len(current_row)\n","\n","        for index in range(len(rows['new_col'])):\n","            max_val_word = max(el[index])\n","            if max_val_word <= threshold:\n","               \n","               current_row.remove(word_list[index])\n","            \n","        x\n","        if current_row == Counter() or (len(current_row)<=1 and len_current_row_original!=1):\n","            threshold-=0.1\n","        else:\n","            to_continue_founction=0\n","\n","\n","    df.at[idx, 'new_col'] = current_row\n","              "]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["df2['new_col'] = df['new_col'].copy()"]},{"cell_type":"markdown","metadata":{},"source":["### Filtering Low-Similarity Tokens\n","\n","To reduce noise and improve the relevance of our input, we filter out **tokens that have low similarity to any label**.\n","\n","This step ensures that only tokens with meaningful connections to the labels are kept, helping the classifier focus on the most important features.\n","\n","Instead of using `cosine_similarity` (which can be slower for large datasets), we use **`np.dot()`** for computing similarity, since our embeddings are already **L2-normalized**. This gives us the same result as cosine similarity but with **better performance**."]},{"cell_type":"markdown","metadata":{},"source":["### Token Embedding with TF-IDF Weighting\n","\n","For each company, we compute **embeddings for its individual tokens**. This step prepares the data for efficient similarity search using **FAISS** later on.\n","\n","We incorporate **TF-IDF weighting** into the embeddings to emphasize more informative terms. Each token's embedding is scaled according to its TF-IDF weight, using the following formula:  \n","weighted_embedding = (embedding_term × tf-idf_weight) / sum(tf-idf_weights_per_row)     \n","\n","This ensures that tokens with higher importance (based on TF-IDF) have a greater influence on the final representation of the company, this improves both accuracy and efficiency in label matching and similarity search."]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["vec_embeddings=[]\n","MAX_TOKENS = max(df['new_col'].apply(len))\n","global rows_\n","def get_sentence_embeddings(word_list, row):\n","    embeddings_ = []\n","    if word_list == Counter():\n","        embeddings_= np.pad(embeddings_, (0, (400-len(embeddings_))), mode = 'constant')\n","    else:\n","        sum_list = sum([match_words_with_tf_idf_valuess[(word, row)] for word in word_list])\n","\n","        embeddings_ = [(match_words_with_tf_idf_valuess[(word, row)] *\n","        ( embeddings_index.get(word, np.zeros(300)\n","        )))/(sum_list)for word in word_list]\n","        embeddings_ = np.mean(embeddings_, axis=0)\n","        if (len(embeddings_)) < X.shape[1]:\n","            embeddings_= np.pad(embeddings_, (0, (400-len(embeddings_))), mode = 'constant')\n","    \n","    return embeddings_\n","vec_embeddings = np.array(df.apply(lambda x: get_sentence_embeddings(x['new_col'], x['rows']), axis=1).to_list())\n","cat_niche_embeddings = np.array(df.apply(lambda x: get_sentence_embeddings(x['category']+x['niche'], x['rows']), axis=1).to_list())"]},{"cell_type":"markdown","metadata":{},"source":["### Selecting the Strongest Terms for Each Label\n","\n","To identify the **strongest term for each label**, we compare all label terms against the tokens of **every company** using semantic similarity.\n","\n","For each label, we select the **term that achieves the highest similarity score** with any company token. This ensures that the chosen term is not just statistically unique, but also **semantically meaningful and relevant**.\n","\n","#### Why We Changed the Approach:\n","Previously, we selected the top term based solely on **TF-IDF score**. However, this often led to choosing **rare adjectives** that had high TF-IDF values but were **not representative** of the label's actual meaning.\n","\n","By switching to a **semantic similarity-based method**, we now capture terms that are **both meaningful and more generalizable** across companies."]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["def replace_word_with_right_one(core_word):\n","    if core_word[-2:]==\"es\" and core_word[:-2] in match_average_words_with_tf_idf_valuess.keys():\n","        core_word =core_word[:-2]\n","    elif core_word[-1:]==\"s\" and core_word[:-1] in match_average_words_with_tf_idf_valuess.keys():\n","        core_word =core_word[:-1]\n","    return core_word"]},{"cell_type":"markdown","metadata":{},"source":["### Generating Final Labels with Domain, Modifier, and Core Structure\n","\n","This step restructures each label by identifying a domain, modifier, and core term to create more consistent, structured class names, which will later be used to boost important terms during classification.\n","\n","### Main Steps\n","\n","- **Splitting Labels**:\n","  - Parse the `refined_labels2` field using NLP to extract:\n","    - **Domain** = first word,\n","    - **Core** = last word,\n","    - **Modifier** = best middle word (chosen by highest TF-IDF score).\n","\n","- **Handling Special Cases**:\n","  - Simplified rules are applied for labels with only 2–3 words.\n","  - Bigrams and trigrams are considered to select the best modifier candidate.\n","\n","- **Marking Generic Terms**:\n","  - Generic or very generic domain/core/modifier words are marked (e.g., `__1`, `::1`) to help during boosting.\n","\n","- **Final Label Construction**:\n","  - Create `new_label_with_categories` by combining domain, modifier, and core.\n","\n","- **Later Usage**:\n","  - During classification, terms extracted from this structured label are **boosted** to increase their influence and improve prediction accuracy.\n","\n","### Goal\n","\n","Create a systematic, structured version of each label that can be leveraged during classification to better guide and boost the most meaningful terms."]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["our_classes['refined_labels'] = our_classes['new_label']\n","our_classes['refined_labels2'] = our_classes['new_label']\n","\n","for idx, row in our_classes.iterrows():\n","    value = row['refined_labels'][-1]\n","\n","    our_classes.at[idx,'refined_labels2'] = list(filter(lambda x: x if len(x.split(\"_\"))==1 or (x.split(\"_\")[0] not in list(row['new_label']) or  x.split(\"_\")[1] not in list(row['new_label'])) else \"\", row['new_label']))\n","    if our_classes.at[idx, 'refined_labels2'] == []:\n","        our_classes.at[idx, 'refined_labels2'].append(value)\n"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["core = [\"\"] * len(our_classes['new_label2'])\n","modifier = [\"\"] * len(our_classes['new_label'])\n","domain = [\"\"] * len(our_classes['new_label'])\n","\n","for i in range(len(our_classes['refined_labels2'])):\n","\n","    doc = nlp(\" \".join(our_classes.at[i, 'refined_labels2']))\n","\n","    list_elements = []\n","    for token in doc:\n","        list_elements.append(token.text)\n","    if len(doc) > 3:\n","        domain[i] = doc[0].text\n","        core[i] = doc[-1].text\n","    \n","        list_elements = []\n","        for token in doc[1:-1]:\n","            list_elements.append(token.text)\n","        best_value_term = -1\n","        best_word = \"\"\n","        list_elements.extend(new_ngrams(list_elements, 2))\n","        list_elements.extend(new_ngrams(list_elements, 3))\n","        for word in list_elements:\n","            if best_value_term < (match_average_words_with_tf_idf_valuess[word]/match_number_words_with_tf_idf_valuess[word]):\n","                best_value_term = (match_average_words_with_tf_idf_valuess[word]/match_number_words_with_tf_idf_valuess[word])\n","                best_word = word\n","        modifier[i] = best_word\n","    elif len(doc) == 3:\n","\n","        list_elements = []\n","        for j in doc:\n","            list_elements.append(j.text)\n","        \n","        domain[i] = list_elements[0]\n","        modifier[i] = list_elements[1]\n","        core[i] = list_elements[2]\n","    elif len(doc)==2:\n","        list_elemnts = [el.text for el in doc[0:2]]\n","        domain[i] = doc[0].text\n","        core[i] = doc[1].text\n","    else:\n","        \n","        domain[i] = doc[0].text\n"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["very_generic_core_terms = {\n","    \"promotion\",\n","    'production',\n","    \"services\",\n","    \"service\"\n","}"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":["\n","for i in range(len(core)):\n","    if core[i] in generic_core_terms:\n","        core[i] = core[i]+\"__1\"\n","    if core[i] in very_generic_core_terms:\n","        core[i] = core[i]+\"::1\"\n","    if domain[i] in generic_domain_terms:\n","        domain[i] = domain[i]+\"__1\"\n","    if modifier[i] == \"\":\n","        modifier[i] = \"0\"\n","    if core[i] == \"\":\n","        core[i] = \"0\""]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["for i in range(len(core)):\n","    our_classes.at[i,'new_label_with_categories'] = f\"{domain[i]} {modifier[i]} {core[i]}\"\n"]},{"cell_type":"markdown","metadata":{},"source":["### Saving Relevant Data to a New File\n","\n","The Python program was getting too long and slow to run all at once.  \n","To make things easier and faster, we saved only the **important data** we found into **separate files**.\n","\n","This helped us keep the project organized and continue working in smaller, manageable steps."]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["df['niche_plus_cat'] = df['niche']\n","\n","for idx, rows in df.iterrows():\n","    df.at[idx, 'niche_plus_cat'] = list(df.at[idx,'niche']+df.at[idx, 'category'])\n","    df.at[idx, 'sector'] = df.at[idx, 'sector'].replace(\" \", \"_\").lower()"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["for idx, _ in df_old.iterrows():\n","    df_old.at[idx, 'new_col']= {k:v for k,v in df_old.at[idx,'new_col'].items() if k!=''}"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["for idx, i in enumerate(df2['description']):\n","    if i=='':\n","        similarity_matrix[idx] = (similarity_matrix_label2[idx] + similarity_matrix_label3[idx])/2\n"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["np.savetxt(\"vec_embeddings.txt\", vec_embeddings, fmt=\"%.12f\")\n","np.savetxt(\"label_embeddings.txt\", label_embeddings, fmt=\"%.12f\")\n","np.savetxt(\"cat_niche_embeddings.txt\", cat_niche_embeddings, fmt=\"%.12f\")\n","np.savetxt(\"context_matrix.txt\", similarity_matrix, fmt=\"%.12f\")\n","np.savetxt(\"niche_matrix.txt\", similarity_matrix_label2, fmt=\"%.12f\")\n","np.savetxt(\"category_matrix.txt\", similarity_matrix_label3, fmt=\"%.12f\")\n","np.savetxt(\"similarity_desc_to_niche.txt\", similarity_matrix2, fmt=\"%.12f\")\n","np.savetxt(\"similarity_desc_to_category.txt\", similarity_matrix3, fmt=\"%.12f\")\n","np.savetxt(\"first_sentence_matrix_total1.txt\", similarity_matrix_first_sentence, fmt=\"%.12f\")\n","\n","df['new_col'].to_csv('tokens_vector.csv')\n","df_old['new_col'].to_csv('counted_elements.csv')\n","df_old['count_niche_cat'].to_csv('counted_categories.csv')\n","df['sector'].to_csv('sectors.csv')\n","df['niche_plus_cat'].to_csv('our_current_categories.csv')\n","our_classes['new_label_with_categories'].to_csv('new_label_with_categories.csv')"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[],"source":["ddd=0\n","for idx, row in enumerate(df['new_col']):\n","    max_v1=-1.0\n","\n","    for wd in row:\n","        \n","        if max_v1 < match_words_with_tf_idf_valuess[(wd, ddd)]:\n","            max_v1 = match_words_with_tf_idf_valuess[(wd, ddd)]\n","    \n","    ddd+=1"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":["fout = \"match_words_with_tf_idf.txt\"\n","fo = open(fout, \"w\")\n","\n","for k, v in match_words_with_tf_idf_valuess.items():\n","    fo.write(str(k[0])+\" \"+str(k[1]) + ' '+ str(v) + '\\n')\n","\n","fo.close()"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":["fout = \"most_important_words.txt\"\n","fo = open(fout, \"w\")\n","\n","for idx, _ in df.iterrows():\n","    list_cur = df['most_important_words'][idx]+df['title_description'][idx]\n","    for i in list(list_cur):\n","        fo.write(i)\n","        fo.write(\" \")\n","    fo.write(\"\\n\")\n","    \n","\n","fo.close()"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["fout = \"average_tf_idf_per_word.txt\"\n","fo = open(fout, \"w\")\n","\n","for k, v in match_average_words_with_tf_idf_valuess2.items():\n","    fo.write(str(k)+ ' '+ str(v/(match_number_words_with_tf_idf_valuess2[k])) + '\\n')\n","\n","fo.close()"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[],"source":["fout = \"correlated_terms.txt\"\n","fo = open(fout, \"w\")\n","\n","for k in correlated_terms:\n","    fo.write(str(k[0])+\" \"+str(k[1])+'\\n')\n","\n","fo.close()"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":2}
